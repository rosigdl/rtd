{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e3ff56-7264-4090-9fd9-7984577f7465",
   "metadata": {},
   "source": [
    "# Kullback-Leibler Divergence\r\n",
    "\r\n",
    "The Kullback-Leibler (KL) Divergence is a measure of how one probability distribution diverges from a second, expected probability distribution.\r\n",
    "\r\n",
    "Mathematically, the KL Divergence of two discrete probability distributions P and Q is defined as:\r\n",
    "\r\n",
    "$$D_{KL}(P || Q) = \\sum_{i} P(i) \\log \\frac{P(i)}{Q(i)}$$\r\n",
    "\r\n",
    "For continuous distributions, the sum is replaced by an integral:\r\n",
    "\r\n",
    "$$D_{KL}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} dx$$\r\n",
    "\r\n",
    "where:\r\n",
    "- $P$ and $Q$ are the two probability distributions.\r\n",
    "- $p(x)$ and $q(x)$ are the probability density functions of $P$ and $Q$, respectively.\r\n",
    "\r\n",
    "It's important to note that KL Divergence is not symmetric. That is, $D_{KL}(P || Q) \\neq D_{KL}(Q || P)$.\r\n",
    "\r\n",
    "In the context of machine learning, KL Divergence is often used as a loss function in optimization problems. The goal is to minimize the divergence of the predicted probability distribution from the true distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e54762-c882-4ed5-8c9f-d1edfdacca8b",
   "metadata": {},
   "source": [
    "## KL Divergence in Machine Learning\r\n",
    "\r\n",
    "In machine learning, KL Divergence can be used in various ways, including but not limited to:\r\n",
    "\r\n",
    "1. **Model Selection:** KL Divergence can be used as a criterion for model selection. The model that produces a probability distribution closest to the true distribution (i.e., with the smallest KL Divergence) is chosen.\r\n",
    "\r\n",
    "2. **Optimization of Probabilistic Models:** In probabilistic models like Variational Autoencoders (VAEs), KL Divergence is used in the loss function to measure the difference between the learned distribution and the prior distribution.\r\n",
    "\r\n",
    "3. **Reinforcement Learning:** In policy optimization methods of reinforcement learning, KL Divergence is used to ensure that the updated policy does not deviate too much from the old policy.\r\n",
    "\r\n",
    "4. **Information Retrieval:** In information retrieval, KL Divergence can be used to measure the divergence between the document language model and the query language model. The document with the smallest divergence is considered the most relevant to the query.\r\n",
    "\r\n",
    "5. **Natural Language Processing:** In NLP, KL Divergence can be used to measure the similarity between two text documents. It can be used in tasks like text classification, clustering, and topic modeling.\r\n",
    "\r\n",
    "Next, let's see a simple example of how KL Divergence can be calculated in Python using the SciPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "417bfa23-0973-4ec6-9fd6-b701cf076db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03068528, 0.        , 0.00790548])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import kl_div\n",
    "import numpy as np\n",
    "\n",
    "# Define two probability distributions\n",
    "P = np.array([0.1, 0.2, 0.7])\n",
    "Q = np.array([0.2, 0.2, 0.6])\n",
    "\n",
    "# Calculate KL Divergence\n",
    "kl_divergence = kl_div(P, Q)\n",
    "print(f\"kl_divergence: {kl_divergence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df9fb1c-36aa-4481-86cd-187b6c2d12dd",
   "metadata": {},
   "source": [
    "The KL Divergence between the two probability distributions P and Q for each corresponding pair of probabilities is calculated as follows:\n",
    "\r\n",
    "\r\n",
    "- For the first pair of probabilities (0.1 and 0.2), the KL Divergence is approximately 0.031.\r\n",
    "- For the second pair of probabilities (0.2 and 0.2), the KL Divergence is 0. This is because the two probabilities are equal, so there is no divergence.\r\n",
    "- For the third pair of probabilities (0.7 and 0.6), the KL Divergence is approximately 0.008.\r\n",
    "\r\n",
    "This shows that the KL Divergence can effectively measure the difference between two probability distributions. In machine learning, minimizing the KL Divergence can help in improving the model's performance by making the predicted probability distribution closer to the true distribution.tion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a805b470-3cd3-4aa8-b6ac-99c74050b7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence: 0.09695352463929668\n",
      "Euclidean Distance: 0.2449489742783178\n",
      "Correlation Coefficient: 0.5765566601970551\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.special import kl_div\n",
    "\n",
    "# Example probability distributions\n",
    "p = np.array([0.2, 0.3, 0.5])\n",
    "q = np.array([0.1, 0.5, 0.4])\n",
    "\n",
    "# Compute KL divergence\n",
    "kl_divergence = kl_div(p, q).sum()\n",
    "\n",
    "# Compute Euclidean distance\n",
    "euclidean_distance = euclidean(p, q)\n",
    "\n",
    "# Compute correlation coefficient\n",
    "correlation_coefficient, _ = pearsonr(p, q)\n",
    "\n",
    "# Print the results\n",
    "print(\"KL Divergence:\", kl_divergence)\n",
    "print(\"Euclidean Distance:\", euclidean_distance)\n",
    "print(\"Correlation Coefficient:\", correlation_coefficient)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
