{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3899cbd2-dbb5-42fe-b409-45551806aa6b",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "# Comparison of One-Hot Encoding and Embedding Layer\n",
    "\n",
    "One-hot encoding and embedding layers are two different techniques used for handling categorical data in machine learning models. In this notebook, we will discuss each of them in detail, along with their pros and cons, and the scenarios where they are most suitable.\n",
    "\n",
    "## One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a process of converting categorical data into a binary vector representation. Each category is mapped to a vector that contains 1 in the position of the category and 0 in all other positions.\n",
    "\n",
    "For example, if we have a feature \"color\" with categories \"red\", \"green\", and \"blue\", the one-hot encoding would look like this:\n",
    "\n",
    "- red: [1, 0, 0]\n",
    "- green: [0, 1, 0]\n",
    "- blue: [0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b83ce-2a3b-4d08-95de-b5009c7cfbcd",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "### Pros of One-Hot Encoding:\n",
    "\n",
    "1. It is a straightforward and easy-to-understand method.\n",
    "2. It is efficient for categorical variables with a few categories.\n",
    "\n",
    "### Cons of One-Hot Encoding:\n",
    "\n",
    "1. It can lead to high memory consumption when dealing with variables with many categories.\n",
    "2. It does not capture any relationship between categories.\n",
    "3. It can lead to the \"curse of dimensionality\", i.e., with each unique category, a new dimension is added, which can negatively impact the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7ae9f-4008-4fe4-84c8-8563c008d344",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## Embedding Layer\n",
    "\n",
    "An embedding layer is a part of neural networks designed to handle categorical data. It maps each category to a dense vector of real numbers (also known as an embedding vector). The key idea here is that similar categories will have similar vectors in the embedding space.\n",
    "\n",
    "### Pros of Embedding Layer:\n",
    "\n",
    "1. It reduces the dimensionality of categorical variables, which can be beneficial when dealing with variables with many categories.\n",
    "2. It can capture relationships between different categories.\n",
    "3. The embeddings are learned during the training process, which allows the model to learn the optimal representation of the categories for the given task.\n",
    "\n",
    "### Cons of Embedding Layer:\n",
    "\n",
    "1. It is more complex and computationally intensive than one-hot encoding.\n",
    "2. The resulting embeddings can be hard to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebba5495-5e03-476d-9ffe-391b4bbde7f3",
   "metadata": {
    "noteable": {
     "cell_type": "markdown"
    }
   },
   "source": [
    "## When to Use Which?\n",
    "\n",
    "- **One-hot encoding** is suitable for categorical variables with a few categories and when there is no need to capture any relationship between categories. It is also a good choice when using algorithms that do not support categorical data natively, like linear regression or logistic regression.\n",
    "\n",
    "- **Embedding layers** are suitable for categorical variables with many categories or when there is a need to capture relationships between categories. They are typically used in deep learning models, like neural networks, where they can be trained as part of the model.\n",
    "\n",
    "### Example\n",
    "\n",
    "If you are working with a feature like \"city\" that can take thousands of unique values, using one-hot encoding would result in a very high-dimensional vector. In this case, using an embedding layer would be more efficient. On the other hand, for a feature like \"color\" with only a few unique values, one-hot encoding would be a simpler and effective choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "noteable": {
   "last_transaction_id": "d140fc71-8b7c-420b-8abc-dbb6c96f72f1"
  },
  "noteable-chatgpt": {
   "create_notebook": {
    "openai_conversation_id": "abdeaea7-9a6a-5dc7-8ec3-19c97d8eedf3",
    "openai_ephemeral_user_id": "f014247b-7c45-5b90-9100-6b81ec869142",
    "openai_subdivision1_iso_code": "KR-49"
   }
  },
  "selected_hardware_size": "small"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
