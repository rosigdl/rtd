{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "946222d2",
   "metadata": {},
   "source": [
    "# Ch 3 Geometric Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b91e9",
   "metadata": {},
   "source": [
    ">Sources\n",
    ">\n",
    ">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges https://arxiv.org/abs/2104.13478\n",
    ">\n",
    ">AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 3 (Geometric Priors I) - Taco Cohen https://youtu.be/qEjWMhRlXgY\n",
    ">\n",
    "> AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 4 (Geometric Priors II) - Joan Bruna https://youtu.be/DpnA8NNUtyU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96885ecc",
   "metadata": {},
   "source": [
    "대칭(symmetry)과 스케일 분리(scale separation)라는 두 가지 기본 원칙을 사용할 수 있는 물리적 구조(physically-structured) 데이터에 대해서는 희망이 있습니다\n",
    "\n",
    "학습 시스템은 어떤 도메인 Ω의 신호(함수)에 대해 작동한다고 가정합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c84a81e8",
   "metadata": {},
   "source": [
    "많은 경우 Ω에 있는 점의 선형 조합은 잘 정의되지 않지만 신호를 선형으로 결합할 수 있습니다. 즉, 신호 공간이 벡터 공간을 형성합니다. 또한 신호 사이의 내적을 정의할 수 있으므로 이 공간은 Hilbert 공간입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c3265b4",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/_kJUUxjJ_FY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "<br>\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yckiapQlruY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "<br>\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/g-eNeXlZKAQ\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93175e66",
   "metadata": {},
   "source": [
    "<div style=\"background-color:Gainsboro\">\n",
    "Ω위에서 C 값 신호의 공간에 대하여(Ω의 경우 추가 구조가 있을 수 있는 집합, C의 경우 차원을 채널이라고 하는 벡터 공간)\n",
    "\n",
    "$$\n",
    "   \\mathcal{X}(\\Omega, \\mathcal{C})  = \\{x:\\Omega \\rightarrow \\mathcal{C} \\} \\tag{1}\n",
    "$$\n",
    "\n",
    "는 벡터 공간 구조를 갖는 함수 공간입니다. 신호의 덧셈과 스칼라 곱셈은 다음과 같이 정의됩니다.\n",
    "\n",
    "$$\n",
    "(\\alpha x + \\beta y)(u) = \\alpha x (u) + \\beta y(u) \\quad \\text{for all }\\quad u \\in \\Omega\n",
    "$$\n",
    "<br>\n",
    "\n",
    "실수 스칼라 α, β에 대하여. 내적 및 Ω에 있는 μ에 대한 측정(이와 관련하여 적분을 정의할 수 있음)가 주어지면 X(Ω, C)에 대한 내적을 다음과 같이 정의할 수 있습니다.\n",
    "\n",
    "$$\n",
    "   \\langle x,y \\rangle = \\int_{\\Omega}\\langle x (u),y(u) \\rangle_{\\mathcal{C}} \\: d\\mu (u) \\tag{2}\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dcdc4711",
   "metadata": {},
   "source": [
    "Ω에 추가 구조가 있는 경우, X(Ω, C)의 신호 종류를 제한할 수 있습니다. 예를 들어 Ω이 평활 다양체(smooth manifold)일 때, 신호가 평활(smooth)해야 됨을 요구할 수 있습니다. 가능하면 간결함을 위해 범위 C를 생략합니다.\n",
    "\n",
    "도메인 Ω이 이산(discrete)적일 때 μ는 계수 측정(counting measure)으로 선택될 수 있으며, 이 경우 적분은 합이 됩니다. 다음에서는 측정을 생략하고 간결함을 위해 du를 사용합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "797ccbad",
   "metadata": {},
   "source": [
    "스케일 분리는 신호를 더 거친(coarser) 버전의 도메인으로 전송할 때 신호의 중요한 특성을 보존하는 능력에서 비롯됩니다(이 예에서는 기본 그리드를 거칠게 하여 이미지를 서브샘플링하는 경우)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3f5b01a",
   "metadata": {},
   "source": [
    "위에서 고려한 이미지의 경우 기하학적 사전은 공유 가중치(변환 대칭-translational symmetry 이용) 및 풀링(척도 분리-scale separation 이용)이 있는 컨볼루션 필터의 형태로 컨볼루션 신경망(CNN)에 구축됩니다.\n",
    "\n",
    "이러한 아이디어를 그래프 및 다양체와 같은 다른 영역으로 확장하고 기본 원리에서 기하학적 사전이 어떻게 나타나는지 보여주는 것이 기하학적 딥 러닝의 주요 목표이자 텍스트의 주제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabbbbe",
   "metadata": {},
   "source": [
    "## 3.1 Symmetries, Representations, and Invariance\n",
    "3.1 대칭, 표현 및 불변성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d01325fd",
   "metadata": {},
   "source": [
    "편하게 말하자면, 개체 또는 시스템의 대칭은 해당 개체 또는 시스템의 특정 속성을 변경하지 않거나 불변으로 남겨두는 변형입니다. 이러한 변환은 매끄럽거나 연속적이거나 불연속적일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51a8d5",
   "metadata": {},
   "source": [
    "### Symmetry groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12454bc1",
   "metadata": {},
   "source": [
    "그룹 이론에서 사용되는 병치 표기법 $\\mathfrak{g ◦ h = gh}$를 따르며 오른쪽에서 왼쪽으로 읽어야 합니다. 먼저 $\\mathfrak{h}$를 적용한 다음 $\\mathfrak{g}$를 적용합니다. 많은 경우 대칭이 비가환적이기 때문에 순서가 중요합니다.\n",
    "\n",
    "Fraktur 글꼴은 그룹 요소를 나타냅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698beb36",
   "metadata": {},
   "source": [
    "#### group axioms\n",
    "<div style=\"background-color:Gainsboro\">\n",
    "A group is a set G along with a binary operation $\\mathfrak{◦ : G × G → G}$ called composition (for brevity, denoted by juxtaposition g ◦ h = gh) satisfying the following axioms:\n",
    "\n",
    "Associativity: $\\mathfrak{(gh)k = g(hk)} \\text{ for all } \\mathfrak{g, h,k ∈ G}$\n",
    "\n",
    "Identity: there exists a unique $\\mathfrak{e ∈ G}$ satisfying $\\mathfrak{eg = ge = g}$ for all $\\mathfrak{g ∈ G}$.\n",
    "\n",
    "Inverse: For each $\\mathfrak{g ∈ G}$ there is a unique inverse $\\mathfrak{g\n",
    "^{−1} ∈ G}$ such that $\\mathfrak{gg^{−1} = g^{−1}g = e}$.\n",
    "\n",
    "Closure: The group is closed under composition, i.e., for every $\\mathfrak{g, h ∈ G}$, we have $\\mathfrak{gh ∈ G}$.\n",
    "</div>\n",
    "그룹은 이진 연산 ◦ : G × G → G 라고 하는 구성(간결함을 위해 g ◦ h = gh로 병치로 표시)과 함께 다음 공리를 충족하는 집합 G입니다.\n",
    "\n",
    "연관성\n",
    "동일성\n",
    "역\n",
    "폐쇄"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "826b1dea",
   "metadata": {},
   "source": [
    "교환성은 이 정의의 일부가 아닙니다. 즉, $\\mathfrak{gh \\not = hg}$ 일 수 있습니다. 모든 $\\mathfrak{g, h ∈ G}$ 에 대해 $\\mathfrak{gh = hg}$ 인 그룹을 가환성/교환성 또는 Abelian이라고 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "266407de",
   "metadata": {},
   "source": [
    "공식적으로, $\\mathfrak{G}$는 모든 요소 $\\mathfrak{g ∈ G}$가 $\\mathcal{S}$ 의 요소와 그 역원의 유한 구성으로 쓰여질 수 있는 경우 부분 집합 $\\mathcal{S} ⊆ \\mathfrak{G}$(그룹 생성기라고 함)에 의해 생성된다고 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cf67df4",
   "metadata": {},
   "source": [
    "여기서 우리는 그룹 요소가 무엇인지(예: 일부 도메인의 변환) 구성 요소가 무엇인지는 말하지 않고, 오직 어떻게 그것들이 구성되는지에 대해서만,추상 객체로 그룹을 정의했습니다. 따라서 매우 다른 종류의 객체가 동일한 대칭 그룹을 가질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd2ccf",
   "metadata": {},
   "source": [
    "### Group Actions and Group Representations\n",
    "\n",
    "그룹 작업 및 그룹 표현"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4bc28de",
   "metadata": {},
   "source": [
    "그룹을 추상적인 엔터티로 간주하기보다는, 그룹이 데이터에 대해 어떻게 작용하는지에 주로 관심이 있습니다. 데이터의 기저에 도메인 Ω이 있다고 가정했기 때문에 그룹이 Ω에 대해 어떻게 작용하는지 연구하고(예: 평면 점의 변환) 거기에서 신호 $\\mathcal{X (\\Omega)}$의 공간에서 동일한 그룹의 작용을 얻습니다. (예: 평면 이미지 및 기능 맵의 변환).\n",
    "\n",
    "집합(set) $\\Omega$에 대한 $\\mathfrak{G}$의 그룹 작업은 그룹 요소 $\\mathfrak{g ∈ G}$와 점 $u ∈ \\Omega$을 그룹 작업과 호환되는 방식으로 Ω의 다른 점과 연결하는 매핑 $(\\mathfrak{g}, u) \\mapsto \\mathfrak{g} \\cdot u$ 로 정의됩니다. 즉 $\\mathfrak{g \\cdot (h \\cdot} u) = \\mathfrak{(gh)} \\cdot u \\text{ for all } \\mathfrak{g, h ∈ G} \\text{ and } u ∈ \\Omega.$.\n",
    "\n",
    "다음 섹션에서 그룹 작업의 수많은 사례를 볼 것입니다. 예를 들어 평면에서 유클리드 그룹 $E(2)$는 유클리드 거리를 유지하고 변환, 회전 및 반사로 구성된 $\\mathbb{R^{2}}$ 의 변환 그룹입니다. 그러나 동일한 그룹은 평면의 이미지 공간(픽셀 격자를 변환, 회전 및 뒤집기)과 신경망에서 학습한 표현 공간에도 작용할 수 있습니다. 보다 정확하게는, Ω에서 작용하는 그룹 $\\mathfrak{G}$가 있는 경우, $\\mathcal{X}(\\Omega)$에서 G의 작용을 자동으로 얻습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53844a7e",
   "metadata": {},
   "source": [
    "$$\n",
    "    (\\mathfrak{g} . x )(u) = x ( \\mathfrak{g^{-1}}u ) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9264feb",
   "metadata": {},
   "source": [
    "g의 역으로 인해,  이것은 실제로 유효한 그룹 작업입니다,  그리고 우리는 다음을 가집니다. $(\\mathfrak{g}.(\\mathfrak{h}.x))(u) = ((\\mathfrak{gh}).x)(u)$.\n",
    "\n",
    "이 텍스트 전체에서 반복적으로 접하게 될 가장 중요한 종류의 그룹 작업은 그룹 표현(group representations)이라고도 하는 선형 그룹 작업입니다. 방정식 (3)에서 신호에 대한 작용은 다음과 같은 의미에서 실제로 선형입니다.\n",
    "\n",
    "$$\n",
    "\\mathfrak{g}.(\\alpha x + \\beta \\acute{x}\n",
    ") = \\alpha(\\mathfrak{g}.x) + \\beta(\\mathfrak{g}.\\acute{x}\n",
    ")\n",
    "$$\n",
    "\n",
    "위 식은 모든 스칼라 $\\alpha, \\beta$ 및 신호 $x, \\acute{x} ∈ X (\\Omega)$ 에 대해 정의 됩니다. 선형 작용을 x에서 선형인 맵 $\\left(\\mathfrak{g}, x\\right) \\mapsto \\mathfrak{g}.x$ 로 설명하거나, 동등하게, 각 그룹 요소 g에 (가역) 행렬 $\\rho(\\mathfrak{g})$를 할당하는 맵 $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ 으로 설명할 수 있습니다. 행렬의 차원 n은 일반적으로 임의적이며, 그룹의 차원이나 Ω의 차원과 반드시 관련이 있는 것은 아니지만, 딥 러닝에 적용할 때 n은 일반적으로 그룹이 작용하는 특징 공간의 차원입니다. 예를 들어, n 픽셀의 이미지 공간에 작용하는 2D 번역 그룹이 있을 수 있습니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f32305a2",
   "metadata": {},
   "source": [
    "그룹 $\\mathfrak{G}$의 n차원 실제 표현(real *representation*)은 맵 $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ 입니다. 각 $\\mathfrak{g ∈ G}$에 가역(invertible)행렬 $\\rho(\\mathfrak{g})$를 할당하고, 모든 $\\mathfrak{g, h ∈ G}$ 에 대해, 조건 $\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$를 충족합니다. 행렬 $\\rho(\\mathfrak{g})$가 모든 $\\mathfrak{g ∈ G}$에 대해 단일 또는 직교인 경우, 표현을 단일(unitary) 또는 직교(orthogonal)라고 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7c53e16",
   "metadata": {},
   "source": [
    "그룹 표현의 언어로 작성된 신호 $x ∈ \\mathcal{X}(\\Omega)$에 대한 G의 작용은 $\\rho(\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}u)$ 로 정의됩니다. 다음을 다시 확인합니다.\n",
    "\n",
    "$$\n",
    "(\\rho(\\mathfrak{g})(\\rho(\\mathfrak{h})x))(u) = (\\rho(\\mathfrak{gh})x(u)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78ac19",
   "metadata": {},
   "source": [
    "### Invariant and Equivariant functions\n",
    "불변 및 등가 함수"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0e1b4e9",
   "metadata": {},
   "source": [
    "신호 X(Ω)의 기저에 있는 도메인 Ω의 대칭은 이러한 신호에 정의된 함수 f에 구조를 부과합니다. 이는 강력한 귀납적 편향으로 판명되어 가능한 보간법 F(X(Ω))의 공간을 대칭 사전을 만족하는 것으로 줄임으로써 학습 효율성을 향상시킵니다. 이 텍스트에서 탐구할 두 가지 중요한 경우는 불변(invariant) 및 등변(equivariant) 함수입니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59a3c7f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{Y}$ is $\\mathfrak{G}$-*invariant* if $f(\\rho(\\mathfrak{g})x) = f(x)$ for all $\\mathfrak{g ∈ G}$ and $x ∈ \\mathcal{X} (\\Omega)$, i.e., its output is unaffected by the group action on the input.\n",
    "\n",
    "함수 $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{Y}$ 는 모든 $\\mathfrak{g ∈ G}$ 와 $x ∈ \\mathcal{X} (\\Omega)$에 대해 $f(\\rho(\\mathfrak{g})x) = f(x)$의 경우 $\\mathfrak{G}$-불변입니다. 즉, 출력이 입력에 대한 그룹 행동의 영향을 받지 않습니다.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c45cd6a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{X} (Ω)$ is $\\mathfrak{G}$-*equivariant* if $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x)$ for all $\\mathfrak{g ∈ G}$, i.e., group action on the input affects the output in the same way.\n",
    "\n",
    "함수 $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{X} (Ω)$는 모든 $\\mathfrak{g ∈ G}$에 대해 $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x)$인 경우 $\\mathfrak{G}$-등변량(G-equivariant)입니다. 즉, 입력에 대한 그룹 작업은 동일한 방식으로 출력에 영향을 줍니다.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c123db12",
   "metadata": {},
   "source": [
    "보다 일반적으로, 서로 다른 영역 $Ω, Ω'$ 및 표현 $ρ, ρ'$을 동일한 그룹 $\\mathfrak{G}$의 입력 및 출력 공간으로 사용하여 $f : \\mathcal{X} (Ω) → \\mathcal{X} (Ω')$를 가질 수 있습니다. 이 경우 등분산은 $f(ρ(g)x) = ρ'(g)f(x)$로 정의됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0364c4a4",
   "metadata": {},
   "source": [
    "그러나 이미지 분류의 이전 사용 사례조차도 일반적으로 일련의 컨볼루션(shift-equivariant) 레이어로 구현된 다음 전역 풀링(shift-invariant)이 뒤따릅니다. 섹션 3.5에서 볼 수 있듯이 이것은 CNN 및 GNN(그래프 신경망)을 포함한 대다수 딥 러닝 아키텍처의 일반적인 청사진입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90366337",
   "metadata": {},
   "source": [
    "## 3.2 Isomorphisms and Automorphisms \n",
    "동형과 자기동형"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4d607a9",
   "metadata": {},
   "source": [
    "### Subgroups and Levels of structure\n",
    "\n",
    "서로 다른 객체 사이의 가역적이고(Invertible) 구조를 보존하는 지도(map)는 종종 동형학(isomorphisms 그리스어'동일한 모양') 이라고 불림니다. 물체에서 그 자체로의 동형(isomorphisms)을 자동형(automorphism) 또는 대칭이라고 합니다.\n",
    "\n",
    "bijections(전단사): 일대일 대응(bijection, one-to-one correspondence)\n",
    "\n",
    "유한 집합(finite set)의 경우 카디널리티(cardinality)는 집합의 요소 수('크기')이고, 무한 집합의 경우 카디널리티(cardinality)는 다양한 종류의 무한대를 나타냅니다. 예를 들어 자연수의 셀 수 있는 무한대 또는 실수 연속체의 셀 수 없는 무한대와 같이.\n",
    "\n",
    "모든 미분 가능한 함수는 연속적입니다. 맵이 '충분히 여러 번' 연속 미분 가능하면 평활('smooth')하다고 합니다.\n",
    "\n",
    "homeomorphisms(유사형): if Ω is a topological space, maps that preserve continuity and in addition to simple bijections between sets, are also continuous and have continuous inverse.\n",
    "Ω이 위상 공간인 경우 연속성을 유지하고, 세트간의 단순 전단사(bijection) 외에도, 연속적이며, 연속적인 가역성을 가지는 맵.\n",
    "\n",
    "맵 $τ (u) = u$는 항등(identity) 요소입니다, 그리고 모든 $τ$에 대해 역함수는 정의에 의해 존재하며 $(τ ◦ τ^{−1})(u) = (τ^{−1} ◦ τ )(u) = u$를 충족합니다.\n",
    "\n",
    "diffeomorphisms(미분변형) and denoted by Diff($Ω$): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77aadb13",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A metric or distance is a function $d : Ω × Ω → [0, ∞)$ satisfying for all\n",
    "$u, v, w ∈ Ω$:\n",
    "\n",
    "도메인 $Ω$에 속하는 모든 $u, v, w$에 대하여 미터(metric) 또는 거리(distance)는 함수로써 $Ω × Ω$를 $0 \\leq$ 이고 $< ∞$은 영역에 매핑합니다.  \n",
    "    \n",
    ">Identity of indiscernibles: $d(u, v) = 0 \\text{ iff } u = v$.\n",
    ">\n",
    ">식별할 수 없는 항목의 동일성: $u = v$ 인 경우에만 $d(u, v) = 0$을 성립힌다. \n",
    "    \n",
    ">Symmetry: $d(u, v) = d(v, u)$.\n",
    ">\n",
    ">대칭\n",
    "\n",
    ">Triangle inequality: $d(u, v) ≤ d(u, w) + d(w, v)$.\n",
    "> \n",
    ">삼각 부등식\n",
    "\n",
    "A space equipped with a metric $(Ω, d)$ is called a metric space.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e720395",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "$(\\mathfrak{G}, ◦)$를 그룹이라고 하고 $\\mathfrak{H ⊆ G}$를 부분집합이라고 하자. (H, ◦)가 동일한 연산을 갖는 그룹을 구성하는 경우, $\\mathfrak{H}$는 $\\mathfrak{G}$의 하위 그룹이라고 합니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0103ed5",
   "metadata": {},
   "source": [
    "### Isomorphisms and Automorphisms\n",
    "동형사상(Isomorphisms)과 자동사상(Automorphisms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "732bbcab",
   "metadata": {},
   "source": [
    "대칭은 구조를 보존하고 객체에서 객체 자체로의 가역가능한(invertible) 맵으로 설명했습니다. 이러한 맵은 자동형성(automorphisms)이라고도 하며, 객체가 그 자체로 동등한 방식을 묘사합니다(describe a way in which an object is equivalent it itself). 그러나 그에 못지않게 중요한 맵 유형은 동형 맵(isomorphisms)으로, 동일하지 않은 두 객체 간에 동등성을 나타내는 맵입니다(an equivalence between two nonidentical objects). 이러한 개념은 종종 혼동되는 경우가 많지만, 명확하게 구분하는 것은 구분하는 것은 다음 논의를 명확하게 하기 위해 필요합니다.\n",
    "\n",
    "차이점을 이해하기 위해 $Ω = \\left\\{0, 1, 2\\right\\}$ 집합을 생각해 보겠습니다. 집합 $Ω$의 자동변환은 순환 이동 $τ (u) = u+ 1 mod 3$과 같은 $τ : Ω → Ω$의 바이제이션입니다.\n",
    "이러한 맵은 카디널리티 속성을 보존하고 $Ω$을 그 자체에 매핑합니다. 만약 원소 수가 같은 다른 집합 $Ω' = \\left\\{a, b, c\\right\\}$가 있다면, $η : Ω → Ω'$ $η(0) = a, η(1) = b, η(2) = c$ 와 같은 바이짓(bijection)은 집합 동형(isomorphism)입니다.\n",
    "\n",
    "그래프에 대한 4.1절에서 살펴보겠지만, 구조의 개념에는 노드의 수뿐만 아니라 연결성 또한 포함됩니다. 두 그래프 $𝐺=(𝑉,𝐸)$ 와 $𝐺′=(𝑉′,𝐸′)$ 사이의 동형 $η:𝑉→𝑉′$ 는 연결된 노드 쌍을 연결된 노드 쌍에 매핑하는 노드 간의 바이제션(bijection)이며, 연결되지 않은 노드 쌍에 대해서도 마찬가지입니다. 따라서 두 개의 동형(isomorphic) 그래프는 구조적으로 동일하며 노드의 순서만 다를 뿐입니다. 반면에 그래프 자동변형(automorphism) 또는 대칭은 맵 $τ:𝑉→𝑉$ 는 연결성을 유지하면서 그래프의 노드를 다시 자기 자신으로 매핑합니다. 사소한 오토모피즘(automorphism)이 아닌 그래프(즉, $τ \\neq id$)를 가진 그래프는 대칭성을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae07ae",
   "metadata": {},
   "source": [
    "## 3.3 Deformation Stability\n",
    "\n",
    "변형 안정성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc491cbe",
   "metadata": {},
   "source": [
    "we are more interested in a far larger set of transformations where global, exact invariance is replaced by a local, inexact one. In our discussion, we will distinguish between two scenarios: the setting where the domain $Ω$ is fixed, and signals $x ∈ \\mathcal{X} (Ω)$ are undergoing deformations, and the setting where the domain $Ω$ itself may be deformed.\n",
    "\n",
    "우리는 전역(global)의 정확한 불변성이 국지적인 부정확한 불변성으로 대체되는 훨씬 더 큰 변환 집합에 더 관심이 있습니다. 여기서는 도메인 Ω이 고정되어 있고 신호 $x ∈ \\mathcal{X} (Ω)$가 변형되는 설정과 도메인 $Ω$ 자체가 변형될 수 있는 설정의 두 가지 시나리오를 구분해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b81e92",
   "metadata": {},
   "source": [
    "### Stability to signal deformations\n",
    "신호 변형에 대한 안정성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c2a9b60",
   "metadata": {},
   "source": [
    "많은 애플리케이션에서 신호 $x$의 작은 변형이 $𝑓(𝑥)$의 출력을 변경해서는 안 된다는 것을 알고 있으므로, 이러한 변형을 대칭으로 간주하고 싶어집니다.\n",
    "\n",
    "더 나은 접근 방식은 주어진 $τ ∈ Diff(Ω)$ 가 주어진 대칭 하위 그룹 $G ⊂ Diff(Ω)$(예: 변환)에서 얼마나 멀리 떨어져 있는지를 복잡도 측정값 $𝑐(τ)$ 를 사용하여 정량화하는 것입니다. 이를 사용하여 $τ∈𝐺$인 경우 $𝑐(τ)=0$ 이 되도록 합니다. 이제 그룹 작용 하에서 정확한 불변성과 등가성에 대한 이전 정의를 변형 안정성(deformation stability)(또는 근사 불변성 approximate invariance)이라는 '더 부드러운(softer)' 개념으로 대체할 수 있습니다:\n",
    "\n",
    "$$\n",
    "    \\|f(ρ(τ )x) − f(x)\\| ≤ Cc(τ )\\|x\\|, , ∀x ∈ \\mathcal{X} (Ω) \\tag{4}\n",
    "$$\n",
    "\n",
    "여기서 $ρ(τ )x(u) = x(τ−1u)$ 이며, 여기서 C는 신호 $x$와 독립적인 상수입니다. $f ∈ F(X (Ω))$ 인 함수가 위의 방정식을 만족하는 경우 기하학적으로 안정적이라고 합니다. 다음 3.4절에서 이러한 함수의 예를 살펴보겠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb72394",
   "metadata": {},
   "source": [
    "$τ ∈ G$에서 $c(τ ) = 0$이므로, 이 정의는 위에서 정의한 G-분산 특성을 일반화합니다. 응용 프로그램에서의 유용성은 적절한 변형 비용을 도입하는 데 달려 있습니다. 연속 유클리드 평면 위에 정의된 이미지의 경우, $τ$의 '탄성', 즉 일정한 벡터 필드에 의한 변위와 얼마나 다른지를 측정하는 $c^{2}(τ) := \\int_{\\Omega}\\|∇τ (u)\\|^{2} du$가 널리 사용됩니다. 이 변형 비용은 사실 디리클레(Dirichlet) 에너지라고도 하는 규범으로, τ가 변환 그룹에서 얼마나 멀리 떨어져 있는지 정량화하는 데 사용할 수 있습니다.\n",
    "\n",
    "$Aut(Ω)$: The set automorphism group\n",
    "\n",
    "$:=$: equal by definition\n",
    "\n",
    "$∇$: Del or nabla. vector differential operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e04c",
   "metadata": {},
   "source": [
    "### Stability to domain deformations\n",
    "도메인 변형에 대한 안정성\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db8c8c4e",
   "metadata": {},
   "source": [
    "많은 애플리케이션에서 변형되는 대상은 신호가 아니라 기하학적 영역 $Ω$ 자체입니다. 그래프와 다양체(manifolds)를 다루는 애플리케이션의 대표적인 예로, 그래프는 약간 다른 사회적 관계를 포함하는 여러 시점의 소셜 네트워크를 모델링하거나(그래프를 따라), 다양체는 비강체(non-rigid) 변형을 겪는 3D 객체를 모델링할 수 있습니다. 이러한 변형은 다음과 같이 정량화할 수 있습니다. $\\mathcal{D}$가 가능한 모든 가변 영역의 공간(예: 모든 그래프의 공간 또는 리만 다양체의 공간)을 나타내는 경우,  $Ω, \\tilde{Ω} ∈ \\mathcal{D}$에 대해 $Ω$과 $\\tilde{Ω}$가 어떤 의미에서 동등한 경우 $d(Ω, \\tilde{Ω}) = 0$을 만족하는 적절한 메트릭('거리') $d(Ω, \\tilde{Ω})$를 정의할 수 있습니다: 예를 들어 그래프 편집 거리는 그래프가 동형일 때 사라지고, 측지 거리가 있는 리만 다양체(Riemannian manifolds) 사이의 그로모프하우스도르프(GromovHausdorff) 거리는 두 다양체가 등각일 때 사라집니다.\n",
    "\n",
    "이러한 도메인 간 거리의 일반적인 구성은 해당 구조가 가장 잘 보존되는 방식으로 도메인을 '정렬'하려는 역전 매핑 $η : Ω → \\tilde{Ω}$의 일부 제품군에 의존합니다. 예를 들어, 그래프나 리만 다양체(측지 거리가 있는 미터법 공간으로 간주됨)의 경우, 이러한 정렬은 쌍별 인접성 또는 거리 구조(각각 d와 $\\tilde{d}$)를 비교할 수 있습니다,\n",
    "\n",
    "$$\n",
    "d_\\mathcal{D}(Ω,\\tilde{Ω}) = \\inf_{η∈\\mathfrak{G}}\\|d − \\tilde{d}◦ (η × η) \\|\n",
    "$$\n",
    "\n",
    "여기서 $\\mathfrak{G}$는 사변형(bijections) 또는 아이소메트리(isometries)와 같은 동형들의 그룹(isomorphisms)이며, 규범은 곱 공간 $Ω × Ω$에 대해 정의됩니다. 즉, $Ω, \\tilde{Ω}$의 요소 사이의 거리는 내부 구조를 보존하는 모든 가능한 정렬을 고려하여 도메인 자체 사이의 거리로 '리프팅(lifted)'됩니다. 신호 $x ∈ X (Ω)$와 변형된 도메인 $\\tilde{Ω}$가 주어지면, 변형된 신호 $\\tilde{x} = x ◦ η^{−1} ∈ X (\\tilde{Ω})$를 고려할 수 있습니다.\n",
    "\n",
    "\n",
    "표기법을 약간 변형하여 $X (D) = {(X (Ω), Ω) : Ω ∈ \\mathcal{D}}$ 를 다양한 영역에 걸쳐 정의된 가능한 입력 신호의 앙상블로 정의합니다. 함수 $f : \\mathcal{X} (\\mathcal{D}) → \\mathcal{Y}$ 는 다음과 같은 경우 도메인 변형에 안정적입니다. 모든 $Ω, \\tilde{Ω} ∈ D$ 와  $x ∈ X (Ω)$에 대하여\n",
    "\n",
    "$$\n",
    "    \\|f(x, Ω) − f(\\tilde{x}, \\tilde{Ω}) \\|  \\leq C \\|x\\|d_{\\mathcal{D}}(Ω,\\tilde{Ω}) \\tag{5}\n",
    "$$\n",
    "\n",
    "이 안정성의 개념은 등각 변형(isometric deformations)이 중요한 역할을 하는 4.4-4.6절의 다양체의 맥락에서 논의할 것입니다. 또한, 도메인 변형에 대한 안정성은 신호 변형에 대한 안정성의 자연스러운 일반화이며, 후자를 체적 형태의 변형 측면에서 보면 알 수 있습니다 (Gama et al. (2019)).\n",
    "\n",
    "$\\inf$: Bound, 유계, 유한한 영역을 가지는 오브젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe679b",
   "metadata": {},
   "source": [
    "## 3.4 Scale Separation\n",
    "스케일 분리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9099b6c5",
   "metadata": {},
   "source": [
    "변형 안정성은 전역 대칭 선행 조건을 상당히 강화하지만, 비공식적으로 말하자면 도메인의 크기가 커짐에 따라 (4)를 존중하는 함수가 여전히 \"너무 많이\" 존재한다는 점에서 그 자체만으로는 차원의 저주를 극복하기에 충분하지 않습니다. 이 차원성의 저주를 극복하기 위한 핵심 인사이트는 물리적 작업의 멀티스케일 구조를 활용하는 것입니다. 멀티스케일 표현을 설명하기 전에 다음과 규모보다는 주파수에 의존하는 푸리에 변환 같은 주요 요소를 소개할 필요가 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd5ad09",
   "metadata": {},
   "source": [
    "### Fourier Transform and Global invariants\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/spUNpyF58BY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
    "\n",
    "푸리에 변환의 핵심적인 측면은 평활도(smoothness)나 전도도(conductance)와 같은 신호와 도메인의 전역 속성을 드러낸다는 점입니다. 이러한 전역적 동작은 변환과 같이 도메인의 전역적 대칭성이 있는 경우에는 편리하지만, 보다 일반적인 이형성(diffeomorphisms)을 연구하는 데는 적합하지 않습니다. 이를 위해서는 다음에 살펴볼 것처럼 공간 및 주파수 로컬라이제이션을 절충하는 표현이 필요합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b11f00de",
   "metadata": {},
   "source": [
    "### Multiscale representations\n",
    "다중 스케일 표현\n",
    "\n",
    "국소 불변성의 개념은 푸리에 주파수 기반 표현에서 웨이블릿과 같은 다중 스케일 분해 방법의 초석인 스케일 기반 표현으로 전환함으로써 명확하게 표현할 수 있습니다. 다중 스케일 방법의 핵심은 도메인 Ω에 정의된 함수를 공간과 주파수 모두에 국한된(localised) 기본(elementary) 함수로 분해하는 것입니다. 웨이블릿의 경우, 이는 변환 및 확장(translated and dilated) 필터(마더 웨이블릿) $ψ$를 상호 연관시켜 연속 웨이블릿 변환(continuous wavelet transform)이라고 하는 결합된 공간-주파수(spatio-frequency) 표현을 생성함으로써 달성됩니다.\n",
    "\n",
    "$$\n",
    "(W_ψx)(u, ξ) = ξ^{−1/2} \\int_{−∞}^{+∞}ψ \\left(\\frac{v − u}{ξ}\\right) x(v)dv\n",
    "$$\n",
    "\n",
    "변환 및 확장된 필터를 웨이브렛 원자라고 하며, 이 원자의 공간적 위치와 확장은 웨이브렛 변환의 좌표 $u$와 $ξ$에 해당합니다. 이러한 좌표는 일반적으로 이원적(dyadically)으로 샘플링되며 $(ξ = 2^{−j}$ 및 $u = 2^{−jk})$ 스케일 $j$와 함께 정의 됩니다. 다중 스케일 신호 표현은 조각별 평활도(smoothness)와 같은 전역 평활도 이상의 규칙성 속성을 캡처하는 데 중요한 이점을 가져다주므로 90년대에 신호 및 이미지 처리와 수치 분석에서 널리 사용되는 도구가 되었습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfc0898c",
   "metadata": {},
   "source": [
    "### Deformation stability of Multiscale representations\n",
    "다중 축척 표현의 변형 안정성\n",
    "\n",
    "푸리에 분해에 비해 멀티스케일 국소화 웨이블릿 분해의 이점은 기본 대칭 그룹 '근처'에 있는 작은 변형의 효과를 고려할 때 드러납니다. 이 중요한 개념을 유클리드 영역과 변환 그룹에서 설명해 보겠습니다. 푸리에 표현은 시프트 연산자(4.2절에서 자세히 살펴보겠지만 컨볼루션으로 생각할 수 있음)를 대각선으로 대칭화하기 때문에 변환 변환에 효율적인 표현입니다. 그러나 푸리에 분해는 고주파수 변형에서 불안정합니다. 이와 대조적으로 웨이블릿 분해는 이러한 경우에 안정적인 표현을 제공합니다.\n",
    "\n",
    "Skip\n",
    "\n",
    "결과적으로 이러한 푸리에 표현은 아무리 작은 변형에도 불안정합니다. 이러한 불안정성은 일반 영역과 비강체(non-rigid) 변환에서 나타나며, 4.4절에서 설명한 푸리에 변환의 자연스러운 확장을 사용한 3D 도형 분석에서 이러한 불안정성의 또 다른 예를 살펴볼 것입니다.\n",
    "\n",
    "웨이브렛은 이 문제에 대한 해결책을 제시하며 다중 축적(multiscale) 표현의 힘을 드러냅니다. 위의 예에서 웨이브렛 분해 $W_ψx$는 변형과 거의 등변수(*approximately equivariant*)임을 보여줄 수 있습니다(Mallat, 2012),\n",
    "\n",
    "즉, 주파수가 아닌 국소 필터를 사용하여 신호 정보를 스케일로 분해하면 전체적으로 불안정한 표현이 국지적으로 안정적인 특징의 집합으로 바뀝니다. 중요한 점은 서로 다른 스케일에서의 이러한 측정값은 아직 불변하지 않으며, 낮은 주파수로 갈수록 점진적으로 처리되어야 한다는 점으로, 이는 현대 신경망의 심층 구성 특성을 암시하며, 다음에 소개할 기하학적 딥러닝 청사진에 포착되어 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6195fe8",
   "metadata": {},
   "source": [
    "### Scale Separation Prior\n",
    "스케일 분리 사전\n",
    "\n",
    "In other words, while the target function $f$ might depend on complex long-range interactions between features over the whole domain, in locally-stable functions it is possible to *separate* the interactions across scales, by first focusing on localised interactions that are then propagated towards the coarse scales.\n",
    "\n",
    "데이터 도메인 Ω을 계층 구조 $Ω_1, . . . , Ω_J$ 로 거칠게 하는 것을 고려할 수 있습니다. 이러한 거칠기는 그리드, 그래프, 다양체 등 매우 일반적인 도메인에서 정의할 수 있습니다. 비공식적으로, 거칠기는 근처의 점 $u, u' ∈ Ω$을 함께 동화시키므로 도메인에서 적절한 메트릭 개념(notion of metric in the domain)만 필요로 합니다. $X_j (Ω_j , C_j ) := {x_j : Ω_j → C_j}$가 거칠어진 영역 $Ω_j$에 정의된 신호를 나타내는 경우, 비공식적으로 함수 $f : \\mathcal{X} (Ω) → \\mathcal{Y}$ 가 $f ≈ f_j ◦ P_j$ 형식의 인수분해를 허용하는 경우, 여기서 $P_j : \\mathcal{X} (Ω) → \\mathcal{X}_j (Ω_j)$는 비선형 거칠기이고 $f_j : \\mathcal{X}_j (Ω_j ) → \\mathcal{Y}$는 크기(scale) $j$에 국소적으로 안정하다고 말할 수 있습니다. 즉, 목표 함수 $f$는 전체 영역에 걸쳐 특징들 간의 복잡한 장거리 상호작용에 의존할 수 있지만, 국지적으로 안정적인 함수에서는 먼저 국지적인 상호작용에 초점을 맞춘 다음 거친 스케일로 전파함으로써 스케일 간 상호작용을 분리할 수 있습니다.\n",
    "\n",
    "이러한 원리는 물리학 및 수학의 많은 분야에서 근본적으로 중요하며, 예를 들어 일명 재정규화(renormalisation) 그룹으로 통계 물리학에서 나타나거나 고속 다중극 방법과 같은 중요한 수치 알고리즘에서 활용됩니다. 머신 러닝에서 멀티스케일 표현과 국소 불변성은 컨볼루션 신경망과 그래프 신경망의 효율성을 뒷받침하는 기본 수학적 원리이며, 일반적으로 로컬 풀링의 형태로 구현됩니다. 향후 작업에서는 이러한 원리를 기하학적 영역 전반에 걸쳐 통합하고 규모 분리(scale separation)의 통계적 학습 이점을 조명하는 계산 고조파 분석 도구를 추가로 개발할 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839f062",
   "metadata": {},
   "source": [
    "## 3.5 The Blueprint of Geometric Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f340825",
   "metadata": {},
   "source": [
    "3.1~3.4절에서 설명한 대칭, 기하학적 안정성, 스케일 분리의 기하학적 원리를 결합하여 고차원 데이터의 안정적인 표현을 학습하기 위한 보편적인 청사진을 제공할 수 있습니다. 이러한 표현은 대칭 그룹 $\\mathfrak{G}$가 부여된 도메인 Ω에 정의된 신호 $\\mathcal{X} (Ω, \\mathcal{C})$에 대해 작동하는 함수 $f$에 의해 생성됩니다. \n",
    "\n",
    "지금까지 설명한 기하학적 선행(priors)은 이러한 표현을 구축하기 위한 특정 아키텍처를 규정하는 것이 아니라 일련의 필수 조건을 규정합니다. 그러나 이러한 기하학적 선행 조건을 증명적으로 만족하는 공리적인 구조를 암시하는 동시에 이러한 선행 조건을 만족하는 모든 목표 함수에 근사치를 구할 수 있는 표현력이 뛰어난 표현을 보장합니다. \n",
    "\n",
    "간단한 첫 번째 관찰은 표현력이 높은 표현을 얻기 위해 비선형 요소를 도입해야 한다는 것입니다. f가 선형이고 G-변수인 경우 모든 $x ∈ \\mathcal{X} (Ω)$에 대해 비선형 요소를 도입해야 하기 때문입니다,\n",
    "\n",
    "<p>\n",
    "$$\n",
    "    f(x) = \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}} f(\\mathfrak{g}.x)dµ(\\mathfrak{g}) = f \\left( \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}} (\\mathfrak{g}.x)dµ(\\mathfrak{g}) \\right)\n",
    "$$\n",
    "</p>\n",
    "\n",
    ">여기서 $µ(g)$는 그룹 G의 하르(Haar) 측정값으로 알려져 있으며, 적분은 전체 그룹에 대해 수행됩니다.\n",
    "\n",
    "이는 $F$가 $\\mathfrak{G}$-평균 $Ax = \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}}(\\mathfrak{g}.x)dµ(\\mathfrak{g})$ 를 통해 x에만 의존한다는 것을 나타냅니다. 이미지 및 번역의 경우 입력의 평균 RGB 색상만 사용해야 합니다!\n",
    "\n",
    "이 추론은 선형 불변량군(linear invariantsis)이 그다지 풍부한 객체는 아니지만, 선형 등식군은 이제 설명하겠지만 적절한 비선형 맵과 함께 구성되어 더욱 풍부하고 안정적인 기능을 구성할 수 있기 때문에 훨씬 더 강력한 도구를 제공합니다. 실제로 $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω, \\mathcal{C'})$가 모든 $x ∈ \\mathcal{X}$와 $\\mathfrak{g} ∈ \\mathfrak{G}$에 대해 $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$를 만족하는 $\\mathfrak{G}$-등식(equivariant)이고 $σ : \\mathcal{C'} → \\mathcal{C''}$이 임의의 (비선형) 맵이라면, 우리는 쉽게 구성 $U := (σ ◦ B) : X (Ω, \\mathcal{C}) → X (Ω, \\mathcal{C''})$ 역시 $\\mathfrak{G}$-등식(equivariant)이며, 여기서 $σ : X (Ω,\\mathcal{C'} ) → X (Ω, \\mathcal{C''})$는 $(σ(x))(u) := σ(x(u))$로 주어진 $σ$의 원소별 인스턴스화입니다.\n",
    "\n",
    "이 간단한 속성을 통해 우리는 $U$를 그룹 평균 $A ◦ U : X (Ω, \\mathcal{C}) → \\mathcal{C''}$ 으로 구성하여 매우 일반적인 G-불변량 군을 정의할 수 있습니다. 따라서 자연스러운 질문은 $B$와 $σ$의 적절한 선택에 대해 이러한 모델에 의해 임의의 정밀도로 G-불변 함수를 근사화할 수 있는지 여부입니다. 구조화되지 않은 벡터 입력에서 표준 범용 근사 정리를 적용하여 그룹 평균을 일반 비선형 불변량으로 적절히 일반화함으로써 얕은 '기하학적' 네트워크도 범용 근사화자임을 보여주는 것은 어렵지 않습니다. 그러나 푸리에 불변수와 웨이블릿 불변수의 경우에서 이미 설명했듯이, 얕은 전역 불변성과 변형 안정성 사이에는 근본적인 긴장이 존재합니다. 따라서 국소화된 등변량 맵을 고려하는 대안적 표현이 필요합니다. $Ω$에 거리 메트릭 d가 추가로 장착되어 있다고 가정할 때, 작은 반지름 $r$에 대해 $N_u = {v : d(u, v) ≤ r}$에 대해 $(Ux)(u)$가 $x(v)$의 값에만 의존하는 경우 등변량 맵 U를 국소화된 맵이라고 하며, 후자의 집합 Nu를 수용장(receptive field)이라고 부릅니다.\n",
    "\n",
    ">의미 있는 메트릭은 그리드, 그래프, 다양체 및 그룹에 정의할 수 있습니다. 주목할 만한 예외는 메트릭에 대한 사전 정의된 개념이 없는 집합(set)입니다.\n",
    "\n",
    "단일 계층의 국부 등변량 맵 $U$는 장거리 상호작용이 있는 함수를 근사화할 수 없는 반면, 여러 국부 등변량 맵 $U_J ◦ U_J−1 · · · ◦ U_1$의 구성은 국부 등변량의 안정성 특성을 보존하면서 수용 필드(receptive field)를 증가시킵니다. 영역을 조악하게(coarsen) 하는 다운샘플링 연산자를 인터리빙하여(다시 메트릭 구조를 가정하여) 수용 필드를 더욱 증가시켜 다중해상도 분석(MRA, Mallat (1999) 참조)과 병렬로 완성합니다. \n",
    "\n",
    ">'수용 필드'라는 용어는 신경과학 문헌에서 유래한 것으로, 주어진 뉴런의 출력에 영향을 미치는 공간 영역을 의미합니다.\n",
    "\n",
    "In summary, the geometry of the input domain, with knowledge of an underyling symmetry group, provides three key building blocks: (i) a local equivariant map, (ii) a global invariant map, and (iii) a coarsening operator. These building blocks provide a rich function approximation space with prescribed invariance and stability properties by combining them together in a scheme we refer to as the Geometric Deep Learning Blueprint (Figure 8)\n",
    "\n",
    "요약하면, 언더링 대칭 그룹에 대한 지식이 있는 입력 영역의 기하학은 (i) 로컬 등변량 맵, (ii) 글로벌 불변량 맵, (iii) 조도(coarsening) 연산자라는 세 가지 주요 구성 요소를 제공합니다. 이러한 빌딩 블록은 기하학적 딥 러닝 블루프린트라고 하는 체계에서 함께 결합하여 규정된 불변성 및 안정성 속성을 갖춘 풍부한 함수 근사화 공간을 제공합니다(그림 8)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec7d4f15",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "Geometric Deep Learning Blueprint\n",
    "\n",
    "Let $Ω$ and $Ω'$ be domains, $\\mathfrak{G}$ a symmetry group over $Ω$, and write $Ω' ⊆ Ω$ if $Ω'$ can be considered a compact version of $Ω$.\n",
    "\n",
    "$Ω$과  $Ω'$을 영역으로, $\\mathfrak{G}$를 $Ω$에 대한 대칭 그룹으로 하고, $Ω' ⊆ Ω$ 이면 $Ω'$ 를  $Ω$의 압축 버전으로 간주할 수 있습니다.\n",
    "\n",
    "We define the following building blocks:\n",
    "\n",
    "다음과 같은 빌딩 블록을 정의합니다:\n",
    "\n",
    "\n",
    ">Linear $\\mathfrak{G}$-equivariant layer $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C'}) \\text{ satisfying } B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x) \\text{ for all } \\mathfrak{g} ∈ \\mathfrak{G} \\text{ and } x ∈ \\mathcal{X} (Ω, \\mathcal{C}).$\n",
    ">\n",
    ">선형 𝔊 -등변량 레이어 $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C'})$ 는 모든 $\\mathfrak{g} ∈ \\mathfrak{G}$ 및 $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$에 대해 $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$ 를 만족합니다.\n",
    "\n",
    ">Nonlinearity $σ : \\mathcal{C} → \\mathcal{C'}$ applied element-wise as $(σ(x))(u) = σ(x(u))$.\n",
    ">\n",
    ">비선형성 $σ : \\mathcal{C} → \\mathcal{C'}$는 $(σ(x))(u) = σ(x(u))$로 요소별로 적용됩니다\n",
    "\n",
    ">Local pooling (coarsening) $P : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C})$, such that $Ω' ⊆ Ω$.\n",
    ">\n",
    ">로컬 풀링(거칠게 함) $P : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C})$, 즉, $Ω' ⊆ Ω$\n",
    "\n",
    ">G-invariant layer (global pooling) $A : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{Y}$ satisfying $A(\\mathfrak{g}.x) = A(x)$ for all $\\mathfrak{g} ∈ \\mathfrak{G}$ and $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$.\n",
    ">\n",
    "> G-변수 레이어(글로벌 풀링) $A : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{Y}$는 모든 $\\mathfrak{g} ∈ \\mathfrak{G}$ 및 $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$에 대해 $A(\\mathfrak{g}.x) = A(x)$를 만족합니다.\n",
    "\n",
    "Using these blocks allows constructing G-invariant functions $f :\\mathfrak{X} (Ω, \\mathfrak{C}) → \\mathfrak{Y}$ of the form\n",
    "\n",
    "이러한 블록을 사용하면 다음과 같은 형식의 G-변수 함수 $f :\\mathfrak{X} (Ω, \\mathfrak{C}) → \\mathfrak{Y}$를 구성할 수 있습니다.\n",
    "\n",
    "$$\n",
    "    f = A ◦ σ_J ◦ B_J ◦ P_{J−1} ◦ . . . ◦ P_1 ◦ σ_1 ◦ B_1\n",
    "$$\n",
    "\n",
    "where the blocks are selected such that the output space of each block\n",
    "matches the input space of the next one. Different blocks may exploit\n",
    "different choices of symmetry groups $\\mathfrak{G}$.\n",
    "\n",
    "여기서 각 블록의 출력 공간이 다음 블록의 입력 공간과 일치하도록 블록이 선택됩니다. 다른 블록은 다른 대칭 그룹 𝔊을 고를 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37f4307f",
   "metadata": {},
   "source": [
    "### Different settings of Geometric Deep Learning\n",
    "\n",
    "기하학적 딥러닝의 다양한 설정\n",
    "\n",
    "도메인 Ω이 고정된 것으로 가정하고 해당 도메인에 정의된 다양한 입력 신호에만 관심이 있는 경우와 도메인에 정의된 신호와 함께 도메인이 입력의 일부인 경우의 설정을 구분할 수 있습니다. 전자의 전형적인 사례는 이미지가 고정된 도메인(그리드)에 정의되어 있다고 가정하는 컴퓨터 비전 애플리케이션에서 볼 수 있습니다. 그래프 분류는 그래프의 구조와 그래프에 정의된 신호(예: 노드 특징)가 모두 중요한 후자의 설정에 대한 예입니다. 다양한 도메인의 경우, 기하학적 안정성(Ω의 변형에 민감하지 않다는 의미에서)이 기하학적 딥 러닝 아키텍처에서 중요한 역할을 합니다.\n",
    "\n",
    "이 청사진은 다양한 기하학적 영역에서 사용할 수 있는 적절한 수준의 범용성을 갖추고 있습니다. 따라서 기하학적 딥러닝 방법마다 도메인, 대칭 그룹, 앞서 언급한 빌딩 블록의 구체적인 구현 세부 사항에 대한 선택이 다릅니다. 아래에서 살펴보겠지만, 현재 사용 중인 많은 종류의 딥러닝 아키텍처가 이 체계에 속하며, 따라서 일반적인 기하학적 원리에서 파생될 수 있습니다. \n",
    "\n",
    "다음 섹션(4.1-4.6)에서는 '5G' 에 초점을 맞춘 다양한 기하학적 영역에 대해 설명하고, 섹션 5.1-5.8에서는 이러한 영역에서 기하학적 딥러닝의 구체적인 구현을 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336294b9",
   "metadata": {},
   "source": [
    "| Architecture         | Domain Ω       | Symmetry group G                            |\n",
    "|----------------------|----------------|---------------------------------------------|\n",
    "| CNN                  | Grid           | Translation                                 |\n",
    "| Spherical CNN        | Sphere / SO(3)  | Rotation SO(3)                              |\n",
    "| Intrinsic / Mesh CNN | Manifold       | Isometry Iso(Ω) <br>   Gauge symmetry SO(2) |\n",
    "| GNN                  | Graph          | Permutation Σn                              |\n",
    "| Deep Sets            | Set            | Permutation Σn                              |\n",
    "| Transformer          | Complete Graph | Permutation Σn                              |\n",
    "| LSTM                 | 1D Grid        | Time warping                                |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
