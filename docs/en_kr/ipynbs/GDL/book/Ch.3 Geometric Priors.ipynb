{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946222d2",
   "metadata": {},
   "source": [
    "# Ch.3 Geometric Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b91e9",
   "metadata": {},
   "source": [
    ">Sources\n",
    ">\n",
    ">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges https://arxiv.org/abs/2104.13478\n",
    ">\n",
    ">AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 3 (Geometric Priors I) - Taco Cohen https://youtu.be/qEjWMhRlXgY\n",
    ">\n",
    "> AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 4 (Geometric Priors II) - Joan Bruna https://youtu.be/DpnA8NNUtyU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96885ecc",
   "metadata": {},
   "source": [
    "There is hope for physically-structured data, where we can employ two fundamental principles: *symmetry* and *scale separation*.\n",
    "\n",
    "대칭(symmetry)과 스케일 분리(scale separation)라는 두 가지 기본 원칙을 사용할 수 있는 물리적 구조 데이터에 대해서는 희망이 있습니다\n",
    "\n",
    "We will assume that our machine learning system operates on signals (functions) on some domain Ω.\n",
    "\n",
    "학습 시스템은 어떤 도메인 Ω의 신호(함수)에 대해 작동한다고 가정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a81e8",
   "metadata": {},
   "source": [
    "While in many cases linear combinations of points on Ω is not well-defined, we can linearly combine signals on it, i.e., the space of signals forms a vector space. Moreover, since we can define an inner product between signals, this space is a Hilbert space.\n",
    "\n",
    "많은 경우 Ω에 있는 점의 선형 조합은 잘 정의되지 않지만 신호를 선형으로 결합할 수 있습니다. 즉, 신호 공간이 벡터 공간을 형성합니다. 또한 신호 사이의 내적을 정의할 수 있으므로 이 공간은 Hilbert 공간입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c3265b4",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yckiapQlruY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93175e66",
   "metadata": {},
   "source": [
    "The space of C-valued signals on Ω (for Ω a set, possibly with additional\n",
    "structure, and C a vector space, whose dimensions are called channels)\n",
    "\n",
    "Ω위에서 C 값 신호의 공간에 대하여(Ω의 경우 추가 구조가 있을 수 있는 집합, C의 경우 차원을 채널이라고 하는 벡터 공간)\n",
    "\n",
    "$$\n",
    "   \\mathcal{X}(\\Omega, \\mathcal{C})  = \\{x:\\Omega \\rightarrow \\mathcal{C} \\} \n",
    "$$\n",
    "\n",
    "is a function space that has a vector space structure. Addition and scalar\n",
    "multiplication of signals is defined as:\n",
    "\n",
    "는 벡터 공간 구조를 갖는 함수 공간입니다. 신호의 덧셈과 스칼라 곱셈은 다음과 같이 정의됩니다.\n",
    "\n",
    "\n",
    "$$\n",
    "(\\alpha x + \\beta y)(u) = \\alpha x (u) + \\beta y(u) \\quad \\text{for all }\\quad u \\in \\Omega\n",
    "$$\n",
    "<br>\n",
    "\n",
    "with real scalars α, β. Given an inner product $\\langle v,w \\rangle_{\\mathcal{C}}$ on C and a measure µ on Ω (with respect to which we can define an integral), we can define\n",
    "an inner product on X (Ω, C) as\n",
    "\n",
    "실수 스칼라 α, β에 대하여. 내적 및 Ω에 있는 μ에 대한 측정(이와 관련하여 적분을 정의할 수 있음)가 주어지면 X(Ω, C)에 대한 내적을 다음과 같이 정의할 수 있습니다.\n",
    "\n",
    "$$\n",
    "   \\langle x,y \\rangle = \\int_{\\Omega}\\langle x (u),y(u) \\rangle_{\\mathcal{C}} \\: d\\mu (u)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc4711",
   "metadata": {},
   "source": [
    "When Ω has some additional structure, we may further restrict the kinds of signals in X (Ω, C). For example, when Ω is a smooth manifold, we may require the signals to be smooth. Whenever possible, we will omit the range C for brevity.\n",
    "\n",
    "Ω에 추가 구조가 있는 경우, X(Ω, C)의 신호 종류를 제한할 수 있습니다. 예를 들어 Ω이 평활 다양체(smooth manifold)일 때, 신호가 평활(smooth)해야 됨을 요구할 수 있습니다. 가능하면 간결함을 위해 범위 C를 생략합니다.\n",
    "\n",
    "When the domain Ω is discrete, µ can be chosen as the counting measure, in which case the integral becomes a sum. In the following, we will omit the measure and use du for brevity.\n",
    "\n",
    "도메인 Ω이 이산(discrete)적일 때 μ는 계수 측정(counting measure)으로 선택될 수 있으며, 이 경우 적분은 합이 됩니다. 다음에서는 측정을 생략하고 간결함을 위해 du를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ccbad",
   "metadata": {},
   "source": [
    "Scale separation results from our ability to preserve important characteristics of the signal when transferring it onto a coarser version of the domain (in our example, subsampling the image by coarsening the underlying grid).\n",
    "\n",
    "스케일 분리는 신호를 더 거친(coarser) 버전의 도메인으로 전송할 때 신호의 중요한 특성을 보존하는 능력에서 비롯됩니다(이 예에서는 기본 그리드를 거칠게 하여 이미지를 서브샘플링하는 경우)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5b01a",
   "metadata": {},
   "source": [
    "In the case of images considered above, geometric priors are built into Convolutional Neural Networks (CNNs) in the form of convolutional filters with\n",
    "shared weights (exploiting translational symmetry) and pooling (exploiting\n",
    "scale separation).\n",
    "\n",
    "위에서 고려한 이미지의 경우 기하학적 사전은 공유 가중치(변환 대칭-translational symmetry 이용) 및 풀링(척도 분리-scale separation 이용)이 있는 컨볼루션 필터의 형태로 컨볼루션 신경망(CNN)에 구축됩니다.\n",
    "\n",
    "Extending these ideas to other domains such as graphs and manifolds and showing how geometric priors emerge from fundamental principles is the main goal of Geometric Deep Learning and the *leitmotif* of our text.\n",
    "\n",
    "이러한 아이디어를 그래프 및 다양체와 같은 다른 영역으로 확장하고 기본 원리에서 기하학적 사전이 어떻게 나타나는지 보여주는 것이 기하학적 딥 러닝의 주요 목표이자 텍스트의 주제입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabbbbe",
   "metadata": {},
   "source": [
    "## 3.1 Symmetries, Representations, and Invariance\n",
    "3.1 대칭, 표현 및 불변성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01325fd",
   "metadata": {},
   "source": [
    "Informally, a *symmetry* of an object or system is a transformation that leaves\n",
    "a certain property of said object or system unchanged or *invariant*. Such\n",
    "transformations may be either smooth, continuous, or discrete.\n",
    "\n",
    "편하게 말하자면, 개체 또는 시스템의 대칭은 해당 개체 또는 시스템의 특정 속성을 변경하지 않거나 불변으로 남겨두는 변형입니다. 이러한 변환은 매끄럽거나 연속적이거나 불연속적일 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51a8d5",
   "metadata": {},
   "source": [
    "### Symmetry groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12454bc1",
   "metadata": {},
   "source": [
    "We will follow the juxtaposition notation convention used in group theory, $\\mathfrak{g ◦ h = gh}$, which should be read right-to-left: we first apply $\\mathfrak{h}$ and then $\\mathfrak{g}$. The order is important, as in many cases symmetries are non-commutative. \n",
    "\n",
    "그룹 이론에서 사용되는 병치 표기법 g ◦ h = gh를 따르며 오른쪽에서 왼쪽으로 읽어야 합니다. 먼저 h를 적용한 다음 g를 적용합니다. 많은 경우 대칭이 비가환적이기 때문에 순서가 중요합니다.\n",
    "\n",
    "the Fraktur font to denote group elements.\n",
    "\n",
    "Fraktur 글꼴은 그룹 요소를 나타냅니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698beb36",
   "metadata": {},
   "source": [
    "#### group axioms\n",
    "<div style=\"background-color:Gainsboro\">\n",
    "A group is a set G along with a binary operation $\\mathfrak{◦ : G × G → G}$ called composition (for brevity, denoted by juxtaposition g ◦ h = gh) satisfying the following axioms:\n",
    "\n",
    "Associativity: $\\mathfrak{(gh)k = g(hk)} \\text{ for all } \\mathfrak{g, h,k ∈ G}$\n",
    "\n",
    "Identity: there exists a unique $\\mathfrak{e ∈ G}$ satisfying $\\mathfrak{eg = ge = g}$ for all $\\mathfrak{g ∈ G}$.\n",
    "\n",
    "Inverse: For each $\\mathfrak{g ∈ G}$ there is a unique inverse $\\mathfrak{g\n",
    "^{−1} ∈ G}$ such that $\\mathfrak{gg^{−1} = g^{−1}g = e}$.\n",
    "\n",
    "Closure: The group is closed under composition, i.e., for every $\\mathfrak{g, h ∈ G}$, we have $\\mathfrak{gh ∈ G}$.\n",
    "</div>\n",
    "그룹은 이진 연산 ◦ : G × G → G 라고 하는 구성(간결함을 위해 g ◦ h = gh로 병치로 표시)과 함께 다음 공리를 충족하는 집합 G입니다.\n",
    "\n",
    "연관성\n",
    "동일성\n",
    "역\n",
    "폐쇄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1dea",
   "metadata": {},
   "source": [
    "Note that commutativity is not part of this definition, i.e. we may have $\\mathfrak{gh \\not = hg}$. Groups for which $\\mathfrak{gh = hg}$ for all $\\mathfrak{g, h ∈ G}$ are called commutative or Abelian.\n",
    "\n",
    "교환성은 이 정의의 일부가 아닙니다. 즉, $\\mathfrak{gh \\not = hg}$ 일 수 있습니다. 모든 $\\mathfrak{g, h ∈ G}$ 에 대해 $\\mathfrak{gh = hg}$ 인 그룹을 가환성/교환성 또는 Abelian이라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266407de",
   "metadata": {},
   "source": [
    "Formally, $\\mathfrak{G}$ is said to be generated by a subset $\\mathcal{S} ⊆ \\mathfrak{G}$ (called the group generator) if every element $\\mathfrak{g ∈ G}$ can be written as a finite composition of the elements of $\\mathcal{S}$ and their inverses.\n",
    "\n",
    "공식적으로, G는 모든 요소 g ∈ G가 S의 요소와 그 역원의 유한 구성으로 쓰여질 수 있는 경우 부분 집합 S ⊆ G(그룹 생성기라고 함)에 의해 생성된다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf67df4",
   "metadata": {},
   "source": [
    "Note that here we have defined a group as an abstract object, without saying what the group elements are (e.g. transformations of some domain), only how they compose. Hence, very different kinds of objects may have the same symmetry group.\n",
    "\n",
    "여기서 우리는 그룹 요소가 무엇인지(예: 일부 도메인의 변환) 구성 요소가 무엇인지는 말하지 않고, 오직 어떻게 그것들이 구성되는지에 대해서만,추상 객체로 그룹을 정의했습니다. 따라서 매우 다른 종류의 객체가 동일한 대칭 그룹을 가질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd2ccf",
   "metadata": {},
   "source": [
    "### Group Actions and Group Representations\n",
    "\n",
    "그룹 작업 및 그룹 표현"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4bc28de",
   "metadata": {},
   "source": [
    "Rather than considering groups as abstract entities, we are mostly interested in how groups *act* on data. Since we assumed that there is some domain Ω underlying our data, we will study how the group acts on Ω (e.g. translation of points of the plane), and from there obtain actions of the same group on the space of signals $\\mathcal{X (\\Omega)}$ (e.g. translations of planar images and feature maps).\n",
    "\n",
    "그룹을 추상적인 엔터티로 간주하기보다는, 그룹이 데이터에 대해 어떻게 작용하는지에 주로 관심이 있습니다. 데이터의 기저에 도메인 Ω이 있다고 가정했기 때문에 그룹이 Ω에 대해 어떻게 작용하는지 연구하고(예: 평면 점의 변환) 거기에서 신호 X(Ω)의 공간에서 동일한 그룹의 작용을 얻습니다. (예: 평면 이미지 및 기능 맵의 번역).\n",
    "\n",
    "A *group action* of $\\mathfrak{G}$ on a set $\\Omega$ is defined as a mapping $(\\mathfrak{g}, u) \\mapsto \\mathfrak{g} \\cdot u$ associating a group element $\\mathfrak{g ∈ G}$ and a point $u ∈ \\Omega$ with some other point on Ω in a way that is compatible with the group operations, i.e., $\\mathfrak{g \\cdot (h \\cdot} u) = \\mathfrak{(gh)} \\cdot u \\text{ for all } \\mathfrak{g, h ∈ G} \\text{ and } u ∈ \\Omega.$\n",
    "\n",
    "집합(set) Ω에 대한 G의 그룹 작업은 그룹 요소 g ∈ G와 점 u ∈ Ω을 그룹 작업과 호환되는 방식으로 Ω의 다른 점과 연결하는 매핑 $(\\mathfrak{g}, u) \\mapsto \\mathfrak{g} \\cdot u$ 로 정의됩니다. 즉 모든 g, h ∈ G 및 u ∈ Ω에 대해 g.(h.u) = (gh).u입니다.\n",
    "\n",
    " We shall see numerous instances of group actions in the following sections. For example, in the plane the Euclidean group E(2) is the group of transformations of $\\mathbb{R^{2}}$ that preserves Euclidean distances, and consists of translations, rotations, and reflections. The same group, however, can also act on the space of images on the plane (by translating, rotating and flipping the grid of pixels), as well as on the representation spaces learned by a neural network. More precisely, if we have a group $\\mathfrak{G}$ acting on $\\Omega$, we automatically obtain an action of $\\mathfrak{G}$ on the space $\\mathcal{X}(\\Omega)$:\n",
    "\n",
    "다음 섹션에서 그룹 작업의 수많은 사례를 볼 것입니다. 예를 들어 평면에서 유클리드 그룹 E(2)는 유클리드 거리를 유지하고 변환, 회전 및 반사로 구성된 R 2 의 변환 그룹입니다. 그러나 동일한 그룹은 평면의 이미지 공간(픽셀 격자를 변환, 회전 및 뒤집기)과 신경망에서 학습한 표현 공간에도 작용할 수 있습니다. 보다 정확하게는, Ω에서 작용하는 그룹 G가 있는 경우, 공간 X(Ω)에서 G의 작용을 자동으로 얻습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53844a7e",
   "metadata": {},
   "source": [
    "$$\n",
    "    (\\mathfrak{g} . x )(u) = x ( \\mathfrak{g^{-1}}u ) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9264feb",
   "metadata": {},
   "source": [
    "Due to the inverse on g, this is indeed a valid group action, in that we have $(\\mathfrak{g}.(\\mathfrak{h}.x))(u) = ((\\mathfrak{gh}).x)(u)$.\n",
    "\n",
    "g의 역으로 인해,  이것은 실제로 유효한 그룹 작업입니다. 그리고 우리는 다음을 가집니다.\n",
    "\n",
    "The most important kind of group actions, which we will encounter repeatedly throughout this text, are linear group actions, also known as group\n",
    "representations. The action on signals in equation (3) is indeed linear, in the sense that\n",
    "\n",
    "이 텍스트 전체에서 반복적으로 접하게 될 가장 중요한 종류의 그룹 작업은 그룹 표현이라고도 하는 선형 그룹 작업입니다. 방정식 (3)에서 신호에 대한 작용은 다음과 같은 의미에서 실제로 선형입니다.\n",
    "\n",
    "$$\n",
    "\\mathfrak{g}.(\\alpha x + \\beta \\acute{x}\n",
    ") = \\alpha(\\mathfrak{g}.x) + \\beta(\\mathfrak{g}.\\acute{x}\n",
    ")\n",
    "$$\n",
    "\n",
    "for any scalars $\\alpha, \\beta$ and signals $x, \\acute{x} ∈ X (\\Omega)$. We can describe linear actions either as maps $\\left(\\mathfrak{g}, x\\right) \\mapsto \\mathfrak{g}.x$ that are linear in $x$, or equivalently, by currying, as a map $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ that assigns to each group element $\\mathfrak{g}$ an (invertible) matrix $\\rho(\\mathfrak{g})$. The dimension n of the matrix is in general arbitrary and not necessarily related to the dimensionality of the group or the dimensionality of $\\Omega$, but in applications to deep learning n will usually be the dimensionality of the feature space on which the group acts.  For instance, we may have the group of 2D translations acting on a space of images with n pixels.\n",
    "\n",
    "위 식은 모든 스칼라 α, β 및 신호 $x, \\acute{x} ∈ X (\\Omega)$ 에 대해 정의 됩니다. 선형 작용을 x에서 선형인 맵 $\\left(\\mathfrak{g}, x\\right) \\mapsto \\mathfrak{g}.x$ 로 설명하거나, 동등하게, 각 그룹 요소 g에 (가역) 행렬 ρ(g)를 할당하는 맵 $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ 으로 설명할 수 있습니다. 행렬의 차원 n은 일반적으로 임의적이며, 그룹의 차원이나 Ω의 차원과 반드시 관련이 있는 것은 아니지만, 딥 러닝에 적용할 때 n은 일반적으로 그룹이 작용하는 특징 공간의 차원입니다. 예를 들어, n 픽셀의 이미지 공간에 작용하는 2D 번역 그룹이 있을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32305a2",
   "metadata": {},
   "source": [
    "A n-dimensional real *representation* of a group $\\mathfrak{G}$ is a map $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$, assigning to each $\\mathfrak{g ∈ G}$ an *invertible* matrix $\\rho(\\mathfrak{g})$, and satisfying the\n",
    "condition $\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$ for all $\\mathfrak{g, h ∈ G}$. A representation is called unitary or orthogonal if the matrix $\\rho(\\mathfrak{g})$ is unitary or orthogonal for all $\\mathfrak{g ∈ G}$\n",
    "\n",
    "그룹 G의 n차원 실제 표현은 맵 ρ : G → R n×n 입니다. 각 g ∈ G에 가역(invertible)행렬 ρ(g)를 할당하고, 모든 g,h ∈ G 에 대해, 조건 ρ(gh) = ρ(g)ρ(h)를 충족합니다. 행렬 ρ(g)가 모든 g ∈ G에 대해 단일 또는 직교인 경우, 표현을 단일(unitary) 또는 직교(orthogonal)라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c53e16",
   "metadata": {},
   "source": [
    "Written in the language of group representations, the action of $\\mathfrak{G}$ on signals $x ∈ \\mathcal{X}(\\Omega)$ is defined as $\\rho(\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}u)$. We again verify that\n",
    "\n",
    "그룹 표현의 언어로 작성된 신호 x ∈ X(Ω)에 대한 G의 작용은 $\\rho(\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}u)$ 로 정의됩니다. 다음을 다시 확인합니다.\n",
    "\n",
    "$$\n",
    "(\\rho(\\mathfrak{g})(\\rho(\\mathfrak{h})x))(u) = (\\rho(\\mathfrak{gh})x(u)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78ac19",
   "metadata": {},
   "source": [
    "### Invariant and Equivariant functions\n",
    "불변 및 등가 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1b4e9",
   "metadata": {},
   "source": [
    "The symmetry of the domain Ω underlying the signals X (Ω) imposes structure on the function f defined on such signals. It turns out to be a powerful inductive bias, improving learning efficiency by reducing the space of possible interpolants, F(X (Ω)), to those which satisfy the symmetry priors. Two important cases we will be exploring in this text are *invariant* and *equivariant* functions.\n",
    "\n",
    "신호 X(Ω)의 기저에 있는 도메인 Ω의 대칭은 이러한 신호에 정의된 함수 f에 구조를 부과합니다. 이는 강력한 귀납적 편향으로 판명되어 가능한 보간법 F(X(Ω))의 공간을 대칭 사전을 만족하는 것으로 줄임으로써 학습 효율성을 향상시킵니다. 이 텍스트에서 탐구할 두 가지 중요한 경우는 불변(invariant) 및 등변(equivariant) 함수입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3c7f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{Y}$ is $\\mathfrak{G}$-*invariant* if $f(\\rho(\\mathfrak{g})x) = f(x)$ for all $\\mathfrak{g ∈ G}$ and $x ∈ \\mathcal{X} (\\Omega)$, i.e., its output is unaffected by the group action on the input.\n",
    "\n",
    "함수 $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{Y}$ 는 모든 g ∈ G 및 x ∈ X(Ω)에 대해 f(ρ(g)x) = f(x)의 경우 G-불변입니다. 즉, 출력이 입력에 대한 그룹 행동의 영향을 받지 않습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cd6a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (Ω) \\rightarrow \\mathcal{X} (Ω)$ is $\\mathfrak{G}$-*equivariant* if $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x)$ for all $\\mathfrak{g ∈ G}$, i.e., group action on the input affects the output in the same way.\n",
    "\n",
    "함수 f : X(Ω) → X(Ω)는 모든 g ∈ G에 대해 f(ρ(g)x) = ρ(g)f(x)인 경우 G-등변량(G-equivariant)입니다. 즉, 입력에 대한 그룹 작업은 동일한 방식으로 출력에 영향을 줍니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123db12",
   "metadata": {},
   "source": [
    "\n",
    "More generally, we might have $f : \\mathcal{X} (Ω) → \\mathcal{X} (Ω')$ with input and output spaces having different domains $Ω, Ω'$ and representations $ρ,ρ'$ of the same group $\\mathfrak{G}$. In this case, equivariance is defined as $f(ρ(g)x) = ρ'(g)f(x)$.\n",
    "\n",
    "보다 일반적으로, 서로 다른 영역 Ω, Ω' 및 표현 ρ, ρ'을 동일한 그룹 G의 입력 및 출력 공간으로 사용하여 f : X(Ω) → X(Ω')를 가질 수 있습니다. 이 경우 등분산은 f(ρ(g)x) = ρ'(g)f(x)로 정의됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364c4a4",
   "metadata": {},
   "source": [
    "However, even the previous use case of image classification is usually implemented as a sequence of convolutional (shift-equivariant) layers, followed by global pooling (which is shift-invariant). As we will see in Section 3.5, this is a general blueprint of a majority of deep learning architectures, including CNNs and Graph Neural Networks (GNNs).\n",
    "\n",
    "그러나 이미지 분류의 이전 사용 사례조차도 일반적으로 일련의 컨볼루션(shift-equivariant) 레이어로 구현된 다음 전역 풀링(shift-invariant)이 뒤따릅니다. 섹션 3.5에서 볼 수 있듯이 이것은 CNN 및 GNN(그래프 신경망)을 포함한 대다수 딥 러닝 아키텍처의 일반적인 청사진입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90366337",
   "metadata": {},
   "source": [
    "## 3.2 Isomorphisms and Automorphisms \n",
    "동형과 자기동형"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d607a9",
   "metadata": {},
   "source": [
    "### Subgroups and Levels of structure\n",
    "Invertible and structure-preserving maps between different objects often go under the generic name of isomorphisms (Greek for ‘equal shape’). An isomorphism from an object to itself is called an automorphism, or symmetry.\n",
    "\n",
    "\n",
    "서로 다른 객체 사이의 가역적이고(Invertible) 구조를 보존하는 지도는 종종 동형학(isomorphisms 그리스어'동일한 모양') 이라고 불림니다. 물체에서 그 자체로의 동형(isomorphisms)을 자동형(automorphism) 또는 대칭이라고 합니다.\n",
    "\n",
    "bijections(전단사): 일대일 대응(bijection, one-to-one correspondence)\n",
    "\n",
    "For a finite set, the cardinality is the number of elements (‘size’) of the set, and for infinite sets the cardinality indicates different kinds of\n",
    "infinities, such as the countable infinity of the natural numbers, or the\n",
    "uncountable infinity of the continuum R.\n",
    "\n",
    "유한 집합의 경우 카디널리티(cardinality)는 집합의 요소 수('크기')이고, 무한 집합의 경우 카디널리티(cardinality)는 다양한 종류의 무한대를 나타냅니다. 예를 들어 자연수의 셀 수 있는 무한대 또는 실수 연속체의 셀 수 없는 무한대와 같이.\n",
    "\n",
    "Every differentiable function is continuous. If the map is continuously differentiable‘sufficiently many times’, it is said to be smooth.\n",
    "\n",
    "모든 미분 가능한 함수는 연속적입니다. 맵이 '충분히 여러 번' 연속 미분 가능하면 평활('smooth')하다고 합니다.\n",
    "\n",
    "homeomorphisms(유사형): if Ω is a topological space, maps that preserve continuity and in addition to simple bijections between sets, are also continuous and have continuous inverse. Ω이 위상 공간인 경우 연속성을 유지하고, 세트간의 단순 전단사(bijection) 외에도, 연속적이며, 연속적인 가역성을 가지는 맵.\n",
    "\n",
    "the map $τ (u) = u$ is the identity element, and for every $τ$ the inverse\n",
    "exists by definition, satisfying $(τ ◦ τ^{−1})(u) = (τ^{−1} ◦ τ )(u) = u$.\n",
    "\n",
    "맵 $τ (u) = u$는 항등(identity) 요소입니다, 그리고 모든 $τ$에 대해 역함수는 정의에 의해 존재하며 $(τ ◦ τ^{−1})(u) = (τ^{−1} ◦ τ )(u) = u$를 충족합니다.\n",
    "\n",
    "diffeomorphisms(미분변형) and denoted by Diff($Ω$): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77aadb13",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A metric or distance is a function $d : Ω × Ω → [0, ∞)$ satisfying for all\n",
    "$u, v, w ∈ Ω$:\n",
    "\n",
    "도메인 $Ω$에 속하는 모든 $u, v, w$에 대하여 미터(metric) 또는 거리(distance)는 함수로써 $Ω × Ω$를 $0 \\leq$ 이고 $< ∞$은 영역에 매핑합니다.  \n",
    "    \n",
    ">Identity of indiscernibles: $d(u, v) = 0 \\text{ iff } u = v$.\n",
    ">\n",
    ">식별할 수 없는 항목의 동일성: $u = v$ 인 경우에만 $d(u, v) = 0$을 성립힌다. \n",
    "    \n",
    ">Symmetry: $d(u, v) = d(v, u)$.\n",
    ">\n",
    ">대칭\n",
    "\n",
    ">Triangle inequality: $d(u, v) ≤ d(u, w) + d(w, v)$.\n",
    "> \n",
    ">삼각 부등식\n",
    "\n",
    "A space equipped with a metric $(Ω, d)$ is called a metric space.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e720395",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "Let $(\\mathfrak{G}, ◦)$ be a group and $\\mathfrak{H ⊆ G}$ a subset. $\\mathfrak{H}$ is said to be a *subgroup* of $\\mathfrak{G}$ if $(\\mathfrak{H}, ◦)$ constitutes a group with the same operation.\n",
    "\n",
    "(G, ◦)를 그룹이라고 하고 H ⊆ G를 부분집합이라고 하자. (H, ◦)가 동일한 연산을 갖는 그룹을 구성하는 경우, H는 G의 하위 그룹이라고 합니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0103ed5",
   "metadata": {},
   "source": [
    "### Isomorphisms and Automorphisms\n",
    "동형사상(Isomorphisms)과 자동사상(Automorphisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbcab",
   "metadata": {},
   "source": [
    "We have described symmetries as structure preserving and invertible maps from *an object to itself*. Such maps are also known as *automorphisms*, and describe a way in which an object is equivalent it itself. However, an equally important class of maps are the so-called *isomorphisms*, which exhibit an equivalence between two nonidentical objects. These concepts are often conflated, but distinguishing them is necessary to create clarity for our following discussion.\n",
    "\n",
    "대칭은 구조를 보존하고 객체에서 객체 자체로의 반전 가능한 맵으로 설명했습니다. 이러한 맵은 자동형성이라고도 하며, 객체가 그 자체로 동등한 방식을 설명합니다. 그러나 그에 못지않게 중요한 맵 유형은 동형 맵으로, 동일하지 않은 두 객체 간에 동등성을 나타내는 맵입니다. 이러한 개념은 종종 혼동되는 경우가 많지만, 명확하게 구분하는 것은 구분하는 것은 다음 논의를 명확하게 하기 위해 필요합니다.\n",
    "\n",
    "To understand the difference, consider a set $Ω = \\left\\{0, 1, 2\\right\\}$. An automorphism of the set $Ω$ is a bijection $τ : Ω → Ω$ such as a cyclic shift $τ (u) = u+ 1 mod 3$. Such a map preserves the cardinality property, and maps $Ω$ onto itself. If we have another set $Ω' = \\left\\{a, b, c\\right\\}$ with the same number of elements, then a bijection $η : Ω → Ω'$ such as $η(0) = a, η(1) = b, η(2) = c$ is a *set isomorphism*.\n",
    "\n",
    "차이점을 이해하기 위해 Ω = {0, 1, 2} 집합을 생각해 보겠습니다. 집합 Ω의 자동변환은 순환 이동 τ(u) = u+ 1 mod 3과 같은 τ : Ω → Ω의 바이제이션입니다.\n",
    "이러한 맵은 카디널리티 속성을 보존하고 Ω을 그 자체에 매핑합니다. 만약\n",
    "원소 수가 같은 다른 집합 Ω' = {a, b, c}가 있다면, η : Ω → Ω' η(0) = a, η(1) = b, η(2) = c와 같은 바이짓은 집합 동형입니다.\n",
    "\n",
    "As we will see in Section 4.1 for graphs, the notion of structure includes\n",
    "not just the number of nodes, but also the connectivity. An isomorphism\n",
    "$η : V → V'$ between two graphs $G = (V, E)$ and $G' = (V', E')$ is thus a\n",
    "bijection between the nodes that maps pairs of connected nodes to pairs\n",
    "of connected nodes, and likewise for pairs of non-connected nodes. Two\n",
    "isomorphic graphs are thus structurally identical, and differ only in the\n",
    "way their nodes are ordered. On the other hand, a graph automorphism or\n",
    "symmetry is a map $τ : V → V$ maps the nodes of the graph back to itself,\n",
    "while preserving the connectivity. A graph with a non-trivial automorphism\n",
    "(i.e., $τ \\neq id$) presents symmetries.\n",
    "\n",
    "그래프에 대한 4.1절에서 살펴보겠지만, 구조의 개념에는 노드의 수뿐만 아니라 연결성 또한 포함됩니다. 두 그래프 𝐺=(𝑉,𝐸) 와 𝐺′=(𝑉′,𝐸′) 사이의 동형 η:𝑉→𝑉′ 는 연결된 노드 쌍을 연결된 노드 쌍에 매핑하는 노드 간의 바이제션이며, 연결되지 않은 노드 쌍에 대해서도 마찬가지입니다. 따라서 두 개의 동형 그래프는 구조적으로 동일하며 노드의 순서만 다를 뿐입니다. 반면에 그래프 자동변형 또는 대칭은 맵 τ:𝑉→𝑉 는 연결성을 유지하면서 그래프의 노드를 다시 자기 자신으로 매핑합니다. 사소한 오토모피즘이 아닌 그래프(즉, τ≠𝑖𝑑 )를 가진 그래프는 대칭성을 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae07ae",
   "metadata": {},
   "source": [
    "## 3.3 Deformation Stability\n",
    "\n",
    "변형 안정성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc491cbe",
   "metadata": {},
   "source": [
    "we are more interested in a far larger set of transformations where global, exact invariance is replaced by a local, inexact one. In our discussion, we will distinguish between two scenarios: the setting where the domain $Ω$ is fixed, and signals $x ∈ \\mathcal{X} (Ω)$ are undergoing deformations, and the setting where the domain $Ω$ itself may be deformed.\n",
    "\n",
    "우리는 전역의 정확한 불변성이 국지적인 부정확한 불변성으로 대체되는 훨씬 더 큰 변환 집합에 더 관심이 있습니다. 여기서는 도메인 Ω이 고정되어 있고 신호 x ∈ X (Ω)가 변형되는 설정과 도메인 Ω 자체가 변형될 수 있는 설정의 두 가지 시나리오를 구분해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b81e92",
   "metadata": {},
   "source": [
    "### Stability to signal deformations\n",
    "신호 변형에 대한 안정성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a9b60",
   "metadata": {},
   "source": [
    "In many applications, we know a priori that a small deformation of the signal x should not change the output of $f(x)$, so it is tempting to consider such deformations as symmetries.\n",
    "\n",
    "많은 애플리케이션에서 신호 x의 작은 변형이 𝑓(𝑥)의 출력을 변경해서는 안 된다는 것을 알고 있으므로, 이러한 변형을 대칭으로 간주하고 싶어집니다.\n",
    "\n",
    "A better approach is to quantify how “far” a given $τ ∈ Diff(Ω)$ is from a given symmetry subgroup $G ⊂ Diff(Ω)$ (e.g. translations) with a complexity measure $c(τ )$, so that $c(τ ) = 0$ whenever $τ ∈ G$. We can now replace our previous definition of exact invariance and equivarance under group actions with a ‘softer’ notion of *deformation stability* (or approximate invariance):\n",
    "\n",
    "더 나은 접근 방식은 주어진 τ∈𝐷𝑖𝑓𝑓(Ω) 가 주어진 대칭 하위 그룹 𝐺⊂𝐷𝑖𝑓𝑓(Ω)(예: 변환)에서 얼마나 멀리 떨어져 있는지를 복잡도 측정값 𝑐(τ) 를 사용하여 정량화하는 것입니다. 이를 사용하여 τ∈𝐺인 경우 𝑐(τ)=0 이 되도록 합니다. 이제 그룹 작용 하에서 정확한 불변성과 등가성에 대한 이전 정의를 변형 안정성(deformation stability)(또는 근사 불변성 approximate invariance )이라는 '더 부드러운' 개념으로 대체할 수 있습니다:\n",
    "\n",
    "\n",
    "$$\n",
    "    \\|f(ρ(τ )x) − f(x)\\| ≤ Cc(τ )\\|x\\|, , ∀x ∈ \\mathcal{X} (Ω) \\tag{4}\n",
    "$$\n",
    "\n",
    "where $ρ(τ )x(u) = x(τ−1u)$ as before, and where C is some constant independent of the signal $x$. A function $f ∈ F(X (Ω))$ satisfying the above equation is said to be geometrically stable. We will see examples of such functions in the next Section 3.4.\n",
    "\n",
    "여기서 ρ(τ)𝑥(𝑢)=𝑥(τ-1𝑢) 이며, 여기서 C는 신호 𝑢와 독립적인 상수입니다. 𝑓∈𝐹(𝑋(Ω)) 인 함수가 위의 방정식을 만족하는 경우 기하학적으로 안정적이라고 합니다. 다음 3.4절에서 이러한 함수의 예를 살펴보겠습니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb72394",
   "metadata": {},
   "source": [
    "Since c(τ ) = 0 for τ ∈ G, this definition generalises the G-invariance property defined above. Its utility in applications depends on introducing an appropriate deformation cost. In the case of images defined over a continuous Euclidean plane, a popular choice is $c^{2}(τ) := \\int_{\\Omega}\\|∇τ (u)\\|^{2} du$, which measures the ‘elasticity’ of τ , i.e., how different it is from the displacement by a constant vector field. This deformation cost is in fact a norm often called the Dirichlet energy, and can be used to quantify how far τ is from the translation group.\n",
    "\n",
    "τ ∈ G에서 c(τ ) = 0이므로, 이 정의는 위에서 정의한 G-분산 특성을 일반화합니다. 응용 프로그램에서의 유용성은 적절한 변형 비용을 도입하는 데 달려 있습니다. 연속 유클리드 평면 위에 정의된 이미지의 경우, τ의 '탄성', 즉 일정한 벡터 필드에 의한 변위와 얼마나 다른지를 측정하는 $c^{2}(τ) := \\int_{\\Omega}\\|∇τ (u)\\|^{2} du$가 널리 사용됩니다. 이 변형 비용은 사실 디리클레(Dirichlet) 에너지라고도 하는 규범으로, τ가 변환 그룹에서 얼마나 멀리 떨어져 있는지 정량화하는 데 사용할 수 있습니다.\n",
    "\n",
    "$Aut(Ω)$: The set automorphism group\n",
    "\n",
    "$:=$: equal by definition\n",
    "\n",
    "$∇$: Del or nabla. vector differential operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e04c",
   "metadata": {},
   "source": [
    "### Stability to domain deformations\n",
    "도메인 변형에 대한 안정성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c8c4e",
   "metadata": {},
   "source": [
    "In many applications, the object being deformed is not the signal, but the geometric domain $Ω$ itself. Canonical instances of this are applications dealing with graphs and manifolds: a graph can model a social network at different instance of time containing slightly different social relations (follow graph), or a manifold can model a 3D object undergoing non-rigid deformations. This deformation can be quantified as follows. If $\\mathcal{D}$ denotes the space of all possible variable domains (such as the space of all graphs, or the space of Riemannian manifolds), one can define for $Ω, \\tilde{Ω} ∈ \\mathcal{D}$ an appropriate metric (‘distance’) $d(Ω, \\tilde{Ω})$ satisfying $d(Ω, \\tilde{Ω}) = 0$ if $Ω$ and $\\tilde{Ω}$ are equivalent in some sense: for example, the graph edit distance vanishes when the graphs are isomorphic, and the GromovHausdorff distance between Riemannian manifolds equipped with geodesic distances vanishes when two manifolds are isometric.\n",
    "\n",
    "많은 애플리케이션에서 변형되는 대상은 신호가 아니라 기하학적 영역 Ω 자체입니다. 그래프와 다양체를 다루는 애플리케이션의 대표적인 예로, 그래프는 약간 다른 사회적 관계를 포함하는 여러 시점의 소셜 네트워크를 모델링하거나(그래프를 따라), 다양체는 비강체(non-rigid) 변형을 겪는 3D 객체를 모델링할 수 있습니다. 이러한 변형은 다음과 같이 정량화할 수 있습니다. $\\mathcal{D}$가 가능한 모든 가변 영역의 공간(예: 모든 그래프의 공간 또는 리만 다양체의 공간)을 나타내는 경우,  $Ω, \\tilde{Ω} ∈ \\mathcal{D}$에 대해 $Ω$과 $\\tilde{Ω}$가 어떤 의미에서 동등한 경우 $d(Ω, \\tilde{Ω}) = 0$을 만족하는 적절한 메트릭('거리') $d(Ω, \\tilde{Ω})$를 정의할 수 있습니다: 예를 들어 그래프 편집 거리는 그래프가 동형일 때 사라지고, 측지 거리가 있는 리만 다양체 사이의 그로모프하우스도르프 거리는 두 다양체가 등각일 때 사라집니다.\n",
    "\n",
    "A common construction of such distances between domains relies on some family of invertible mapping $η : Ω → \\tilde{Ω}$ that try to ‘align’ the domains in a way that the corresponding structures are best preserved. For example, in the case of graphs or Riemannian manifolds (regarded as metric spaces with the geodesic distance), this alignment can compare pair-wise adjacency or distance structures (d and $\\tilde{d}$, respectively),\n",
    "\n",
    "이러한 도메인 간 거리의 일반적인 구성은 해당 구조가 가장 잘 보존되는 방식으로 도메인을 '정렬'하려는 역전 매핑 $η : Ω → \\tilde{Ω}$의 일부 제품군에 의존합니다. 예를 들어, 그래프나 리만 다양체(측지 거리가 있는 미터법 공간으로 간주됨)의 경우, 이러한 정렬은 쌍별 인접성 또는 거리 구조(각각 d와 $\\tilde{d}$)를 비교할 수 있습니다,\n",
    "\n",
    "$$\n",
    "d_\\mathcal{D}(Ω,\\tilde{Ω}) = \\inf_{η∈\\mathfrak{G}}\\|d − \\tilde{d}◦ (η × η) \\|\n",
    "$$\n",
    "\n",
    "where $\\mathfrak{G}$ is the group of isomorphisms such as bijections or isometries, and the norm is defined over the product space $Ω × Ω$. In other words, a distance between elements of $Ω, \\tilde{Ω}$ is ‘lifted’ to a distance between the domains themselves, by accounting for all the possible alignments that preserve the internal structure. Given a signal $x ∈ X (Ω)$ and a deformed domain $\\tilde{Ω}$, one can then consider the deformed signal $\\tilde{x} = x ◦ η^{−1} ∈ X (\\tilde{Ω})$.\n",
    "\n",
    "여기서 $\\mathfrak{G}$는 사변형 또는 아이소메트리와 같은 동형들의 그룹이며, 규범은 곱 공간 $Ω × Ω$에 대해 정의됩니다. 즉, $Ω, \\tilde{Ω}$의 요소 사이의 거리는 내부 구조를 보존하는 모든 가능한 정렬을 고려하여 도메인 자체 사이의 거리로 '리프팅'됩니다. 신호 $x ∈ X (Ω)$와 변형된 도메인 $\\tilde{Ω}$가 주어지면, 변형된 신호 $\\tilde{x} = x ◦ η^{−1} ∈ X (\\tilde{Ω})$를 고려할 수 있습니다.\n",
    "\n",
    "By slightly abusing the notation, we define $X (D) = {(X (Ω), Ω) : Ω ∈ \\mathcal{D}}$ as\n",
    "the ensemble of possible input signals defined over a varying domain. A function $f : \\mathcal{X} (\\mathcal{D}) → \\mathcal{Y}$ is stable to domain deformations if\n",
    "\n",
    "표기법을 약간 변형하여 $X (D) = {(X (Ω), Ω) : Ω ∈ \\mathcal{D}}$ 를 다양한 영역에 걸쳐 정의된 가능한 입력 신호의 앙상블로 정의합니다. 함수 $f : \\mathcal{X} (\\mathcal{D}) → \\mathcal{Y}$ 는 다음과 같은 경우 도메인 변형에 안정적입니다. 모든 $Ω, \\tilde{Ω} ∈ D$ 와  $x ∈ X (Ω)$에 대하여\n",
    "\n",
    "$$\n",
    "    \\|f(x, Ω) − f(\\tilde{x}, \\tilde{Ω}) \\|  \\leq C \\|x\\|d_{\\mathcal{D}}(Ω,\\tilde{Ω}) \\tag{5}\n",
    "$$\n",
    "\n",
    "for all $Ω, \\tilde{Ω} ∈ D$, and $x ∈ X (Ω)$. We will discuss this notion of stability in the context of manifolds in Sections 4.4–4.6, where isometric deformations play a crucial role. Furthermore, it can be shown that the stability to domain deformations is a natural generalisation of the stability to signal eformations, by viewing the latter in terms of deformations of the volume form Gama et al. (2019).\n",
    "\n",
    "이 안정성의 개념은 등각 변형이 중요한 역할을 하는 4.4-4.6절의 다양체의 맥락에서 논의할 것입니다. 또한, 도메인 변형에 대한 안정성은 신호 변형에 대한 안정성의 자연스러운 일반화이며, 후자를 체적 형태의 변형 측면에서 보면 알 수 있습니다 (Gama et al. (2019)).\n",
    "\n",
    "$\\inf$: Bound, 유계, 유한한 영역을 가지는 오브젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe679b",
   "metadata": {},
   "source": [
    "## 3.4 Scale Separation\n",
    "스케일 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099b6c5",
   "metadata": {},
   "source": [
    "While deformation stability substantially strengthens the global symmetry priors, it is not sufficient in itself to overcome the curse of dimensionality, in the sense that, informally speaking, there are still “too many\" functions that respect (4) as the size of the domain grows. A key insight to overcome this curse is to exploit the multiscale structure of physical tasks. Before describing multiscale representations, we need to introduce the main elements of Fourier transforms, which rely on frequency rather than scale.\n",
    "\n",
    "변형 안정성은 전역 대칭 선행 조건을 상당히 강화하지만, 비공식적으로 말하자면 도메인의 크기가 커짐에 따라 (4)를 존중하는 함수가 여전히 \"너무 많이\" 존재한다는 점에서 그 자체만으로는 차원의 저주를 극복하기에 충분하지 않습니다. 이 차원성의 저주를 극복하기 위한 핵심 인사이트는 물리적 작업의 멀티스케일 구조를 활용하는 것입니다. 멀티스케일 표현을 설명하기 전에 다음과 규모보다는 주파수에 의존하는 푸리에 변환 같은 주요 요소를 소개할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5ad09",
   "metadata": {},
   "source": [
    "### Fourier Transform and Global invariants\n",
    "[But what is the Fourier Transform? A visual introduction.](https://youtu.be/spUNpyF58BY)\n",
    "\n",
    "An essential aspect of Fourier transforms is that they reveal global properties of the signal and the domain, such as smoothness or conductance. Such global behavior is convenient in presence of global symmetries of the domain such as translation, but not to study more general diffeomorphisms. This requires a representation that trades off spatial and frequential localisation, as we see next.\n",
    "\n",
    "푸리에 변환의 핵심적인 측면은 평활도(smoothness)나 전도도(conductance)와 같은 신호와 도메인의 전역 속성을 드러낸다는 점입니다. 이러한 전역적 동작은 변환과 같이 도메인의 전역적 대칭성이 있는 경우에는 편리하지만, 보다 일반적인 이형성을 연구하는 데는 적합하지 않습니다. 이를 위해서는 다음에 살펴볼 것처럼 공간 및 주파수 로컬라이제이션을 절충하는 표현이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f00de",
   "metadata": {},
   "source": [
    "### Multiscale representations\n",
    "다중 스케일 표현\n",
    "\n",
    "The notion of local invariance can be articulated by switching from a Fourier frequency-based representation to a scalebased representation, the cornerstone of multi-scale decomposition methods such as wavelets. The essential insight of multi-scale methods is to decompose functions defined over the domain Ω into elementary functions that are localised both in space and frequency. In the case of wavelets, this is achieved by correlating a translated and dilated filter (mother wavelet) ψ, producing a combined spatio-frequency representation called a continuous wavelet transform.\n",
    "\n",
    "국소 불변성의 개념은 푸리에 주파수 기반 표현에서 웨이블릿과 같은 다중 스케일 분해 방법의 초석인 스케일 기반 표현으로 전환함으로써 명확하게 표현할 수 있습니다. 다중 스케일 방법의 핵심은 도메인 Ω에 정의된 함수를 공간과 주파수 모두에 국한된 기본 함수로 분해하는 것입니다. 웨이블릿의 경우, 이는 변환 및 확장 필터(마더 웨이블릿) ψ를 상호 연관시켜 연속 웨이블릿 변환이라고 하는 결합된 공간-주파수 표현을 생성함으로써 달성됩니다.\n",
    "\n",
    "$$\n",
    "(W_ψx)(u, ξ) = ξ^{−1/2} \\int_{−∞}^{+∞}ψ \\left(\\frac{v − u}{ξ}\\right) x(v)dv\n",
    "$$\n",
    "\n",
    "The translated and dilated filters are called wavelet atoms; their spatial position and dilation correspond to the coordinates u and $ξ$ of the wavelet transform. These coordinates are usually sampled dyadically $(ξ = 2^{−j}$ and $u = 2^{−jk})$, with $j$ referred to as scale. Multi-scale signal representations bring important benefits in terms of capturing regularity properties beyond global smoothness, such as piece-wise smoothness, which made them a popular tool in signal and image processing and numerical analysis in the 90s.\n",
    "\n",
    "변환 및 확장된 필터를 웨이브렛 원자라고 하며, 이 원자의 공간적 위치와 확장은 웨이브렛 변환의 좌표 u와 ξ에 해당합니다. 이러한 좌표는 일반적으로 이원적으로 샘플링되며 $(ξ = 2^{−j}$ 및 $u = 2^{−jk})$, j를 스케일이라고 합니다. 다중 스케일 신호 표현은 조각별 평활도와 같은 전역 평활도 이상의 규칙성 속성을 캡처하는 데 중요한 이점을 가져다주므로 90년대에 신호 및 이미지 처리와 수치 분석에서 널리 사용되는 도구가 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0898c",
   "metadata": {},
   "source": [
    "### Deformation stability of Multiscale representations\n",
    "다중 축척 표현의 변형 안정성\n",
    "\n",
    "The benefit of multiscale localised wavelet decompositions over Fourier decompositions is revealed when considering the effect of small deformations ‘nearby’ the underlying symmetry group. Let us illustrate this important concept in the Euclidean domain and the translation group. Since the Fourier representation diagonalises the shift operator (which can be thought of as convolution, as we will see in more detail in Section 4.2), it is an efficient representation for translation transformations. However, Fourier decompositions are unstable under high-frequency deformations. In contrast, wavelet decompositions offer a stable representation in such cases.\n",
    "\n",
    "푸리에 분해에 비해 멀티스케일 국소화 웨이블릿 분해의 이점은 기본 대칭 그룹 '근처'에 있는 작은 변형의 효과를 고려할 때 드러납니다. 이 중요한 개념을 유클리드 영역과 변환 그룹에서 설명해 보겠습니다. 푸리에 표현은 시프트 연산자(4.2절에서 자세히 살펴보겠지만 컨볼루션으로 생각할 수 있음)를 대각선으로 대칭화하기 때문에 변환 변환에 효율적인 표현입니다. 그러나 푸리에 분해는 고주파수 변형에서 불안정합니다. 이와 대조적으로 웨이블릿 분해는 이러한 경우에 안정적인 표현을 제공합니다.\n",
    "\n",
    "Skip\n",
    "\n",
    "Consequently, such Fourier representation is *unstable under deformations*, however small. This unstability is manifested in general domains and non-rigid transformations; we will see another instance of this unstability in the analysis of 3d shapes using the natural extension of Fourier transforms described in Section 4.4\n",
    "\n",
    "결과적으로 이러한 푸리에 표현은 아무리 작은 변형에도 불안정합니다. 이러한 불안정성은 일반 영역과 비강체 변환에서 나타나며, 4.4절에서 설명한 푸리에 변환의 자연스러운 확장을 사용한 3D 도형 분석에서 이러한 불안정성의 또 다른 예를 살펴볼 것입니다.\n",
    "\n",
    "Wavelets offer a remedy to this problem that also reveals the power of ultiscale representations. In the above example, we can show (Mallat, 2012) that the wavelet decomposition $W_ψx$ is *approximately equivariant* to deformations,\n",
    "\n",
    "웨이브렛은 이 문제에 대한 해결책을 제시하며 울트라스케일 표현의 힘을 드러냅니다. 위의 예에서 웨이브렛 분해 Wψx는 변형과 거의 등변수임을 보여줄 수 있습니다(Mallat, 2012),\n",
    "\n",
    "In other words, decomposing the signal information into scales using localised filters rather than frequencies turns a global unstable representation into a family of locally stable features. Importantly, such measurements at different scales are not yet invariant, and need to be progressively processed towards the low frequencies, hinting at the deep compositional nature of modern neural networks, and captured in our Blueprint for Geometric Deep Learning, presented next.\n",
    "\n",
    "즉, 주파수가 아닌 국소 필터를 사용하여 신호 정보를 스케일로 분해하면 전체적으로 불안정한 표현이 국지적으로 안정적인 특징의 집합으로 바뀝니다. 중요한 점은 서로 다른 스케일에서의 이러한 측정값은 아직 불변하지 않으며, 낮은 주파수로 갈수록 점진적으로 처리되어야 한다는 점으로, 이는 현대 신경망의 심층 구성 특성을 암시하며, 다음에 소개할 기하학적 딥러닝 청사진에 포착되어 있습니다. 학습 청사진에 담겨 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6195fe8",
   "metadata": {},
   "source": [
    "### Scale Separation Prior\n",
    "스케일 분리 사전\n",
    "\n",
    "We can build from this insight by considering a multiscale coarsening of the data domain $Ω$ into a hierarchy $Ω_1, . . . , Ω_J$. As it turns out, such coarsening can be defined on very general domains, including grids, graphs, and manifolds. Informally, a coarsening assimilates nearby points $u, u' ∈ Ω$ together, and thus only requires an appropriate notion of metric in the domain. If $X_j (Ω_j , C_j ) := {x_j : Ω_j → C_j}$ denotes signals defined over the coarsened domain $Ω_j$ , we informally say that a function $f : \\mathcal{X} (Ω) → \\mathcal{Y}$ is *locally stable* at scale $j$ if it admits a factorisation of the form $f ≈ f_j ◦ P_j$ , where $P_j : \\mathcal{X} (Ω) → \\mathcal{X}_j (Ω_j)$ is a non-linear *coarse graining* and $f_j : \\mathcal{X}_j (Ω_j ) → \\mathcal{Y}$. In other words, while the target function $f$ might depend on complex long-range interactions between features over the whole domain, in locally-stable functions it is possible to *separate* the interactions across scales, by first focusing on localised interactions that are then propagated towards the coarse scales.\n",
    "\n",
    "데이터 도메인 Ω을 계층 구조 Ω1, . . . . , ΩJ 로 거칠게 하는 것을 고려할 수 있습니다. 이러한 거칠기는 그리드, 그래프, 다양체 등 매우 일반적인 도메인에서 정의할 수 있습니다. 비공식적으로, 거칠기는 근처의 점 u, u0 ∈ Ω을 함께 동화시키므로 도메인에서 적절한 메트릭 개념만 필요합니다. $X_j (Ω_j , C_j ) := {x_j : Ω_j → C_j}$가 거칠어진 영역 Ωj에 정의된 신호를 나타내는 경우, 비공식적으로 함수 $f : \\mathcal{X} (Ω) → \\mathcal{Y}$ 가 f ≈ fj ◦ Pj 형식의 인수분해를 허용하는 경우, 여기서 $P_j : \\mathcal{X} (Ω) → \\mathcal{X}_j (Ω_j)$는 비선형 거칠기이고 $f_j : \\mathcal{X}_j (Ω_j ) → \\mathcal{Y}$는 국소적으로 안정하다고 말할 수 있습니다. 즉, 목표 함수 f는 전체 영역에 걸쳐 특징들 간의 복잡한 장거리 상호작용에 의존할 수 있지만, 국지적으로 안정적인 함수에서는 먼저 국지적인 상호작용에 초점을 맞춘 다음 거친 스케일로 전파함으로써 스케일 간 상호작용을 분리할 수 있습니다.\n",
    "\n",
    "Such principles are of fundamental importance in many areas of physics and mathematics, as manifested for instance in statistical physics in the so-called renormalisation group, or leveraged in important numerical algorithms such as the Fast Multipole Method. In machine learning, multiscale representations and local invariance are the fundamental mathematical principles underpinning the efficiency of Convolutional Neural Networks and Graph Neural Networks and are typically implemented in the form of *local pooling*. In future work, we will further develop tools from computational harmonic analysis that unify these principles across our geometric domains and will shed light onto the statistical learning benefits of scale separation.\n",
    "\n",
    "이러한 원리는 물리학 및 수학의 많은 분야에서 근본적으로 중요하며, 예를 들어 재노멀라이제이션 그룹의 통계 물리학에서 나타나거나 고속 다중극 방법과 같은 중요한 수치 알고리즘에서 활용됩니다. 머신 러닝에서 멀티스케일 표현과 국소 불변성은 컨볼루션 신경망과 그래프 신경망의 효율성을 뒷받침하는 기본 수학적 원리이며, 일반적으로 로컬 풀링의 형태로 구현됩니다. 향후 작업에서는 이러한 원리를 기하학적 영역 전반에 걸쳐 통합하고 규모 분리의 통계적 학습 이점을 조명하는 계산 고조파 분석 도구를 추가로 개발할 예정입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839f062",
   "metadata": {},
   "source": [
    "## 3.5 The Blueprint of Geometric Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f340825",
   "metadata": {},
   "source": [
    "The geometric principles of Symmetry, Geometric Stability, and Scale Separation discussed in Sections 3.1–3.4 can be combined to provide a universal blueprint for learning stable representations of high-dimensional data. These representations will be produced by functions f operating on signals $\\mathcal{X} (Ω, \\mathcal{C})$ defined on the domain $Ω$, which is endowed with a symmetry group $\\mathfrak{G}$. \n",
    "\n",
    "3.1~3.4절에서 설명한 대칭, 기하학적 안정성, 스케일 분리의 기하학적 원리를 결합하여 고차원 데이터의 안정적인 표현을 학습하기 위한 보편적인 청사진을 제공할 수 있습니다. 이러한 표현은 대칭 그룹 G가 부여된 도메인 Ω에 정의된 신호 $X(Ω, C)$에 대해 작동하는 함수 $f$에 의해 생성됩니다. \n",
    "\n",
    "The geometric priors we have described so far do not prescribe a specific architecture for building such representation, but rather a series of necessary conditions. However, they hint at an axiomatic construction that provably satisfies these geometric priors, while ensuring a highly expressive representation that can approximate any target function satisfying such priors. \n",
    "\n",
    "지금까지 설명한 기하학적 선행은 이러한 표현을 구축하기 위한 특정 아키텍처를 규정하는 것이 아니라 일련의 필수 조건을 규정합니다. 그러나 이러한 기하학적 선행 조건을 증명적으로 만족하는 공리적인 구조를 암시하는 동시에 이러한 선행 조건을 만족하는 모든 목표 함수에 근사치를 구할 수 있는 표현력이 뛰어난 표현을 보장합니다. \n",
    "\n",
    "A simple initial observation is that, in order to obtain a highly expressive representation, we are required to introduce a non-linear element, since if $f$ is linear and G-invariant, then for all $x ∈ \\mathcal{X} (Ω)$,\n",
    "\n",
    "간단한 첫 번째 관찰은 표현력이 높은 표현을 얻기 위해 비선형 요소를 도입해야 한다는 것입니다. f가 선형이고 G-변수인 경우 모든 x ∈ X (Ω)에 대해 비선형 요소를 도입해야 하기 때문입니다,\n",
    "\n",
    "<p>\n",
    "$$\n",
    "f(x) = \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}} f(\\mathfrak{g}.x)dµ(\\mathfrak{g}) = f \\left( \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}} (\\mathfrak{g}.x)dµ(\\mathfrak{g}) \\right)\n",
    "$$\n",
    "</p>\n",
    "\n",
    ">Here, $µ(g)$ is known as the Haar measure of the group $\\mathcal{G}$, and the integral is performed over the entire group.\n",
    ">\n",
    ">여기서 $µ(g)$는 그룹 G의 하르 측정값으로 알려져 있으며 적분은 전체 그룹에 대해 수행됩니다.\n",
    "\n",
    "which indicates that $F$ only depends on x through the $\\mathfrak{G}$-average $Ax = \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}}(\\mathfrak{g}.x)dµ(\\mathfrak{g})$. In the case of images and translation, this would entail using only the average RGB color of the input! \n",
    "\n",
    "이는 $F$가 $\\mathfrak{G}$-평균 $Ax = \\frac{1}{µ(\\mathfrak{G})} \\int_{\\mathfrak{G}}(\\mathfrak{g}.x)dµ(\\mathfrak{g})$ 를 통해 x에만 의존한다는 것을 나타냅니다. 이미지 및 번역의 경우 입력의 평균 RGB 색상만 사용해야 합니다!\n",
    "\n",
    "While this reasoning shows that the family of *linear invariantsis* not a very rich object, the family of *linear equivariants* provides a much more powerful tool, since it enables the construction of rich and stable features by composition with appropriate non-linear maps, as we will now explain. Indeed, if $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω, \\mathcal{C'})$ is $\\mathfrak{G}$-equivariant satisfying $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$ for all $x ∈ \\mathcal{X}$ and $\\mathfrak{g} ∈ \\mathfrak{G}$, and $σ : \\mathcal{C'} → \\mathcal{C''}$ is an arbitrary (non-linear) map, then we easily verify that the composition $U := (σ ◦ B) : X (Ω, \\mathcal{C}) → X (Ω, \\mathcal{C''})$ is also $\\mathfrak{G}$-equivariant, where $σ : X (Ω,\\mathcal{C'} ) → X (Ω, \\mathcal{C''})$ is the element-wise instantiation of σ given as $(σ(x))(u) := σ(x(u))$.\n",
    "\n",
    "이 추론은 선형 불변량군이 그다지 풍부한 객체는 아니지만, 선형 등식군은 이제 설명하겠지만 적절한 비선형 맵과 함께 구성되어 더욱 풍부하고 안정적인 기능을 구성할 수 있기 때문에 훨씬 더 강력한 도구를 제공합니다. 실제로 B : X (Ω, C) → X (Ω, C 0 )가 모든 x ∈ X와 g ∈ G에 대해 B(g.x) = g.B(x)를 만족하는 G 등식이고 σ : C 0 → C00이 임의의 (비선형) 맵이라면, 우리는 쉽게 구성 U := (σ ◦ B) : X (Ω, C) → X (Ω, C 00) 역시 G 등식이며, 여기서 σ : X (Ω, C 0 ) → X (Ω, C 00)는 (σ(x))(u) := σ(x(u))로 주어진 σ의 원소별 인스턴스화입니다.\n",
    "\n",
    "This simple property allows us to define a very general family of G-invariants, by composing $U$ with the group averages $A ◦ U : X (Ω, \\mathcal{C}) → \\mathcal{C''}$. A natural question is thus whether any $\\mathfrak{G}$-invariant function can be approximated at arbitrary precision by such a model, for appropriate choices of $B$ and $σ$. It is not hard to adapt the standard Universal Approximation Theorems from unstructured vector inputs to show that shallow ‘geometric’ networks are also universal approximators, by properly generalising the group average to a general non-linear invariant. However, as already described in the case of Fourier versus Wavelet invariants, there is a fundamental tension between shallow global invariance and deformation stability. This motivates an alternative representation, which considers instead localised equivariant maps. Assuming that $Ω$ is further equipped with a distance metric d, we call an equivariant map U localised if $(Ux)(u)$ depends only on the values of $x(v)$ for $N_u = {v : d(u, v) ≤ r}$, for some small radius $r$; the latter set $N_u$ is called the *receptive field*. \n",
    "\n",
    "이 간단한 속성을 통해 우리는 U를 그룹 평균 $A ◦ U : X (Ω, \\mathcal{C}) → \\mathcal{C''}$ 으로 구성하여 매우 일반적인 G-불변량 군을 정의할 수 있습니다. 따라서 자연스러운 질문은 $B$와 $σ$의 적절한 선택에 대해 이러한 모델에 의해 임의의 정밀도로 G-불변 함수를 근사화할 수 있는지 여부입니다. 구조화되지 않은 벡터 입력에서 표준 범용 근사 정리를 적용하여 그룹 평균을 일반 비선형 불변량으로 적절히 일반화함으로써 얕은 '기하학적' 네트워크도 범용 근사화자임을 보여주는 것은 어렵지 않습니다. 그러나 푸리에 불변수와 웨이블릿 불변수의 경우에서 이미 설명했듯이, 얕은 전역 불변성과 변형 안정성 사이에는 근본적인 긴장이 존재합니다. 따라서 국소화된 등변량 맵을 고려하는 대안적 표현이 필요합니다. $Ω$에 거리 메트릭 d가 추가로 장착되어 있다고 가정할 때, 작은 반지름 $r$에 대해 $N_u = {v : d(u, v) ≤ r}$에 대해 $(Ux)(u)$가 $x(v)$의 값에만 의존하는 경우 등변량 맵 U를 국소화된 맵이라고 하며, 후자의 집합 Nu를 수용장이라고 부릅니다.\n",
    "\n",
    ">Such proofs have been demonstrated, for example, for the Deep Sets model by Zaheer et al. (2017).\n",
    "\n",
    ">Meaningful metrics can be defined on grids, graphs, manifolds, and groups. A notable exception are sets, where there is no predefined notion of metric.\n",
    ">\n",
    ">의미 있는 메트릭은 그리드, 그래프, 다양체 및 그룹에 정의할 수 있습니다. 주목할 만한 예외는 메트릭에 대한 사전 정의된 개념이 없는 집합입니다.\n",
    "\n",
    "A single layer of local equivariant map $U$ cannot approximate functions with long-range interactions, but a composition of several local equivariant maps $U_J ◦ U_J−1 · · · ◦ U_1$ increases the receptive field while preserving the stability properties of local equivariants. The receptive field is further increased by interleaving downsampling operators that coarsen the domain (again assuming a metric structure), completing the parallel with Multiresolution Analysis (MRA, see e.g. Mallat (1999)). \n",
    "\n",
    "단일 계층의 국부 등변량 맵 U는 장거리 상호작용이 있는 함수를 근사화할 수 없는 반면, 여러 국부 등변량 맵 UJ ◦ UJ-1 - - - ◦ U1의 구성은 국부 등변량의 안정성 특성을 보존하면서 수용 필드를 증가시킵니다. 영역을 거칠게 하는 다운샘플링 연산자를 인터리빙하여(다시 메트릭 구조를 가정하여) 수용 필드를 더욱 증가시켜 다중해상도 분석(MRA, Mallat (1999) 참조)과 병렬로 완성합니다. \n",
    "\n",
    ">The term ‘receptive field’ originated in the neuroscience literature, referring to the spatial domain that affects the output of a given neuron.\n",
    ">\n",
    ">'수용 필드'라는 용어는 신경과학 문헌에서 유래한 것으로, 주어진 뉴런의 출력에 영향을 미치는 공간 영역을 의미합니다.\n",
    "\n",
    "In summary, the geometry of the input domain, with knowledge of an underyling symmetry group, provides three key building blocks: (i) a local equivariant map, (ii) a global invariant map, and (iii) a coarsening operator. These building blocks provide a rich function approximation space with prescribed invariance and stability properties by combining them together in a scheme we refer to as the Geometric Deep Learning Blueprint (Figure 8)\n",
    "\n",
    "요약하면, 언더링 대칭 그룹에 대한 지식이 있는 입력 영역의 기하학은 (i) 로컬 등변량 맵, (ii) 글로벌 불변량 맵, (iii) 조도(coarsening) 연산자라는 세 가지 주요 구성 요소를 제공합니다. 이러한 빌딩 블록은 기하학적 딥 러닝 블루프린트라고 하는 체계에서 함께 결합하여 규정된 불변성 및 안정성 속성을 갖춘 풍부한 함수 근사화 공간을 제공합니다(그림 8)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec7d4f15",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "Geometric Deep Learning Blueprint\n",
    "\n",
    "Let $Ω$ and $Ω'$ be domains, $\\mathfrak{G}$ a symmetry group over $Ω$, and write $Ω' ⊆ Ω$ if $Ω'$ can be considered a compact version of $Ω$.\n",
    "\n",
    "$Ω$과  $Ω'$을 영역으로, $\\mathfrak{G}$를 $Ω$에 대한 대칭 그룹으로 하고, $Ω' ⊆ Ω$ 이면 $Ω'$ 를  $Ω$의 압축 버전으로 간주할 수 있습니다.\n",
    "\n",
    "We define the following building blocks:\n",
    "\n",
    "다음과 같은 빌딩 블록을 정의합니다:\n",
    "\n",
    "\n",
    ">Linear $\\mathfrak{G}$-equivariant layer $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C'}) \\text{ satisfying } B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x) \\text{ for all } \\mathfrak{g} ∈ \\mathfrak{G} \\text{ and } x ∈ \\mathcal{X} (Ω, \\mathcal{C}).$\n",
    ">\n",
    ">선형 𝔊 -등변량 레이어 $B : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C'})$ 는 모든 $\\mathfrak{g} ∈ \\mathfrak{G}$ 및 $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$에 대해 $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$ 를 만족합니다.\n",
    "\n",
    ">Nonlinearity $σ : \\mathcal{C} → \\mathcal{C'}$ applied element-wise as $(σ(x))(u) = σ(x(u))$.\n",
    ">\n",
    ">비선형성 $σ : \\mathcal{C} → \\mathcal{C'}$는 $(σ(x))(u) = σ(x(u))$로 요소별로 적용됩니다\n",
    "\n",
    ">Local pooling (coarsening) $P : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C})$, such that $Ω' ⊆ Ω$.\n",
    ">\n",
    ">로컬 풀링(거칠게 함) $P : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{X} (Ω', \\mathcal{C})$, 즉, $Ω' ⊆ Ω$\n",
    "\n",
    ">G-invariant layer (global pooling) $A : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{Y}$ satisfying $A(\\mathfrak{g}.x) = A(x)$ for all $\\mathfrak{g} ∈ \\mathfrak{G}$ and $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$.\n",
    ">\n",
    "> G-변수 레이어(글로벌 풀링) $A : \\mathcal{X} (Ω, \\mathcal{C}) → \\mathcal{Y}$는 모든 $\\mathfrak{g} ∈ \\mathfrak{G}$ 및 $x ∈ \\mathcal{X} (Ω, \\mathcal{C})$에 대해 $A(\\mathfrak{g}.x) = A(x)$를 만족합니다.\n",
    "\n",
    "Using these blocks allows constructing G-invariant functions $f :\\mathfrak{X} (Ω, \\mathfrak{C}) → \\mathfrak{Y}$ of the form\n",
    "\n",
    "이러한 블록을 사용하면 다음과 같은 형식의 G-변수 함수 $f :\\mathfrak{X} (Ω, \\mathfrak{C}) → \\mathfrak{Y}$를 구성할 수 있습니다.\n",
    "\n",
    "$$\n",
    "    f = A ◦ σ_J ◦ B_J ◦ P_{J−1} ◦ . . . ◦ P_1 ◦ σ_1 ◦ B_1\n",
    "$$\n",
    "\n",
    "where the blocks are selected such that the output space of each block\n",
    "matches the input space of the next one. Different blocks may exploit\n",
    "different choices of symmetry groups $\\mathfrak{G}$.\n",
    "\n",
    "여기서 각 블록의 출력 공간이 다음 블록의 입력 공간과 일치하도록 블록이 선택됩니다. 다른 블록은 다른 대칭 그룹 𝔊을 고를 수 있습니다.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4307f",
   "metadata": {},
   "source": [
    "### Different settings of Geometric Deep Learning\n",
    "\n",
    "기하학적 딥러닝의 다양한 설정\n",
    "\n",
    "One can make an important distinction between the setting when the domain Ω is assumed to be fixed and one is only interested in varying input signals defined on that domain, or the domain is part of the input as varies together with signals defined on it. A classical instance of the former case is encountered in computer vision applications, where images are assumed to be defined on a fixed domain (grid). Graph classification is an example of the latter setting, where both the structure of the graph as well as the signal defined on it (e.g. node features) are important. In the case of varying domain, geometric stability (in the sense of insensitivity to the deformation of Ω) plays a crucial role in Geometric Deep Learning architecture.\n",
    "\n",
    "도메인 Ω이 고정된 것으로 가정하고 해당 도메인에 정의된 다양한 입력 신호에만 관심이 있는 경우와 도메인에 정의된 신호와 함께 도메인이 입력의 일부인 경우의 설정을 구분할 수 있습니다. 전자의 전형적인 사례는 이미지가 고정된 도메인(그리드)에 정의되어 있다고 가정하는 컴퓨터 비전 애플리케이션에서 볼 수 있습니다. 그래프 분류는 그래프의 구조와 그래프에 정의된 신호(예: 노드 특징)가 모두 중요한 후자의 설정에 대한 예입니다. 다양한 도메인의 경우, 기하학적 안정성(Ω의 변형에 민감하지 않다는 의미에서)이 기하학적 딥 러닝 아키텍처에서 중요한 역할을 합니다.\n",
    "\n",
    "This blueprint has the right level of generality to be used across a wide range of geometric domains. Different Geometric Deep Learning methods thus differ in their choice of the domain, symmetry group, and the specific implementation details of the aforementioned building blocks. As we will see in the following, a large class of deep learning architectures currently in use fall into this scheme and can thus be derived from common geometric principles. \n",
    "\n",
    "이 청사진은 다양한 기하학적 영역에서 사용할 수 있는 적절한 수준의 범용성을 갖추고 있습니다. 따라서 기하학적 딥러닝 방법마다 도메인, 대칭 그룹, 앞서 언급한 빌딩 블록의 구체적인 구현 세부 사항에 대한 선택이 다릅니다. 아래에서 살펴보겠지만, 현재 사용 중인 많은 종류의 딥러닝 아키텍처가 이 체계에 속하며, 따라서 일반적인 기하학적 원리에서 파생될 수 있습니다. \n",
    "\n",
    "In the following sections (4.1–4.6) we will describe the various geometric domains focusing on the ‘5G’, and in Sections 5.1–5.8 the specific implementations of Geometric Deep Learning on these domains. \n",
    "\n",
    "다음 섹션(4.1-4.6)에서는 '5G' 에 초점을 맞춘 다양한 기하학적 영역에 대해 설명하고, 섹션 5.1-5.8에서는 이러한 영역에서 기하학적 딥러닝의 구체적인 구현을 설명합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336294b9",
   "metadata": {},
   "source": [
    "| Architecture         | Domain Ω       | Symmetry group G                            |\n",
    "|----------------------|----------------|---------------------------------------------|\n",
    "| CNN                  | Grid           | Translation                                 |\n",
    "| Spherical CNN        | Sphere / SO(3)  | Rotation SO(3)                              |\n",
    "| Intrinsic / Mesh CNN | Manifold       | Isometry Iso(Ω) <br>   Gauge symmetry SO(2) |\n",
    "| GNN                  | Graph          | Permutation Σn                              |\n",
    "| Deep Sets            | Set            | Permutation Σn                              |\n",
    "| Transformer          | Complete Graph | Permutation Σn                              |\n",
    "| LSTM                 | 1D Grid        | Time warping                                |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
