{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946222d2",
   "metadata": {},
   "source": [
    "# Ch.3 Geometric Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8b91e9",
   "metadata": {},
   "source": [
    ">Sources\n",
    ">\n",
    ">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges https://arxiv.org/abs/2104.13478\n",
    ">\n",
    ">AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 3 (Geometric Priors I) - Taco Cohen https://youtu.be/qEjWMhRlXgY\n",
    ">\n",
    "> AMMI 2022 Course \"Geometric Deep Learning\" - Lecture 4 (Geometric Priors II) - Joan Bruna https://youtu.be/DpnA8NNUtyU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96885ecc",
   "metadata": {},
   "source": [
    "There is hope for physically-structured data, where we can employ two fundamental principles: *symmetry* and *scale separation*.\n",
    "\n",
    "ëŒ€ì¹­(symmetry)ê³¼ ìŠ¤ì¼€ì¼ ë¶„ë¦¬(scale separation)ë¼ëŠ” ë‘ ê°€ì§€ ê¸°ë³¸ ì›ì¹™ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¬¼ë¦¬ì  êµ¬ì¡° ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” í¬ë§ì´ ìˆìŠµë‹ˆë‹¤\n",
    "\n",
    "We will assume that our machine learning system operates on signals (functions) on some domain â„¦.\n",
    "\n",
    "í•™ìŠµ ì‹œìŠ¤í…œì€ ì–´ë–¤ ë„ë©”ì¸ Î©ì˜ ì‹ í˜¸(í•¨ìˆ˜)ì— ëŒ€í•´ ì‘ë™í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a81e8",
   "metadata": {},
   "source": [
    "While in many cases linear combinations of points on â„¦ is not well-defined, we can linearly combine signals on it, i.e., the space of signals forms a vector space. Moreover, since we can define an inner product between signals, this space is a Hilbert space.\n",
    "\n",
    "ë§ì€ ê²½ìš° Î©ì— ìˆëŠ” ì ì˜ ì„ í˜• ì¡°í•©ì€ ì˜ ì •ì˜ë˜ì§€ ì•Šì§€ë§Œ ì‹ í˜¸ë¥¼ ì„ í˜•ìœ¼ë¡œ ê²°í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì‹ í˜¸ ê³µê°„ì´ ë²¡í„° ê³µê°„ì„ í˜•ì„±í•©ë‹ˆë‹¤. ë˜í•œ ì‹ í˜¸ ì‚¬ì´ì˜ ë‚´ì ì„ ì •ì˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ ê³µê°„ì€ Hilbert ê³µê°„ì…ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c3265b4",
   "metadata": {},
   "source": [
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/yckiapQlruY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93175e66",
   "metadata": {},
   "source": [
    "The space of C-valued signals on â„¦ (for â„¦ a set, possibly with additional\n",
    "structure, and C a vector space, whose dimensions are called channels)\n",
    "\n",
    "Î©ìœ„ì—ì„œ C ê°’ ì‹ í˜¸ì˜ ê³µê°„ì— ëŒ€í•˜ì—¬(Î©ì˜ ê²½ìš° ì¶”ê°€ êµ¬ì¡°ê°€ ìˆì„ ìˆ˜ ìˆëŠ” ì§‘í•©, Cì˜ ê²½ìš° ì°¨ì›ì„ ì±„ë„ì´ë¼ê³  í•˜ëŠ” ë²¡í„° ê³µê°„)\n",
    "\n",
    "$$\n",
    "   \\mathcal{X}(\\Omega, \\mathcal{C})  = \\{x:\\Omega \\rightarrow \\mathcal{C} \\} \n",
    "$$\n",
    "\n",
    "is a function space that has a vector space structure. Addition and scalar\n",
    "multiplication of signals is defined as:\n",
    "\n",
    "ëŠ” ë²¡í„° ê³µê°„ êµ¬ì¡°ë¥¼ ê°–ëŠ” í•¨ìˆ˜ ê³µê°„ì…ë‹ˆë‹¤. ì‹ í˜¸ì˜ ë§ì…ˆê³¼ ìŠ¤ì¹¼ë¼ ê³±ì…ˆì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "$$\n",
    "(\\alpha x + \\beta y)(u) = \\alpha x (u) + \\beta y(u) \\quad \\text{for all }\\quad u \\in \\Omega\n",
    "$$\n",
    "<br>\n",
    "\n",
    "with real scalars Î±, Î². Given an inner product $\\langle v,w \\rangle_{\\mathcal{C}}$ on C and a measure Âµ on â„¦ (with respect to which we can define an integral), we can define\n",
    "an inner product on X (â„¦, C) as\n",
    "\n",
    "ì‹¤ìˆ˜ ìŠ¤ì¹¼ë¼ Î±, Î²ì— ëŒ€í•˜ì—¬. ë‚´ì  ë° Î©ì— ìˆëŠ” Î¼ì— ëŒ€í•œ ì¸¡ì •(ì´ì™€ ê´€ë ¨í•˜ì—¬ ì ë¶„ì„ ì •ì˜í•  ìˆ˜ ìˆìŒ)ê°€ ì£¼ì–´ì§€ë©´ X(Î©, C)ì— ëŒ€í•œ ë‚´ì ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "   \\langle x,y \\rangle = \\int_{\\Omega}\\langle x (u),y(u) \\rangle_{\\mathcal{C}} \\: d\\mu (u)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc4711",
   "metadata": {},
   "source": [
    "When â„¦ has some additional structure, we may further restrict the kinds of signals in X (â„¦, C). For example, when â„¦ is a smooth manifold, we may require the signals to be smooth. Whenever possible, we will omit the range C for brevity.\n",
    "\n",
    "Î©ì— ì¶”ê°€ êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°, X(Î©, C)ì˜ ì‹ í˜¸ ì¢…ë¥˜ë¥¼ ì œí•œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ Î©ì´ í‰í™œ ë‹¤ì–‘ì²´(smooth manifold)ì¼ ë•Œ, ì‹ í˜¸ê°€ í‰í™œ(smooth)í•´ì•¼ ë¨ì„ ìš”êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ëŠ¥í•˜ë©´ ê°„ê²°í•¨ì„ ìœ„í•´ ë²”ìœ„ Cë¥¼ ìƒëµí•©ë‹ˆë‹¤.\n",
    "\n",
    "When the domain â„¦ is discrete, Âµ can be chosen as the counting measure, in which case the integral becomes a sum. In the following, we will omit the measure and use du for brevity.\n",
    "\n",
    "ë„ë©”ì¸ Î©ì´ ì´ì‚°(discrete)ì ì¼ ë•Œ Î¼ëŠ” ê³„ìˆ˜ ì¸¡ì •(counting measure)ìœ¼ë¡œ ì„ íƒë  ìˆ˜ ìˆìœ¼ë©°, ì´ ê²½ìš° ì ë¶„ì€ í•©ì´ ë©ë‹ˆë‹¤. ë‹¤ìŒì—ì„œëŠ” ì¸¡ì •ì„ ìƒëµí•˜ê³  ê°„ê²°í•¨ì„ ìœ„í•´ duë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ccbad",
   "metadata": {},
   "source": [
    "Scale separation results from our ability to preserve important characteristics of the signal when transferring it onto a coarser version of the domain (in our example, subsampling the image by coarsening the underlying grid).\n",
    "\n",
    "ìŠ¤ì¼€ì¼ ë¶„ë¦¬ëŠ” ì‹ í˜¸ë¥¼ ë” ê±°ì¹œ(coarser) ë²„ì „ì˜ ë„ë©”ì¸ìœ¼ë¡œ ì „ì†¡í•  ë•Œ ì‹ í˜¸ì˜ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ë³´ì¡´í•˜ëŠ” ëŠ¥ë ¥ì—ì„œ ë¹„ë¡¯ë©ë‹ˆë‹¤(ì´ ì˜ˆì—ì„œëŠ” ê¸°ë³¸ ê·¸ë¦¬ë“œë¥¼ ê±°ì¹ ê²Œ í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì„œë¸Œìƒ˜í”Œë§í•˜ëŠ” ê²½ìš°)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5b01a",
   "metadata": {},
   "source": [
    "In the case of images considered above, geometric priors are built into Convolutional Neural Networks (CNNs) in the form of convolutional filters with\n",
    "shared weights (exploiting translational symmetry) and pooling (exploiting\n",
    "scale separation).\n",
    "\n",
    "ìœ„ì—ì„œ ê³ ë ¤í•œ ì´ë¯¸ì§€ì˜ ê²½ìš° ê¸°í•˜í•™ì  ì‚¬ì „ì€ ê³µìœ  ê°€ì¤‘ì¹˜(ë³€í™˜ ëŒ€ì¹­-translational symmetry ì´ìš©) ë° í’€ë§(ì²™ë„ ë¶„ë¦¬-scale separation ì´ìš©)ì´ ìˆëŠ” ì»¨ë³¼ë£¨ì…˜ í•„í„°ì˜ í˜•íƒœë¡œ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§(CNN)ì— êµ¬ì¶•ë©ë‹ˆë‹¤.\n",
    "\n",
    "Extending these ideas to other domains such as graphs and manifolds and showing how geometric priors emerge from fundamental principles is the main goal of Geometric Deep Learning and the *leitmotif* of our text.\n",
    "\n",
    "ì´ëŸ¬í•œ ì•„ì´ë””ì–´ë¥¼ ê·¸ë˜í”„ ë° ë‹¤ì–‘ì²´ì™€ ê°™ì€ ë‹¤ë¥¸ ì˜ì—­ìœ¼ë¡œ í™•ì¥í•˜ê³  ê¸°ë³¸ ì›ë¦¬ì—ì„œ ê¸°í•˜í•™ì  ì‚¬ì „ì´ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ê¸°í•˜í•™ì  ë”¥ ëŸ¬ë‹ì˜ ì£¼ìš” ëª©í‘œì´ì í…ìŠ¤íŠ¸ì˜ ì£¼ì œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabbbbe",
   "metadata": {},
   "source": [
    "## 3.1 Symmetries, Representations, and Invariance\n",
    "3.1 ëŒ€ì¹­, í‘œí˜„ ë° ë¶ˆë³€ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01325fd",
   "metadata": {},
   "source": [
    "Informally, a *symmetry* of an object or system is a transformation that leaves\n",
    "a certain property of said object or system unchanged or *invariant*. Such\n",
    "transformations may be either smooth, continuous, or discrete.\n",
    "\n",
    "í¸í•˜ê²Œ ë§í•˜ìë©´, ê°œì²´ ë˜ëŠ” ì‹œìŠ¤í…œì˜ ëŒ€ì¹­ì€ í•´ë‹¹ ê°œì²´ ë˜ëŠ” ì‹œìŠ¤í…œì˜ íŠ¹ì • ì†ì„±ì„ ë³€ê²½í•˜ì§€ ì•Šê±°ë‚˜ ë¶ˆë³€ìœ¼ë¡œ ë‚¨ê²¨ë‘ëŠ” ë³€í˜•ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í™˜ì€ ë§¤ë„ëŸ½ê±°ë‚˜ ì—°ì†ì ì´ê±°ë‚˜ ë¶ˆì—°ì†ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a51a8d5",
   "metadata": {},
   "source": [
    "### Symmetry groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12454bc1",
   "metadata": {},
   "source": [
    "We will follow the juxtaposition notation convention used in group theory, $\\mathfrak{g â—¦ h = gh}$, which should be read right-to-left: we first apply $\\mathfrak{h}$ and then $\\mathfrak{g}$. The order is important, as in many cases symmetries are non-commutative. \n",
    "\n",
    "ê·¸ë£¹ ì´ë¡ ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë³‘ì¹˜ í‘œê¸°ë²• g â—¦ h = ghë¥¼ ë”°ë¥´ë©° ì˜¤ë¥¸ìª½ì—ì„œ ì™¼ìª½ìœ¼ë¡œ ì½ì–´ì•¼ í•©ë‹ˆë‹¤. ë¨¼ì € hë¥¼ ì ìš©í•œ ë‹¤ìŒ gë¥¼ ì ìš©í•©ë‹ˆë‹¤. ë§ì€ ê²½ìš° ëŒ€ì¹­ì´ ë¹„ê°€í™˜ì ì´ê¸° ë•Œë¬¸ì— ìˆœì„œê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "the Fraktur font to denote group elements.\n",
    "\n",
    "Fraktur ê¸€ê¼´ì€ ê·¸ë£¹ ìš”ì†Œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698beb36",
   "metadata": {},
   "source": [
    "#### group axioms\n",
    "<div style=\"background-color:Gainsboro\">\n",
    "A group is a set G along with a binary operation $\\mathfrak{â—¦ : G Ã— G â†’ G}$ called composition (for brevity, denoted by juxtaposition g â—¦ h = gh) satisfying the following axioms:\n",
    "\n",
    "Associativity: $\\mathfrak{(gh)k = g(hk)} \\text{ for all } \\mathfrak{g, h,k âˆˆ G}$\n",
    "\n",
    "Identity: there exists a unique $\\mathfrak{e âˆˆ G}$ satisfying $\\mathfrak{eg = ge = g}$ for all $\\mathfrak{g âˆˆ G}$.\n",
    "\n",
    "Inverse: For each $\\mathfrak{g âˆˆ G}$ there is a unique inverse $\\mathfrak{g\n",
    "^{âˆ’1} âˆˆ G}$ such that $\\mathfrak{gg^{âˆ’1} = g^{âˆ’1}g = e}$.\n",
    "\n",
    "Closure: The group is closed under composition, i.e., for every $\\mathfrak{g, h âˆˆ G}$, we have $\\mathfrak{gh âˆˆ G}$.\n",
    "</div>\n",
    "ê·¸ë£¹ì€ ì´ì§„ ì—°ì‚° â—¦ : G Ã— G â†’ G ë¼ê³  í•˜ëŠ” êµ¬ì„±(ê°„ê²°í•¨ì„ ìœ„í•´ g â—¦ h = ghë¡œ ë³‘ì¹˜ë¡œ í‘œì‹œ)ê³¼ í•¨ê»˜ ë‹¤ìŒ ê³µë¦¬ë¥¼ ì¶©ì¡±í•˜ëŠ” ì§‘í•© Gì…ë‹ˆë‹¤.\n",
    "\n",
    "ì—°ê´€ì„±\n",
    "ë™ì¼ì„±\n",
    "ì—­\n",
    "íì‡„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b1dea",
   "metadata": {},
   "source": [
    "Note that commutativity is not part of this definition, i.e. we may have $\\mathfrak{gh \\not = hg}$. Groups for which $\\mathfrak{gh = hg}$ for all $\\mathfrak{g, h âˆˆ G}$ are called commutative or Abelian.\n",
    "\n",
    "êµí™˜ì„±ì€ ì´ ì •ì˜ì˜ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤. ì¦‰, $\\mathfrak{gh \\not = hg}$ ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  $\\mathfrak{g, h âˆˆ G}$ ì— ëŒ€í•´ $\\mathfrak{gh = hg}$ ì¸ ê·¸ë£¹ì„ ê°€í™˜ì„±/êµí™˜ì„± ë˜ëŠ” Abelianì´ë¼ê³  í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266407de",
   "metadata": {},
   "source": [
    "Formally, $\\mathfrak{G}$ is said to be generated by a subset $\\mathcal{S} âŠ† \\mathfrak{G}$ (called the group generator) if every element $\\mathfrak{g âˆˆ G}$ can be written as a finite composition of the elements of $\\mathcal{S}$ and their inverses.\n",
    "\n",
    "ê³µì‹ì ìœ¼ë¡œ, GëŠ” ëª¨ë“  ìš”ì†Œ g âˆˆ Gê°€ Sì˜ ìš”ì†Œì™€ ê·¸ ì—­ì›ì˜ ìœ í•œ êµ¬ì„±ìœ¼ë¡œ ì“°ì—¬ì§ˆ ìˆ˜ ìˆëŠ” ê²½ìš° ë¶€ë¶„ ì§‘í•© S âŠ† G(ê·¸ë£¹ ìƒì„±ê¸°ë¼ê³  í•¨)ì— ì˜í•´ ìƒì„±ëœë‹¤ê³  í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf67df4",
   "metadata": {},
   "source": [
    "Note that here we have defined a group as an abstract object, without saying what the group elements are (e.g. transformations of some domain), only how they compose. Hence, very different kinds of objects may have the same symmetry group.\n",
    "\n",
    "ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ê·¸ë£¹ ìš”ì†Œê°€ ë¬´ì—‡ì¸ì§€(ì˜ˆ: ì¼ë¶€ ë„ë©”ì¸ì˜ ë³€í™˜) êµ¬ì„± ìš”ì†Œê°€ ë¬´ì—‡ì¸ì§€ëŠ” ë§í•˜ì§€ ì•Šê³ , ì˜¤ì§ ì–´ë–»ê²Œ ê·¸ê²ƒë“¤ì´ êµ¬ì„±ë˜ëŠ”ì§€ì— ëŒ€í•´ì„œë§Œ,ì¶”ìƒ ê°ì²´ë¡œ ê·¸ë£¹ì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë§¤ìš° ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ê°ì²´ê°€ ë™ì¼í•œ ëŒ€ì¹­ ê·¸ë£¹ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd2ccf",
   "metadata": {},
   "source": [
    "### Group Actions and Group Representations\n",
    "\n",
    "ê·¸ë£¹ ì‘ì—… ë° ê·¸ë£¹ í‘œí˜„"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4bc28de",
   "metadata": {},
   "source": [
    "Rather than considering groups as abstract entities, we are mostly interested in how groups *act* on data. Since we assumed that there is some domain â„¦ underlying our data, we will study how the group acts on â„¦ (e.g. translation of points of the plane), and from there obtain actions of the same group on the space of signals $\\mathcal{X (\\Omega)}$ (e.g. translations of planar images and feature maps).\n",
    "\n",
    "ê·¸ë£¹ì„ ì¶”ìƒì ì¸ ì—”í„°í‹°ë¡œ ê°„ì£¼í•˜ê¸°ë³´ë‹¤ëŠ”, ê·¸ë£¹ì´ ë°ì´í„°ì— ëŒ€í•´ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€ì— ì£¼ë¡œ ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ ê¸°ì €ì— ë„ë©”ì¸ Î©ì´ ìˆë‹¤ê³  ê°€ì •í–ˆê¸° ë•Œë¬¸ì— ê·¸ë£¹ì´ Î©ì— ëŒ€í•´ ì–´ë–»ê²Œ ì‘ìš©í•˜ëŠ”ì§€ ì—°êµ¬í•˜ê³ (ì˜ˆ: í‰ë©´ ì ì˜ ë³€í™˜) ê±°ê¸°ì—ì„œ ì‹ í˜¸ X(Î©)ì˜ ê³µê°„ì—ì„œ ë™ì¼í•œ ê·¸ë£¹ì˜ ì‘ìš©ì„ ì–»ìŠµë‹ˆë‹¤. (ì˜ˆ: í‰ë©´ ì´ë¯¸ì§€ ë° ê¸°ëŠ¥ ë§µì˜ ë²ˆì—­).\n",
    "\n",
    "A *group action* of $\\mathfrak{G}$ on a set $\\Omega$ is defined as a mapping $(\\mathfrak{g}, u) \\mapsto \\mathfrak{g} \\cdot u$ associating a group element $\\mathfrak{g âˆˆ G}$ and a point $u âˆˆ \\Omega$ with some other point on â„¦ in a way that is compatible with the group operations, i.e., $\\mathfrak{g \\cdot (h \\cdot} u) = \\mathfrak{(gh)} \\cdot u \\text{ for all } \\mathfrak{g, h âˆˆ G} \\text{ and } u âˆˆ \\Omega.$\n",
    "\n",
    "ì§‘í•©(set) Î©ì— ëŒ€í•œ Gì˜ ê·¸ë£¹ ì‘ì—…ì€ ê·¸ë£¹ ìš”ì†Œ g âˆˆ Gì™€ ì  u âˆˆ Î©ì„ ê·¸ë£¹ ì‘ì—…ê³¼ í˜¸í™˜ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ Î©ì˜ ë‹¤ë¥¸ ì ê³¼ ì—°ê²°í•˜ëŠ” ë§¤í•‘ $(\\mathfrak{g}, u) \\mapsto \\mathfrak{g} \\cdot u$ ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì¦‰ ëª¨ë“  g, h âˆˆ G ë° u âˆˆ Î©ì— ëŒ€í•´ g.(h.u) = (gh).uì…ë‹ˆë‹¤.\n",
    "\n",
    " We shall see numerous instances of group actions in the following sections. For example, in the plane the Euclidean group E(2) is the group of transformations of $\\mathbb{R^{2}}$ that preserves Euclidean distances, and consists of translations, rotations, and reflections. The same group, however, can also act on the space of images on the plane (by translating, rotating and flipping the grid of pixels), as well as on the representation spaces learned by a neural network. More precisely, if we have a group $\\mathfrak{G}$ acting on $\\Omega$, we automatically obtain an action of $\\mathfrak{G}$ on the space $\\mathcal{X}(\\Omega)$:\n",
    "\n",
    "ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ê·¸ë£¹ ì‘ì—…ì˜ ìˆ˜ë§ì€ ì‚¬ë¡€ë¥¼ ë³¼ ê²ƒì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í‰ë©´ì—ì„œ ìœ í´ë¦¬ë“œ ê·¸ë£¹ E(2)ëŠ” ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ìœ ì§€í•˜ê³  ë³€í™˜, íšŒì „ ë° ë°˜ì‚¬ë¡œ êµ¬ì„±ëœ R 2 ì˜ ë³€í™˜ ê·¸ë£¹ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë™ì¼í•œ ê·¸ë£¹ì€ í‰ë©´ì˜ ì´ë¯¸ì§€ ê³µê°„(í”½ì…€ ê²©ìë¥¼ ë³€í™˜, íšŒì „ ë° ë’¤ì§‘ê¸°)ê³¼ ì‹ ê²½ë§ì—ì„œ í•™ìŠµí•œ í‘œí˜„ ê³µê°„ì—ë„ ì‘ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´ë‹¤ ì •í™•í•˜ê²ŒëŠ”, Î©ì—ì„œ ì‘ìš©í•˜ëŠ” ê·¸ë£¹ Gê°€ ìˆëŠ” ê²½ìš°, ê³µê°„ X(Î©)ì—ì„œ Gì˜ ì‘ìš©ì„ ìë™ìœ¼ë¡œ ì–»ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53844a7e",
   "metadata": {},
   "source": [
    "$$\n",
    "    (\\mathfrak{g} . x )(u) = x ( \\mathfrak{g^{-1}}u ) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9264feb",
   "metadata": {},
   "source": [
    "Due to the inverse on g, this is indeed a valid group action, in that we have $(\\mathfrak{g}.(\\mathfrak{h}.x))(u) = ((\\mathfrak{gh}).x)(u)$.\n",
    "\n",
    "gì˜ ì—­ìœ¼ë¡œ ì¸í•´,  ì´ê²ƒì€ ì‹¤ì œë¡œ ìœ íš¨í•œ ê·¸ë£¹ ì‘ì—…ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ìš°ë¦¬ëŠ” ë‹¤ìŒì„ ê°€ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "The most important kind of group actions, which we will encounter repeatedly throughout this text, are linear group actions, also known as group\n",
    "representations. The action on signals in equation (3) is indeed linear, in the sense that\n",
    "\n",
    "ì´ í…ìŠ¤íŠ¸ ì „ì²´ì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ì ‘í•˜ê²Œ ë  ê°€ì¥ ì¤‘ìš”í•œ ì¢…ë¥˜ì˜ ê·¸ë£¹ ì‘ì—…ì€ ê·¸ë£¹ í‘œí˜„ì´ë¼ê³ ë„ í•˜ëŠ” ì„ í˜• ê·¸ë£¹ ì‘ì—…ì…ë‹ˆë‹¤. ë°©ì •ì‹ (3)ì—ì„œ ì‹ í˜¸ì— ëŒ€í•œ ì‘ìš©ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¯¸ì—ì„œ ì‹¤ì œë¡œ ì„ í˜•ì…ë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "\\mathfrak{g}.(\\alpha x + \\beta \\acute{x}\n",
    ") = \\alpha(\\mathfrak{g}.x) + \\beta(\\mathfrak{g}.\\acute{x}\n",
    ")\n",
    "$$\n",
    "\n",
    "for any scalars $\\alpha, \\beta$ and signals $x, \\acute{x} âˆˆ X (\\Omega)$. We can describe linear actions either as maps $\\left(\\mathfrak{g}, x\\right) \\mapsto \\mathfrak{g}.x$ that are linear in $x$, or equivalently, by currying, as a map $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ that assigns to each group element $\\mathfrak{g}$ an (invertible) matrix $\\rho(\\mathfrak{g})$. The dimension n of the matrix is in general arbitrary and not necessarily related to the dimensionality of the group or the dimensionality of $\\Omega$, but in applications to deep learning n will usually be the dimensionality of the feature space on which the group acts.  For instance, we may have the group of 2D translations acting on a space of images with n pixels.\n",
    "\n",
    "ìœ„ ì‹ì€ ëª¨ë“  ìŠ¤ì¹¼ë¼ Î±, Î² ë° ì‹ í˜¸ $x, \\acute{x} âˆˆ X (\\Omega)$ ì— ëŒ€í•´ ì •ì˜ ë©ë‹ˆë‹¤. ì„ í˜• ì‘ìš©ì„ xì—ì„œ ì„ í˜•ì¸ ë§µ $\\left(\\mathfrak{g}, x\\right) \\mapsto \\mathfrak{g}.x$ ë¡œ ì„¤ëª…í•˜ê±°ë‚˜, ë™ë“±í•˜ê²Œ, ê° ê·¸ë£¹ ìš”ì†Œ gì— (ê°€ì—­) í–‰ë ¬ Ï(g)ë¥¼ í• ë‹¹í•˜ëŠ” ë§µ $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$ ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í–‰ë ¬ì˜ ì°¨ì› nì€ ì¼ë°˜ì ìœ¼ë¡œ ì„ì˜ì ì´ë©°, ê·¸ë£¹ì˜ ì°¨ì›ì´ë‚˜ Î©ì˜ ì°¨ì›ê³¼ ë°˜ë“œì‹œ ê´€ë ¨ì´ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, ë”¥ ëŸ¬ë‹ì— ì ìš©í•  ë•Œ nì€ ì¼ë°˜ì ìœ¼ë¡œ ê·¸ë£¹ì´ ì‘ìš©í•˜ëŠ” íŠ¹ì§• ê³µê°„ì˜ ì°¨ì›ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, n í”½ì…€ì˜ ì´ë¯¸ì§€ ê³µê°„ì— ì‘ìš©í•˜ëŠ” 2D ë²ˆì—­ ê·¸ë£¹ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32305a2",
   "metadata": {},
   "source": [
    "A n-dimensional real *representation* of a group $\\mathfrak{G}$ is a map $\\rho : \\mathfrak{G} \\rightarrow \\mathbb{R}^{n x n}$, assigning to each $\\mathfrak{g âˆˆ G}$ an *invertible* matrix $\\rho(\\mathfrak{g})$, and satisfying the\n",
    "condition $\\rho(\\mathfrak{gh}) = \\rho(\\mathfrak{g})\\rho(\\mathfrak{h})$ for all $\\mathfrak{g, h âˆˆ G}$. A representation is called unitary or orthogonal if the matrix $\\rho(\\mathfrak{g})$ is unitary or orthogonal for all $\\mathfrak{g âˆˆ G}$\n",
    "\n",
    "ê·¸ë£¹ Gì˜ nì°¨ì› ì‹¤ì œ í‘œí˜„ì€ ë§µ Ï : G â†’ R nÃ—n ì…ë‹ˆë‹¤. ê° g âˆˆ Gì— ê°€ì—­(invertible)í–‰ë ¬ Ï(g)ë¥¼ í• ë‹¹í•˜ê³ , ëª¨ë“  g,h âˆˆ G ì— ëŒ€í•´, ì¡°ê±´ Ï(gh) = Ï(g)Ï(h)ë¥¼ ì¶©ì¡±í•©ë‹ˆë‹¤. í–‰ë ¬ Ï(g)ê°€ ëª¨ë“  g âˆˆ Gì— ëŒ€í•´ ë‹¨ì¼ ë˜ëŠ” ì§êµì¸ ê²½ìš°, í‘œí˜„ì„ ë‹¨ì¼(unitary) ë˜ëŠ” ì§êµ(orthogonal)ë¼ê³  í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c53e16",
   "metadata": {},
   "source": [
    "Written in the language of group representations, the action of $\\mathfrak{G}$ on signals $x âˆˆ \\mathcal{X}(\\Omega)$ is defined as $\\rho(\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}u)$. We again verify that\n",
    "\n",
    "ê·¸ë£¹ í‘œí˜„ì˜ ì–¸ì–´ë¡œ ì‘ì„±ëœ ì‹ í˜¸ x âˆˆ X(Î©)ì— ëŒ€í•œ Gì˜ ì‘ìš©ì€ $\\rho(\\mathfrak{g})x(u) = x(\\mathfrak{g}^{-1}u)$ ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ë‹¤ìŒì„ ë‹¤ì‹œ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "(\\rho(\\mathfrak{g})(\\rho(\\mathfrak{h})x))(u) = (\\rho(\\mathfrak{gh})x(u)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78ac19",
   "metadata": {},
   "source": [
    "### Invariant and Equivariant functions\n",
    "ë¶ˆë³€ ë° ë“±ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1b4e9",
   "metadata": {},
   "source": [
    "The symmetry of the domain â„¦ underlying the signals X (â„¦) imposes structure on the function f defined on such signals. It turns out to be a powerful inductive bias, improving learning efficiency by reducing the space of possible interpolants, F(X (â„¦)), to those which satisfy the symmetry priors. Two important cases we will be exploring in this text are *invariant* and *equivariant* functions.\n",
    "\n",
    "ì‹ í˜¸ X(Î©)ì˜ ê¸°ì €ì— ìˆëŠ” ë„ë©”ì¸ Î©ì˜ ëŒ€ì¹­ì€ ì´ëŸ¬í•œ ì‹ í˜¸ì— ì •ì˜ëœ í•¨ìˆ˜ fì— êµ¬ì¡°ë¥¼ ë¶€ê³¼í•©ë‹ˆë‹¤. ì´ëŠ” ê°•ë ¥í•œ ê·€ë‚©ì  í¸í–¥ìœ¼ë¡œ íŒëª…ë˜ì–´ ê°€ëŠ¥í•œ ë³´ê°„ë²• F(X(Î©))ì˜ ê³µê°„ì„ ëŒ€ì¹­ ì‚¬ì „ì„ ë§Œì¡±í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¤„ì„ìœ¼ë¡œì¨ í•™ìŠµ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ì—ì„œ íƒêµ¬í•  ë‘ ê°€ì§€ ì¤‘ìš”í•œ ê²½ìš°ëŠ” ë¶ˆë³€(invariant) ë° ë“±ë³€(equivariant) í•¨ìˆ˜ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3c7f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (â„¦) \\rightarrow \\mathcal{Y}$ is $\\mathfrak{G}$-*invariant* if $f(\\rho(\\mathfrak{g})x) = f(x)$ for all $\\mathfrak{g âˆˆ G}$ and $x âˆˆ \\mathcal{X} (\\Omega)$, i.e., its output is unaffected by the group action on the input.\n",
    "\n",
    "í•¨ìˆ˜ $f : \\mathcal{X} (â„¦) \\rightarrow \\mathcal{Y}$ ëŠ” ëª¨ë“  g âˆˆ G ë° x âˆˆ X(Î©)ì— ëŒ€í•´ f(Ï(g)x) = f(x)ì˜ ê²½ìš° G-ë¶ˆë³€ì…ë‹ˆë‹¤. ì¦‰, ì¶œë ¥ì´ ì…ë ¥ì— ëŒ€í•œ ê·¸ë£¹ í–‰ë™ì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45cd6a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A function $f : \\mathcal{X} (â„¦) \\rightarrow \\mathcal{X} (â„¦)$ is $\\mathfrak{G}$-*equivariant* if $f(\\rho(\\mathfrak{g})x) = \\rho(\\mathfrak{g})f(x)$ for all $\\mathfrak{g âˆˆ G}$, i.e., group action on the input affects the output in the same way.\n",
    "\n",
    "í•¨ìˆ˜ f : X(Î©) â†’ X(Î©)ëŠ” ëª¨ë“  g âˆˆ Gì— ëŒ€í•´ f(Ï(g)x) = Ï(g)f(x)ì¸ ê²½ìš° G-ë“±ë³€ëŸ‰(G-equivariant)ì…ë‹ˆë‹¤. ì¦‰, ì…ë ¥ì— ëŒ€í•œ ê·¸ë£¹ ì‘ì—…ì€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¶œë ¥ì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123db12",
   "metadata": {},
   "source": [
    "\n",
    "More generally, we might have $f : \\mathcal{X} (â„¦) â†’ \\mathcal{X} (â„¦')$ with input and output spaces having different domains $â„¦, â„¦'$ and representations $Ï,Ï'$ of the same group $\\mathfrak{G}$. In this case, equivariance is defined as $f(Ï(g)x) = Ï'(g)f(x)$.\n",
    "\n",
    "ë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ, ì„œë¡œ ë‹¤ë¥¸ ì˜ì—­ Î©, Î©' ë° í‘œí˜„ Ï, Ï'ì„ ë™ì¼í•œ ê·¸ë£¹ Gì˜ ì…ë ¥ ë° ì¶œë ¥ ê³µê°„ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ f : X(Î©) â†’ X(Î©')ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° ë“±ë¶„ì‚°ì€ f(Ï(g)x) = Ï'(g)f(x)ë¡œ ì •ì˜ë©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0364c4a4",
   "metadata": {},
   "source": [
    "However, even the previous use case of image classification is usually implemented as a sequence of convolutional (shift-equivariant) layers, followed by global pooling (which is shift-invariant). As we will see in Section 3.5, this is a general blueprint of a majority of deep learning architectures, including CNNs and Graph Neural Networks (GNNs).\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì´ë¯¸ì§€ ë¶„ë¥˜ì˜ ì´ì „ ì‚¬ìš© ì‚¬ë¡€ì¡°ì°¨ë„ ì¼ë°˜ì ìœ¼ë¡œ ì¼ë ¨ì˜ ì»¨ë³¼ë£¨ì…˜(shift-equivariant) ë ˆì´ì–´ë¡œ êµ¬í˜„ëœ ë‹¤ìŒ ì „ì—­ í’€ë§(shift-invariant)ì´ ë’¤ë”°ë¦…ë‹ˆë‹¤. ì„¹ì…˜ 3.5ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì´ê²ƒì€ CNN ë° GNN(ê·¸ë˜í”„ ì‹ ê²½ë§)ì„ í¬í•¨í•œ ëŒ€ë‹¤ìˆ˜ ë”¥ ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì˜ ì¼ë°˜ì ì¸ ì²­ì‚¬ì§„ì…ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90366337",
   "metadata": {},
   "source": [
    "## 3.2 Isomorphisms and Automorphisms \n",
    "ë™í˜•ê³¼ ìê¸°ë™í˜•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d607a9",
   "metadata": {},
   "source": [
    "### Subgroups and Levels of structure\n",
    "Invertible and structure-preserving maps between different objects often go under the generic name of isomorphisms (Greek for â€˜equal shapeâ€™). An isomorphism from an object to itself is called an automorphism, or symmetry.\n",
    "\n",
    "\n",
    "ì„œë¡œ ë‹¤ë¥¸ ê°ì²´ ì‚¬ì´ì˜ ê°€ì—­ì ì´ê³ (Invertible) êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ” ì§€ë„ëŠ” ì¢…ì¢… ë™í˜•í•™(isomorphisms ê·¸ë¦¬ìŠ¤ì–´'ë™ì¼í•œ ëª¨ì–‘') ì´ë¼ê³  ë¶ˆë¦¼ë‹ˆë‹¤. ë¬¼ì²´ì—ì„œ ê·¸ ìì²´ë¡œì˜ ë™í˜•(isomorphisms)ì„ ìë™í˜•(automorphism) ë˜ëŠ” ëŒ€ì¹­ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "bijections(ì „ë‹¨ì‚¬): ì¼ëŒ€ì¼ ëŒ€ì‘(bijection, one-to-one correspondence)\n",
    "\n",
    "For a finite set, the cardinality is the number of elements (â€˜sizeâ€™) of the set, and for infinite sets the cardinality indicates different kinds of\n",
    "infinities, such as the countable infinity of the natural numbers, or the\n",
    "uncountable infinity of the continuum R.\n",
    "\n",
    "ìœ í•œ ì§‘í•©ì˜ ê²½ìš° ì¹´ë””ë„ë¦¬í‹°(cardinality)ëŠ” ì§‘í•©ì˜ ìš”ì†Œ ìˆ˜('í¬ê¸°')ì´ê³ , ë¬´í•œ ì§‘í•©ì˜ ê²½ìš° ì¹´ë””ë„ë¦¬í‹°(cardinality)ëŠ” ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ ë¬´í•œëŒ€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìì—°ìˆ˜ì˜ ì…€ ìˆ˜ ìˆëŠ” ë¬´í•œëŒ€ ë˜ëŠ” ì‹¤ìˆ˜ ì—°ì†ì²´ì˜ ì…€ ìˆ˜ ì—†ëŠ” ë¬´í•œëŒ€ì™€ ê°™ì´.\n",
    "\n",
    "Every differentiable function is continuous. If the map is continuously differentiableâ€˜sufficiently many timesâ€™, it is said to be smooth.\n",
    "\n",
    "ëª¨ë“  ë¯¸ë¶„ ê°€ëŠ¥í•œ í•¨ìˆ˜ëŠ” ì—°ì†ì ì…ë‹ˆë‹¤. ë§µì´ 'ì¶©ë¶„íˆ ì—¬ëŸ¬ ë²ˆ' ì—°ì† ë¯¸ë¶„ ê°€ëŠ¥í•˜ë©´ í‰í™œ('smooth')í•˜ë‹¤ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "homeomorphisms(ìœ ì‚¬í˜•): if â„¦ is a topological space, maps that preserve continuity and in addition to simple bijections between sets, are also continuous and have continuous inverse. Î©ì´ ìœ„ìƒ ê³µê°„ì¸ ê²½ìš° ì—°ì†ì„±ì„ ìœ ì§€í•˜ê³ , ì„¸íŠ¸ê°„ì˜ ë‹¨ìˆœ ì „ë‹¨ì‚¬(bijection) ì™¸ì—ë„, ì—°ì†ì ì´ë©°, ì—°ì†ì ì¸ ê°€ì—­ì„±ì„ ê°€ì§€ëŠ” ë§µ.\n",
    "\n",
    "the map $Ï„ (u) = u$ is the identity element, and for every $Ï„$ the inverse\n",
    "exists by definition, satisfying $(Ï„ â—¦ Ï„^{âˆ’1})(u) = (Ï„^{âˆ’1} â—¦ Ï„ )(u) = u$.\n",
    "\n",
    "ë§µ $Ï„ (u) = u$ëŠ” í•­ë“±(identity) ìš”ì†Œì…ë‹ˆë‹¤, ê·¸ë¦¬ê³  ëª¨ë“  $Ï„$ì— ëŒ€í•´ ì—­í•¨ìˆ˜ëŠ” ì •ì˜ì— ì˜í•´ ì¡´ì¬í•˜ë©° $(Ï„ â—¦ Ï„^{âˆ’1})(u) = (Ï„^{âˆ’1} â—¦ Ï„ )(u) = u$ë¥¼ ì¶©ì¡±í•©ë‹ˆë‹¤.\n",
    "\n",
    "diffeomorphisms(ë¯¸ë¶„ë³€í˜•) and denoted by Diff($â„¦$): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77aadb13",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "A metric or distance is a function $d : â„¦ Ã— â„¦ â†’ [0, âˆ)$ satisfying for all\n",
    "$u, v, w âˆˆ â„¦$:\n",
    "\n",
    "ë„ë©”ì¸ $â„¦$ì— ì†í•˜ëŠ” ëª¨ë“  $u, v, w$ì— ëŒ€í•˜ì—¬ ë¯¸í„°(metric) ë˜ëŠ” ê±°ë¦¬(distance)ëŠ” í•¨ìˆ˜ë¡œì¨ $â„¦ Ã— â„¦$ë¥¼ $0 \\leq$ ì´ê³  $< âˆ$ì€ ì˜ì—­ì— ë§¤í•‘í•©ë‹ˆë‹¤.  \n",
    "    \n",
    ">Identity of indiscernibles: $d(u, v) = 0 \\text{ iff } u = v$.\n",
    ">\n",
    ">ì‹ë³„í•  ìˆ˜ ì—†ëŠ” í•­ëª©ì˜ ë™ì¼ì„±: $u = v$ ì¸ ê²½ìš°ì—ë§Œ $d(u, v) = 0$ì„ ì„±ë¦½íŒë‹¤. \n",
    "    \n",
    ">Symmetry: $d(u, v) = d(v, u)$.\n",
    ">\n",
    ">ëŒ€ì¹­\n",
    "\n",
    ">Triangle inequality: $d(u, v) â‰¤ d(u, w) + d(w, v)$.\n",
    "> \n",
    ">ì‚¼ê° ë¶€ë“±ì‹\n",
    "\n",
    "A space equipped with a metric $(â„¦, d)$ is called a metric space.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e720395",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "Let $(\\mathfrak{G}, â—¦)$ be a group and $\\mathfrak{H âŠ† G}$ a subset. $\\mathfrak{H}$ is said to be a *subgroup* of $\\mathfrak{G}$ if $(\\mathfrak{H}, â—¦)$ constitutes a group with the same operation.\n",
    "\n",
    "(G, â—¦)ë¥¼ ê·¸ë£¹ì´ë¼ê³  í•˜ê³  H âŠ† Gë¥¼ ë¶€ë¶„ì§‘í•©ì´ë¼ê³  í•˜ì. (H, â—¦)ê°€ ë™ì¼í•œ ì—°ì‚°ì„ ê°–ëŠ” ê·¸ë£¹ì„ êµ¬ì„±í•˜ëŠ” ê²½ìš°, HëŠ” Gì˜ í•˜ìœ„ ê·¸ë£¹ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0103ed5",
   "metadata": {},
   "source": [
    "### Isomorphisms and Automorphisms\n",
    "ë™í˜•ì‚¬ìƒ(Isomorphisms)ê³¼ ìë™ì‚¬ìƒ(Automorphisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732bbcab",
   "metadata": {},
   "source": [
    "We have described symmetries as structure preserving and invertible maps from *an object to itself*. Such maps are also known as *automorphisms*, and describe a way in which an object is equivalent it itself. However, an equally important class of maps are the so-called *isomorphisms*, which exhibit an equivalence between two nonidentical objects. These concepts are often conflated, but distinguishing them is necessary to create clarity for our following discussion.\n",
    "\n",
    "ëŒ€ì¹­ì€ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ê³  ê°ì²´ì—ì„œ ê°ì²´ ìì²´ë¡œì˜ ë°˜ì „ ê°€ëŠ¥í•œ ë§µìœ¼ë¡œ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë§µì€ ìë™í˜•ì„±ì´ë¼ê³ ë„ í•˜ë©°, ê°ì²´ê°€ ê·¸ ìì²´ë¡œ ë™ë“±í•œ ë°©ì‹ì„ ì„¤ëª…í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê·¸ì— ëª»ì§€ì•Šê²Œ ì¤‘ìš”í•œ ë§µ ìœ í˜•ì€ ë™í˜• ë§µìœ¼ë¡œ, ë™ì¼í•˜ì§€ ì•Šì€ ë‘ ê°ì²´ ê°„ì— ë™ë“±ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë§µì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œë…ì€ ì¢…ì¢… í˜¼ë™ë˜ëŠ” ê²½ìš°ê°€ ë§ì§€ë§Œ, ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” ê²ƒì€ êµ¬ë¶„í•˜ëŠ” ê²ƒì€ ë‹¤ìŒ ë…¼ì˜ë¥¼ ëª…í™•í•˜ê²Œ í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "\n",
    "To understand the difference, consider a set $â„¦ = \\left\\{0, 1, 2\\right\\}$. An automorphism of the set $â„¦$ is a bijection $Ï„ : â„¦ â†’ â„¦$ such as a cyclic shift $Ï„ (u) = u+ 1 mod 3$. Such a map preserves the cardinality property, and maps $â„¦$ onto itself. If we have another set $â„¦' = \\left\\{a, b, c\\right\\}$ with the same number of elements, then a bijection $Î· : â„¦ â†’ â„¦'$ such as $Î·(0) = a, Î·(1) = b, Î·(2) = c$ is a *set isomorphism*.\n",
    "\n",
    "ì°¨ì´ì ì„ ì´í•´í•˜ê¸° ìœ„í•´ Î© = {0, 1, 2} ì§‘í•©ì„ ìƒê°í•´ ë³´ê² ìŠµë‹ˆë‹¤. ì§‘í•© Î©ì˜ ìë™ë³€í™˜ì€ ìˆœí™˜ ì´ë™ Ï„(u) = u+ 1 mod 3ê³¼ ê°™ì€ Ï„ : Î© â†’ Î©ì˜ ë°”ì´ì œì´ì…˜ì…ë‹ˆë‹¤.\n",
    "ì´ëŸ¬í•œ ë§µì€ ì¹´ë””ë„ë¦¬í‹° ì†ì„±ì„ ë³´ì¡´í•˜ê³  Î©ì„ ê·¸ ìì²´ì— ë§¤í•‘í•©ë‹ˆë‹¤. ë§Œì•½\n",
    "ì›ì†Œ ìˆ˜ê°€ ê°™ì€ ë‹¤ë¥¸ ì§‘í•© Î©' = {a, b, c}ê°€ ìˆë‹¤ë©´, Î· : Î© â†’ Î©' Î·(0) = a, Î·(1) = b, Î·(2) = cì™€ ê°™ì€ ë°”ì´ì§“ì€ ì§‘í•© ë™í˜•ì…ë‹ˆë‹¤.\n",
    "\n",
    "As we will see in Section 4.1 for graphs, the notion of structure includes\n",
    "not just the number of nodes, but also the connectivity. An isomorphism\n",
    "$Î· : V â†’ V'$ between two graphs $G = (V, E)$ and $G' = (V', E')$ is thus a\n",
    "bijection between the nodes that maps pairs of connected nodes to pairs\n",
    "of connected nodes, and likewise for pairs of non-connected nodes. Two\n",
    "isomorphic graphs are thus structurally identical, and differ only in the\n",
    "way their nodes are ordered. On the other hand, a graph automorphism or\n",
    "symmetry is a map $Ï„ : V â†’ V$ maps the nodes of the graph back to itself,\n",
    "while preserving the connectivity. A graph with a non-trivial automorphism\n",
    "(i.e., $Ï„ \\neq id$) presents symmetries.\n",
    "\n",
    "ê·¸ë˜í”„ì— ëŒ€í•œ 4.1ì ˆì—ì„œ ì‚´í´ë³´ê² ì§€ë§Œ, êµ¬ì¡°ì˜ ê°œë…ì—ëŠ” ë…¸ë“œì˜ ìˆ˜ë¿ë§Œ ì•„ë‹ˆë¼ ì—°ê²°ì„± ë˜í•œ í¬í•¨ë©ë‹ˆë‹¤. ë‘ ê·¸ë˜í”„ ğº=(ğ‘‰,ğ¸) ì™€ ğºâ€²=(ğ‘‰â€²,ğ¸â€²) ì‚¬ì´ì˜ ë™í˜• Î·:ğ‘‰â†’ğ‘‰â€² ëŠ” ì—°ê²°ëœ ë…¸ë“œ ìŒì„ ì—°ê²°ëœ ë…¸ë“œ ìŒì— ë§¤í•‘í•˜ëŠ” ë…¸ë“œ ê°„ì˜ ë°”ì´ì œì…˜ì´ë©°, ì—°ê²°ë˜ì§€ ì•Šì€ ë…¸ë“œ ìŒì— ëŒ€í•´ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‘ ê°œì˜ ë™í˜• ê·¸ë˜í”„ëŠ” êµ¬ì¡°ì ìœ¼ë¡œ ë™ì¼í•˜ë©° ë…¸ë“œì˜ ìˆœì„œë§Œ ë‹¤ë¥¼ ë¿ì…ë‹ˆë‹¤. ë°˜ë©´ì— ê·¸ë˜í”„ ìë™ë³€í˜• ë˜ëŠ” ëŒ€ì¹­ì€ ë§µ Ï„:ğ‘‰â†’ğ‘‰ ëŠ” ì—°ê²°ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ ë‹¤ì‹œ ìê¸° ìì‹ ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤. ì‚¬ì†Œí•œ ì˜¤í† ëª¨í”¼ì¦˜ì´ ì•„ë‹Œ ê·¸ë˜í”„(ì¦‰, Ï„â‰ ğ‘–ğ‘‘ )ë¥¼ ê°€ì§„ ê·¸ë˜í”„ëŠ” ëŒ€ì¹­ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ae07ae",
   "metadata": {},
   "source": [
    "## 3.3 Deformation Stability\n",
    "\n",
    "ë³€í˜• ì•ˆì •ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc491cbe",
   "metadata": {},
   "source": [
    "we are more interested in a far larger set of transformations where global, exact invariance is replaced by a local, inexact one. In our discussion, we will distinguish between two scenarios: the setting where the domain $â„¦$ is fixed, and signals $x âˆˆ \\mathcal{X} (â„¦)$ are undergoing deformations, and the setting where the domain $â„¦$ itself may be deformed.\n",
    "\n",
    "ìš°ë¦¬ëŠ” ì „ì—­ì˜ ì •í™•í•œ ë¶ˆë³€ì„±ì´ êµ­ì§€ì ì¸ ë¶€ì •í™•í•œ ë¶ˆë³€ì„±ìœ¼ë¡œ ëŒ€ì²´ë˜ëŠ” í›¨ì”¬ ë” í° ë³€í™˜ ì§‘í•©ì— ë” ê´€ì‹¬ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë„ë©”ì¸ Î©ì´ ê³ ì •ë˜ì–´ ìˆê³  ì‹ í˜¸ x âˆˆ X (Î©)ê°€ ë³€í˜•ë˜ëŠ” ì„¤ì •ê³¼ ë„ë©”ì¸ Î© ìì²´ê°€ ë³€í˜•ë  ìˆ˜ ìˆëŠ” ì„¤ì •ì˜ ë‘ ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ êµ¬ë¶„í•´ ë³´ê² ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b81e92",
   "metadata": {},
   "source": [
    "### Stability to signal deformations\n",
    "ì‹ í˜¸ ë³€í˜•ì— ëŒ€í•œ ì•ˆì •ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2a9b60",
   "metadata": {},
   "source": [
    "In many applications, we know a priori that a small deformation of the signal x should not change the output of $f(x)$, so it is tempting to consider such deformations as symmetries.\n",
    "\n",
    "ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‹ í˜¸ xì˜ ì‘ì€ ë³€í˜•ì´ ğ‘“(ğ‘¥)ì˜ ì¶œë ¥ì„ ë³€ê²½í•´ì„œëŠ” ì•ˆ ëœë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆìœ¼ë¯€ë¡œ, ì´ëŸ¬í•œ ë³€í˜•ì„ ëŒ€ì¹­ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì‹¶ì–´ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "A better approach is to quantify how â€œfarâ€ a given $Ï„ âˆˆ Diff(â„¦)$ is from a given symmetry subgroup $G âŠ‚ Diff(â„¦)$ (e.g. translations) with a complexity measure $c(Ï„ )$, so that $c(Ï„ ) = 0$ whenever $Ï„ âˆˆ G$. We can now replace our previous definition of exact invariance and equivarance under group actions with a â€˜softerâ€™ notion of *deformation stability* (or approximate invariance):\n",
    "\n",
    "ë” ë‚˜ì€ ì ‘ê·¼ ë°©ì‹ì€ ì£¼ì–´ì§„ Ï„âˆˆğ·ğ‘–ğ‘“ğ‘“(Î©) ê°€ ì£¼ì–´ì§„ ëŒ€ì¹­ í•˜ìœ„ ê·¸ë£¹ ğºâŠ‚ğ·ğ‘–ğ‘“ğ‘“(Î©)(ì˜ˆ: ë³€í™˜)ì—ì„œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ë¥¼ ë³µì¡ë„ ì¸¡ì •ê°’ ğ‘(Ï„) ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ëŸ‰í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ Ï„âˆˆğºì¸ ê²½ìš° ğ‘(Ï„)=0 ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ì œ ê·¸ë£¹ ì‘ìš© í•˜ì—ì„œ ì •í™•í•œ ë¶ˆë³€ì„±ê³¼ ë“±ê°€ì„±ì— ëŒ€í•œ ì´ì „ ì •ì˜ë¥¼ ë³€í˜• ì•ˆì •ì„±(deformation stability)(ë˜ëŠ” ê·¼ì‚¬ ë¶ˆë³€ì„± approximate invariance )ì´ë¼ëŠ” 'ë” ë¶€ë“œëŸ¬ìš´' ê°œë…ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "\n",
    "$$\n",
    "    \\|f(Ï(Ï„ )x) âˆ’ f(x)\\| â‰¤ Cc(Ï„ )\\|x\\|, , âˆ€x âˆˆ \\mathcal{X} (â„¦) \\tag{4}\n",
    "$$\n",
    "\n",
    "where $Ï(Ï„ )x(u) = x(Ï„âˆ’1u)$ as before, and where C is some constant independent of the signal $x$. A function $f âˆˆ F(X (â„¦))$ satisfying the above equation is said to be geometrically stable. We will see examples of such functions in the next Section 3.4.\n",
    "\n",
    "ì—¬ê¸°ì„œ Ï(Ï„)ğ‘¥(ğ‘¢)=ğ‘¥(Ï„-1ğ‘¢) ì´ë©°, ì—¬ê¸°ì„œ CëŠ” ì‹ í˜¸ ğ‘¢ì™€ ë…ë¦½ì ì¸ ìƒìˆ˜ì…ë‹ˆë‹¤. ğ‘“âˆˆğ¹(ğ‘‹(Î©)) ì¸ í•¨ìˆ˜ê°€ ìœ„ì˜ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš° ê¸°í•˜í•™ì ìœ¼ë¡œ ì•ˆì •ì ì´ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒ 3.4ì ˆì—ì„œ ì´ëŸ¬í•œ í•¨ìˆ˜ì˜ ì˜ˆë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcb72394",
   "metadata": {},
   "source": [
    "Since c(Ï„ ) = 0 for Ï„ âˆˆ G, this definition generalises the G-invariance property defined above. Its utility in applications depends on introducing an appropriate deformation cost. In the case of images defined over a continuous Euclidean plane, a popular choice is $c^{2}(Ï„) := \\int_{\\Omega}\\|âˆ‡Ï„ (u)\\|^{2} du$, which measures the â€˜elasticityâ€™ of Ï„ , i.e., how different it is from the displacement by a constant vector field. This deformation cost is in fact a norm often called the Dirichlet energy, and can be used to quantify how far Ï„ is from the translation group.\n",
    "\n",
    "Ï„ âˆˆ Gì—ì„œ c(Ï„ ) = 0ì´ë¯€ë¡œ, ì´ ì •ì˜ëŠ” ìœ„ì—ì„œ ì •ì˜í•œ G-ë¶„ì‚° íŠ¹ì„±ì„ ì¼ë°˜í™”í•©ë‹ˆë‹¤. ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œì˜ ìœ ìš©ì„±ì€ ì ì ˆí•œ ë³€í˜• ë¹„ìš©ì„ ë„ì…í•˜ëŠ” ë° ë‹¬ë ¤ ìˆìŠµë‹ˆë‹¤. ì—°ì† ìœ í´ë¦¬ë“œ í‰ë©´ ìœ„ì— ì •ì˜ëœ ì´ë¯¸ì§€ì˜ ê²½ìš°, Ï„ì˜ 'íƒ„ì„±', ì¦‰ ì¼ì •í•œ ë²¡í„° í•„ë“œì— ì˜í•œ ë³€ìœ„ì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” $c^{2}(Ï„) := \\int_{\\Omega}\\|âˆ‡Ï„ (u)\\|^{2} du$ê°€ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë³€í˜• ë¹„ìš©ì€ ì‚¬ì‹¤ ë””ë¦¬í´ë ˆ(Dirichlet) ì—ë„ˆì§€ë¼ê³ ë„ í•˜ëŠ” ê·œë²”ìœ¼ë¡œ, Ï„ê°€ ë³€í™˜ ê·¸ë£¹ì—ì„œ ì–¼ë§ˆë‚˜ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ì •ëŸ‰í™”í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$Aut(â„¦)$: The set automorphism group\n",
    "\n",
    "$:=$: equal by definition\n",
    "\n",
    "$âˆ‡$: Del or nabla. vector differential operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564e04c",
   "metadata": {},
   "source": [
    "### Stability to domain deformations\n",
    "ë„ë©”ì¸ ë³€í˜•ì— ëŒ€í•œ ì•ˆì •ì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c8c4e",
   "metadata": {},
   "source": [
    "In many applications, the object being deformed is not the signal, but the geometric domain $â„¦$ itself. Canonical instances of this are applications dealing with graphs and manifolds: a graph can model a social network at different instance of time containing slightly different social relations (follow graph), or a manifold can model a 3D object undergoing non-rigid deformations. This deformation can be quantified as follows. If $\\mathcal{D}$ denotes the space of all possible variable domains (such as the space of all graphs, or the space of Riemannian manifolds), one can define for $â„¦, \\tilde{â„¦} âˆˆ \\mathcal{D}$ an appropriate metric (â€˜distanceâ€™) $d(â„¦, \\tilde{â„¦})$ satisfying $d(â„¦, \\tilde{â„¦}) = 0$ if $â„¦$ and $\\tilde{â„¦}$ are equivalent in some sense: for example, the graph edit distance vanishes when the graphs are isomorphic, and the GromovHausdorff distance between Riemannian manifolds equipped with geodesic distances vanishes when two manifolds are isometric.\n",
    "\n",
    "ë§ì€ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë³€í˜•ë˜ëŠ” ëŒ€ìƒì€ ì‹ í˜¸ê°€ ì•„ë‹ˆë¼ ê¸°í•˜í•™ì  ì˜ì—­ Î© ìì²´ì…ë‹ˆë‹¤. ê·¸ë˜í”„ì™€ ë‹¤ì–‘ì²´ë¥¼ ë‹¤ë£¨ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ëŒ€í‘œì ì¸ ì˜ˆë¡œ, ê·¸ë˜í”„ëŠ” ì•½ê°„ ë‹¤ë¥¸ ì‚¬íšŒì  ê´€ê³„ë¥¼ í¬í•¨í•˜ëŠ” ì—¬ëŸ¬ ì‹œì ì˜ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ë¥¼ ëª¨ë¸ë§í•˜ê±°ë‚˜(ê·¸ë˜í”„ë¥¼ ë”°ë¼), ë‹¤ì–‘ì²´ëŠ” ë¹„ê°•ì²´(non-rigid) ë³€í˜•ì„ ê²ªëŠ” 3D ê°ì²´ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³€í˜•ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ëŸ‰í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $\\mathcal{D}$ê°€ ê°€ëŠ¥í•œ ëª¨ë“  ê°€ë³€ ì˜ì—­ì˜ ê³µê°„(ì˜ˆ: ëª¨ë“  ê·¸ë˜í”„ì˜ ê³µê°„ ë˜ëŠ” ë¦¬ë§Œ ë‹¤ì–‘ì²´ì˜ ê³µê°„)ì„ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°,  $â„¦, \\tilde{â„¦} âˆˆ \\mathcal{D}$ì— ëŒ€í•´ $Î©$ê³¼ $\\tilde{â„¦}$ê°€ ì–´ë–¤ ì˜ë¯¸ì—ì„œ ë™ë“±í•œ ê²½ìš° $d(â„¦, \\tilde{â„¦}) = 0$ì„ ë§Œì¡±í•˜ëŠ” ì ì ˆí•œ ë©”íŠ¸ë¦­('ê±°ë¦¬') $d(â„¦, \\tilde{â„¦})$ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ì˜ˆë¥¼ ë“¤ì–´ ê·¸ë˜í”„ í¸ì§‘ ê±°ë¦¬ëŠ” ê·¸ë˜í”„ê°€ ë™í˜•ì¼ ë•Œ ì‚¬ë¼ì§€ê³ , ì¸¡ì§€ ê±°ë¦¬ê°€ ìˆëŠ” ë¦¬ë§Œ ë‹¤ì–‘ì²´ ì‚¬ì´ì˜ ê·¸ë¡œëª¨í”„í•˜ìš°ìŠ¤ë„ë¥´í”„ ê±°ë¦¬ëŠ” ë‘ ë‹¤ì–‘ì²´ê°€ ë“±ê°ì¼ ë•Œ ì‚¬ë¼ì§‘ë‹ˆë‹¤.\n",
    "\n",
    "A common construction of such distances between domains relies on some family of invertible mapping $Î· : â„¦ â†’ \\tilde{â„¦}$ that try to â€˜alignâ€™ the domains in a way that the corresponding structures are best preserved. For example, in the case of graphs or Riemannian manifolds (regarded as metric spaces with the geodesic distance), this alignment can compare pair-wise adjacency or distance structures (d and $\\tilde{d}$, respectively),\n",
    "\n",
    "ì´ëŸ¬í•œ ë„ë©”ì¸ ê°„ ê±°ë¦¬ì˜ ì¼ë°˜ì ì¸ êµ¬ì„±ì€ í•´ë‹¹ êµ¬ì¡°ê°€ ê°€ì¥ ì˜ ë³´ì¡´ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë„ë©”ì¸ì„ 'ì •ë ¬'í•˜ë ¤ëŠ” ì—­ì „ ë§¤í•‘ $Î· : â„¦ â†’ \\tilde{â„¦}$ì˜ ì¼ë¶€ ì œí’ˆêµ°ì— ì˜ì¡´í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê·¸ë˜í”„ë‚˜ ë¦¬ë§Œ ë‹¤ì–‘ì²´(ì¸¡ì§€ ê±°ë¦¬ê°€ ìˆëŠ” ë¯¸í„°ë²• ê³µê°„ìœ¼ë¡œ ê°„ì£¼ë¨)ì˜ ê²½ìš°, ì´ëŸ¬í•œ ì •ë ¬ì€ ìŒë³„ ì¸ì ‘ì„± ë˜ëŠ” ê±°ë¦¬ êµ¬ì¡°(ê°ê° dì™€ $\\tilde{d}$)ë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤,\n",
    "\n",
    "$$\n",
    "d_\\mathcal{D}(â„¦,\\tilde{â„¦}) = \\inf_{Î·âˆˆ\\mathfrak{G}}\\|d âˆ’ \\tilde{d}â—¦ (Î· Ã— Î·) \\|\n",
    "$$\n",
    "\n",
    "where $\\mathfrak{G}$ is the group of isomorphisms such as bijections or isometries, and the norm is defined over the product space $â„¦ Ã— â„¦$. In other words, a distance between elements of $â„¦, \\tilde{â„¦}$ is â€˜liftedâ€™ to a distance between the domains themselves, by accounting for all the possible alignments that preserve the internal structure. Given a signal $x âˆˆ X (â„¦)$ and a deformed domain $\\tilde{â„¦}$, one can then consider the deformed signal $\\tilde{x} = x â—¦ Î·^{âˆ’1} âˆˆ X (\\tilde{â„¦})$.\n",
    "\n",
    "ì—¬ê¸°ì„œ $\\mathfrak{G}$ëŠ” ì‚¬ë³€í˜• ë˜ëŠ” ì•„ì´ì†Œë©”íŠ¸ë¦¬ì™€ ê°™ì€ ë™í˜•ë“¤ì˜ ê·¸ë£¹ì´ë©°, ê·œë²”ì€ ê³± ê³µê°„ $â„¦ Ã— â„¦$ì— ëŒ€í•´ ì •ì˜ë©ë‹ˆë‹¤. ì¦‰, $â„¦, \\tilde{â„¦}$ì˜ ìš”ì†Œ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë‚´ë¶€ êµ¬ì¡°ë¥¼ ë³´ì¡´í•˜ëŠ” ëª¨ë“  ê°€ëŠ¥í•œ ì •ë ¬ì„ ê³ ë ¤í•˜ì—¬ ë„ë©”ì¸ ìì²´ ì‚¬ì´ì˜ ê±°ë¦¬ë¡œ 'ë¦¬í”„íŒ…'ë©ë‹ˆë‹¤. ì‹ í˜¸ $x âˆˆ X (Î©)$ì™€ ë³€í˜•ëœ ë„ë©”ì¸ $\\tilde{â„¦}$ê°€ ì£¼ì–´ì§€ë©´, ë³€í˜•ëœ ì‹ í˜¸ $\\tilde{x} = x â—¦ Î·^{âˆ’1} âˆˆ X (\\tilde{â„¦})$ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "By slightly abusing the notation, we define $X (D) = {(X (â„¦), â„¦) : â„¦ âˆˆ \\mathcal{D}}$ as\n",
    "the ensemble of possible input signals defined over a varying domain. A function $f : \\mathcal{X} (\\mathcal{D}) â†’ \\mathcal{Y}$ is stable to domain deformations if\n",
    "\n",
    "í‘œê¸°ë²•ì„ ì•½ê°„ ë³€í˜•í•˜ì—¬ $X (D) = {(X (â„¦), â„¦) : â„¦ âˆˆ \\mathcal{D}}$ ë¥¼ ë‹¤ì–‘í•œ ì˜ì—­ì— ê±¸ì³ ì •ì˜ëœ ê°€ëŠ¥í•œ ì…ë ¥ ì‹ í˜¸ì˜ ì•™ìƒë¸”ë¡œ ì •ì˜í•©ë‹ˆë‹¤. í•¨ìˆ˜ $f : \\mathcal{X} (\\mathcal{D}) â†’ \\mathcal{Y}$ ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš° ë„ë©”ì¸ ë³€í˜•ì— ì•ˆì •ì ì…ë‹ˆë‹¤. ëª¨ë“  $â„¦, \\tilde{â„¦} âˆˆ D$ ì™€  $x âˆˆ X (â„¦)$ì— ëŒ€í•˜ì—¬\n",
    "\n",
    "$$\n",
    "    \\|f(x, â„¦) âˆ’ f(\\tilde{x}, \\tilde{â„¦}) \\|  \\leq C \\|x\\|d_{\\mathcal{D}}(â„¦,\\tilde{â„¦}) \\tag{5}\n",
    "$$\n",
    "\n",
    "for all $â„¦, \\tilde{â„¦} âˆˆ D$, and $x âˆˆ X (â„¦)$. We will discuss this notion of stability in the context of manifolds in Sections 4.4â€“4.6, where isometric deformations play a crucial role. Furthermore, it can be shown that the stability to domain deformations is a natural generalisation of the stability to signal eformations, by viewing the latter in terms of deformations of the volume form Gama et al. (2019).\n",
    "\n",
    "ì´ ì•ˆì •ì„±ì˜ ê°œë…ì€ ë“±ê° ë³€í˜•ì´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” 4.4-4.6ì ˆì˜ ë‹¤ì–‘ì²´ì˜ ë§¥ë½ì—ì„œ ë…¼ì˜í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ, ë„ë©”ì¸ ë³€í˜•ì— ëŒ€í•œ ì•ˆì •ì„±ì€ ì‹ í˜¸ ë³€í˜•ì— ëŒ€í•œ ì•ˆì •ì„±ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì¼ë°˜í™”ì´ë©°, í›„ìë¥¼ ì²´ì  í˜•íƒœì˜ ë³€í˜• ì¸¡ë©´ì—ì„œ ë³´ë©´ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Gama et al. (2019)).\n",
    "\n",
    "$\\inf$: Bound, ìœ ê³„, ìœ í•œí•œ ì˜ì—­ì„ ê°€ì§€ëŠ” ì˜¤ë¸Œì íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe679b",
   "metadata": {},
   "source": [
    "## 3.4 Scale Separation\n",
    "ìŠ¤ì¼€ì¼ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9099b6c5",
   "metadata": {},
   "source": [
    "While deformation stability substantially strengthens the global symmetry priors, it is not sufficient in itself to overcome the curse of dimensionality, in the sense that, informally speaking, there are still â€œtoo many\" functions that respect (4) as the size of the domain grows. A key insight to overcome this curse is to exploit the multiscale structure of physical tasks. Before describing multiscale representations, we need to introduce the main elements of Fourier transforms, which rely on frequency rather than scale.\n",
    "\n",
    "ë³€í˜• ì•ˆì •ì„±ì€ ì „ì—­ ëŒ€ì¹­ ì„ í–‰ ì¡°ê±´ì„ ìƒë‹¹íˆ ê°•í™”í•˜ì§€ë§Œ, ë¹„ê³µì‹ì ìœ¼ë¡œ ë§í•˜ìë©´ ë„ë©”ì¸ì˜ í¬ê¸°ê°€ ì»¤ì§ì— ë”°ë¼ (4)ë¥¼ ì¡´ì¤‘í•˜ëŠ” í•¨ìˆ˜ê°€ ì—¬ì „íˆ \"ë„ˆë¬´ ë§ì´\" ì¡´ì¬í•œë‹¤ëŠ” ì ì—ì„œ ê·¸ ìì²´ë§Œìœ¼ë¡œëŠ” ì°¨ì›ì˜ ì €ì£¼ë¥¼ ê·¹ë³µí•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ ì°¨ì›ì„±ì˜ ì €ì£¼ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ëŠ” ë¬¼ë¦¬ì  ì‘ì—…ì˜ ë©€í‹°ìŠ¤ì¼€ì¼ êµ¬ì¡°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë©€í‹°ìŠ¤ì¼€ì¼ í‘œí˜„ì„ ì„¤ëª…í•˜ê¸° ì „ì— ë‹¤ìŒê³¼ ê·œëª¨ë³´ë‹¤ëŠ” ì£¼íŒŒìˆ˜ì— ì˜ì¡´í•˜ëŠ” í‘¸ë¦¬ì— ë³€í™˜ ê°™ì€ ì£¼ìš” ìš”ì†Œë¥¼ ì†Œê°œí•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5ad09",
   "metadata": {},
   "source": [
    "### Fourier Transform and Global invariants\n",
    "[But what is the Fourier Transform? A visual introduction.](https://youtu.be/spUNpyF58BY)\n",
    "\n",
    "An essential aspect of Fourier transforms is that they reveal global properties of the signal and the domain, such as smoothness or conductance. Such global behavior is convenient in presence of global symmetries of the domain such as translation, but not to study more general diffeomorphisms. This requires a representation that trades off spatial and frequential localisation, as we see next.\n",
    "\n",
    "í‘¸ë¦¬ì— ë³€í™˜ì˜ í•µì‹¬ì ì¸ ì¸¡ë©´ì€ í‰í™œë„(smoothness)ë‚˜ ì „ë„ë„(conductance)ì™€ ê°™ì€ ì‹ í˜¸ì™€ ë„ë©”ì¸ì˜ ì „ì—­ ì†ì„±ì„ ë“œëŸ¬ë‚¸ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì „ì—­ì  ë™ì‘ì€ ë³€í™˜ê³¼ ê°™ì´ ë„ë©”ì¸ì˜ ì „ì—­ì  ëŒ€ì¹­ì„±ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” í¸ë¦¬í•˜ì§€ë§Œ, ë³´ë‹¤ ì¼ë°˜ì ì¸ ì´í˜•ì„±ì„ ì—°êµ¬í•˜ëŠ” ë°ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ë‹¤ìŒì— ì‚´í´ë³¼ ê²ƒì²˜ëŸ¼ ê³µê°„ ë° ì£¼íŒŒìˆ˜ ë¡œì»¬ë¼ì´ì œì´ì…˜ì„ ì ˆì¶©í•˜ëŠ” í‘œí˜„ì´ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11f00de",
   "metadata": {},
   "source": [
    "### Multiscale representations\n",
    "ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ í‘œí˜„\n",
    "\n",
    "The notion of local invariance can be articulated by switching from a Fourier frequency-based representation to a scalebased representation, the cornerstone of multi-scale decomposition methods such as wavelets. The essential insight of multi-scale methods is to decompose functions defined over the domain â„¦ into elementary functions that are localised both in space and frequency. In the case of wavelets, this is achieved by correlating a translated and dilated filter (mother wavelet) Ïˆ, producing a combined spatio-frequency representation called a continuous wavelet transform.\n",
    "\n",
    "êµ­ì†Œ ë¶ˆë³€ì„±ì˜ ê°œë…ì€ í‘¸ë¦¬ì— ì£¼íŒŒìˆ˜ ê¸°ë°˜ í‘œí˜„ì—ì„œ ì›¨ì´ë¸”ë¦¿ê³¼ ê°™ì€ ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë¶„í•´ ë°©ë²•ì˜ ì´ˆì„ì¸ ìŠ¤ì¼€ì¼ ê¸°ë°˜ í‘œí˜„ìœ¼ë¡œ ì „í™˜í•¨ìœ¼ë¡œì¨ ëª…í™•í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ë°©ë²•ì˜ í•µì‹¬ì€ ë„ë©”ì¸ Î©ì— ì •ì˜ëœ í•¨ìˆ˜ë¥¼ ê³µê°„ê³¼ ì£¼íŒŒìˆ˜ ëª¨ë‘ì— êµ­í•œëœ ê¸°ë³¸ í•¨ìˆ˜ë¡œ ë¶„í•´í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì›¨ì´ë¸”ë¦¿ì˜ ê²½ìš°, ì´ëŠ” ë³€í™˜ ë° í™•ì¥ í•„í„°(ë§ˆë” ì›¨ì´ë¸”ë¦¿) Ïˆë¥¼ ìƒí˜¸ ì—°ê´€ì‹œì¼œ ì—°ì† ì›¨ì´ë¸”ë¦¿ ë³€í™˜ì´ë¼ê³  í•˜ëŠ” ê²°í•©ëœ ê³µê°„-ì£¼íŒŒìˆ˜ í‘œí˜„ì„ ìƒì„±í•¨ìœ¼ë¡œì¨ ë‹¬ì„±ë©ë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "(W_Ïˆx)(u, Î¾) = Î¾^{âˆ’1/2} \\int_{âˆ’âˆ}^{+âˆ}Ïˆ \\left(\\frac{v âˆ’ u}{Î¾}\\right) x(v)dv\n",
    "$$\n",
    "\n",
    "The translated and dilated filters are called wavelet atoms; their spatial position and dilation correspond to the coordinates u and $Î¾$ of the wavelet transform. These coordinates are usually sampled dyadically $(Î¾ = 2^{âˆ’j}$ and $u = 2^{âˆ’jk})$, with $j$ referred to as scale. Multi-scale signal representations bring important benefits in terms of capturing regularity properties beyond global smoothness, such as piece-wise smoothness, which made them a popular tool in signal and image processing and numerical analysis in the 90s.\n",
    "\n",
    "ë³€í™˜ ë° í™•ì¥ëœ í•„í„°ë¥¼ ì›¨ì´ë¸Œë › ì›ìë¼ê³  í•˜ë©°, ì´ ì›ìì˜ ê³µê°„ì  ìœ„ì¹˜ì™€ í™•ì¥ì€ ì›¨ì´ë¸Œë › ë³€í™˜ì˜ ì¢Œí‘œ uì™€ Î¾ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¢Œí‘œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ì›ì ìœ¼ë¡œ ìƒ˜í”Œë§ë˜ë©° $(Î¾ = 2^{âˆ’j}$ ë° $u = 2^{âˆ’jk})$, jë¥¼ ìŠ¤ì¼€ì¼ì´ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ì‹ í˜¸ í‘œí˜„ì€ ì¡°ê°ë³„ í‰í™œë„ì™€ ê°™ì€ ì „ì—­ í‰í™œë„ ì´ìƒì˜ ê·œì¹™ì„± ì†ì„±ì„ ìº¡ì²˜í•˜ëŠ” ë° ì¤‘ìš”í•œ ì´ì ì„ ê°€ì ¸ë‹¤ì£¼ë¯€ë¡œ 90ë…„ëŒ€ì— ì‹ í˜¸ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ì™€ ìˆ˜ì¹˜ ë¶„ì„ì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë„êµ¬ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0898c",
   "metadata": {},
   "source": [
    "### Deformation stability of Multiscale representations\n",
    "ë‹¤ì¤‘ ì¶•ì²™ í‘œí˜„ì˜ ë³€í˜• ì•ˆì •ì„±\n",
    "\n",
    "The benefit of multiscale localised wavelet decompositions over Fourier decompositions is revealed when considering the effect of small deformations â€˜nearbyâ€™ the underlying symmetry group. Let us illustrate this important concept in the Euclidean domain and the translation group. Since the Fourier representation diagonalises the shift operator (which can be thought of as convolution, as we will see in more detail in Section 4.2), it is an efficient representation for translation transformations. However, Fourier decompositions are unstable under high-frequency deformations. In contrast, wavelet decompositions offer a stable representation in such cases.\n",
    "\n",
    "í‘¸ë¦¬ì— ë¶„í•´ì— ë¹„í•´ ë©€í‹°ìŠ¤ì¼€ì¼ êµ­ì†Œí™” ì›¨ì´ë¸”ë¦¿ ë¶„í•´ì˜ ì´ì ì€ ê¸°ë³¸ ëŒ€ì¹­ ê·¸ë£¹ 'ê·¼ì²˜'ì— ìˆëŠ” ì‘ì€ ë³€í˜•ì˜ íš¨ê³¼ë¥¼ ê³ ë ¤í•  ë•Œ ë“œëŸ¬ë‚©ë‹ˆë‹¤. ì´ ì¤‘ìš”í•œ ê°œë…ì„ ìœ í´ë¦¬ë“œ ì˜ì—­ê³¼ ë³€í™˜ ê·¸ë£¹ì—ì„œ ì„¤ëª…í•´ ë³´ê² ìŠµë‹ˆë‹¤. í‘¸ë¦¬ì— í‘œí˜„ì€ ì‹œí”„íŠ¸ ì—°ì‚°ì(4.2ì ˆì—ì„œ ìì„¸íˆ ì‚´í´ë³´ê² ì§€ë§Œ ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŒ)ë¥¼ ëŒ€ê°ì„ ìœ¼ë¡œ ëŒ€ì¹­í™”í•˜ê¸° ë•Œë¬¸ì— ë³€í™˜ ë³€í™˜ì— íš¨ìœ¨ì ì¸ í‘œí˜„ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í‘¸ë¦¬ì— ë¶„í•´ëŠ” ê³ ì£¼íŒŒìˆ˜ ë³€í˜•ì—ì„œ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì´ì™€ ëŒ€ì¡°ì ìœ¼ë¡œ ì›¨ì´ë¸”ë¦¿ ë¶„í•´ëŠ” ì´ëŸ¬í•œ ê²½ìš°ì— ì•ˆì •ì ì¸ í‘œí˜„ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "Skip\n",
    "\n",
    "Consequently, such Fourier representation is *unstable under deformations*, however small. This unstability is manifested in general domains and non-rigid transformations; we will see another instance of this unstability in the analysis of 3d shapes using the natural extension of Fourier transforms described in Section 4.4\n",
    "\n",
    "ê²°ê³¼ì ìœ¼ë¡œ ì´ëŸ¬í•œ í‘¸ë¦¬ì— í‘œí˜„ì€ ì•„ë¬´ë¦¬ ì‘ì€ ë³€í˜•ì—ë„ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¶ˆì•ˆì •ì„±ì€ ì¼ë°˜ ì˜ì—­ê³¼ ë¹„ê°•ì²´ ë³€í™˜ì—ì„œ ë‚˜íƒ€ë‚˜ë©°, 4.4ì ˆì—ì„œ ì„¤ëª…í•œ í‘¸ë¦¬ì— ë³€í™˜ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í™•ì¥ì„ ì‚¬ìš©í•œ 3D ë„í˜• ë¶„ì„ì—ì„œ ì´ëŸ¬í•œ ë¶ˆì•ˆì •ì„±ì˜ ë˜ ë‹¤ë¥¸ ì˜ˆë¥¼ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤.\n",
    "\n",
    "Wavelets offer a remedy to this problem that also reveals the power of ultiscale representations. In the above example, we can show (Mallat, 2012) that the wavelet decomposition $W_Ïˆx$ is *approximately equivariant* to deformations,\n",
    "\n",
    "ì›¨ì´ë¸Œë ›ì€ ì´ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±…ì„ ì œì‹œí•˜ë©° ìš¸íŠ¸ë¼ìŠ¤ì¼€ì¼ í‘œí˜„ì˜ í˜ì„ ë“œëŸ¬ëƒ…ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì—ì„œ ì›¨ì´ë¸Œë › ë¶„í•´ WÏˆxëŠ” ë³€í˜•ê³¼ ê±°ì˜ ë“±ë³€ìˆ˜ì„ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤(Mallat, 2012),\n",
    "\n",
    "In other words, decomposing the signal information into scales using localised filters rather than frequencies turns a global unstable representation into a family of locally stable features. Importantly, such measurements at different scales are not yet invariant, and need to be progressively processed towards the low frequencies, hinting at the deep compositional nature of modern neural networks, and captured in our Blueprint for Geometric Deep Learning, presented next.\n",
    "\n",
    "ì¦‰, ì£¼íŒŒìˆ˜ê°€ ì•„ë‹Œ êµ­ì†Œ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ í˜¸ ì •ë³´ë¥¼ ìŠ¤ì¼€ì¼ë¡œ ë¶„í•´í•˜ë©´ ì „ì²´ì ìœ¼ë¡œ ë¶ˆì•ˆì •í•œ í‘œí˜„ì´ êµ­ì§€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ íŠ¹ì§•ì˜ ì§‘í•©ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤. ì¤‘ìš”í•œ ì ì€ ì„œë¡œ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ì—ì„œì˜ ì´ëŸ¬í•œ ì¸¡ì •ê°’ì€ ì•„ì§ ë¶ˆë³€í•˜ì§€ ì•Šìœ¼ë©°, ë‚®ì€ ì£¼íŒŒìˆ˜ë¡œ ê°ˆìˆ˜ë¡ ì ì§„ì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ìœ¼ë¡œ, ì´ëŠ” í˜„ëŒ€ ì‹ ê²½ë§ì˜ ì‹¬ì¸µ êµ¬ì„± íŠ¹ì„±ì„ ì•”ì‹œí•˜ë©°, ë‹¤ìŒì— ì†Œê°œí•  ê¸°í•˜í•™ì  ë”¥ëŸ¬ë‹ ì²­ì‚¬ì§„ì— í¬ì°©ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ì²­ì‚¬ì§„ì— ë‹´ê²¨ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6195fe8",
   "metadata": {},
   "source": [
    "### Scale Separation Prior\n",
    "ìŠ¤ì¼€ì¼ ë¶„ë¦¬ ì‚¬ì „\n",
    "\n",
    "We can build from this insight by considering a multiscale coarsening of the data domain $â„¦$ into a hierarchy $â„¦_1, . . . , â„¦_J$. As it turns out, such coarsening can be defined on very general domains, including grids, graphs, and manifolds. Informally, a coarsening assimilates nearby points $u, u' âˆˆ â„¦$ together, and thus only requires an appropriate notion of metric in the domain. If $X_j (â„¦_j , C_j ) := {x_j : â„¦_j â†’ C_j}$ denotes signals defined over the coarsened domain $â„¦_j$ , we informally say that a function $f : \\mathcal{X} (â„¦) â†’ \\mathcal{Y}$ is *locally stable* at scale $j$ if it admits a factorisation of the form $f â‰ˆ f_j â—¦ P_j$ , where $P_j : \\mathcal{X} (â„¦) â†’ \\mathcal{X}_j (â„¦_j)$ is a non-linear *coarse graining* and $f_j : \\mathcal{X}_j (â„¦_j ) â†’ \\mathcal{Y}$. In other words, while the target function $f$ might depend on complex long-range interactions between features over the whole domain, in locally-stable functions it is possible to *separate* the interactions across scales, by first focusing on localised interactions that are then propagated towards the coarse scales.\n",
    "\n",
    "ë°ì´í„° ë„ë©”ì¸ Î©ì„ ê³„ì¸µ êµ¬ì¡° Î©1, . . . . , Î©J ë¡œ ê±°ì¹ ê²Œ í•˜ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê±°ì¹ ê¸°ëŠ” ê·¸ë¦¬ë“œ, ê·¸ë˜í”„, ë‹¤ì–‘ì²´ ë“± ë§¤ìš° ì¼ë°˜ì ì¸ ë„ë©”ì¸ì—ì„œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¹„ê³µì‹ì ìœ¼ë¡œ, ê±°ì¹ ê¸°ëŠ” ê·¼ì²˜ì˜ ì  u, u0 âˆˆ Î©ì„ í•¨ê»˜ ë™í™”ì‹œí‚¤ë¯€ë¡œ ë„ë©”ì¸ì—ì„œ ì ì ˆí•œ ë©”íŠ¸ë¦­ ê°œë…ë§Œ í•„ìš”í•©ë‹ˆë‹¤. $X_j (â„¦_j , C_j ) := {x_j : â„¦_j â†’ C_j}$ê°€ ê±°ì¹ ì–´ì§„ ì˜ì—­ Î©jì— ì •ì˜ëœ ì‹ í˜¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°, ë¹„ê³µì‹ì ìœ¼ë¡œ í•¨ìˆ˜ $f : \\mathcal{X} (â„¦) â†’ \\mathcal{Y}$ ê°€ f â‰ˆ fj â—¦ Pj í˜•ì‹ì˜ ì¸ìˆ˜ë¶„í•´ë¥¼ í—ˆìš©í•˜ëŠ” ê²½ìš°, ì—¬ê¸°ì„œ $P_j : \\mathcal{X} (â„¦) â†’ \\mathcal{X}_j (â„¦_j)$ëŠ” ë¹„ì„ í˜• ê±°ì¹ ê¸°ì´ê³  $f_j : \\mathcal{X}_j (â„¦_j ) â†’ \\mathcal{Y}$ëŠ” êµ­ì†Œì ìœ¼ë¡œ ì•ˆì •í•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ëª©í‘œ í•¨ìˆ˜ fëŠ” ì „ì²´ ì˜ì—­ì— ê±¸ì³ íŠ¹ì§•ë“¤ ê°„ì˜ ë³µì¡í•œ ì¥ê±°ë¦¬ ìƒí˜¸ì‘ìš©ì— ì˜ì¡´í•  ìˆ˜ ìˆì§€ë§Œ, êµ­ì§€ì ìœ¼ë¡œ ì•ˆì •ì ì¸ í•¨ìˆ˜ì—ì„œëŠ” ë¨¼ì € êµ­ì§€ì ì¸ ìƒí˜¸ì‘ìš©ì— ì´ˆì ì„ ë§ì¶˜ ë‹¤ìŒ ê±°ì¹œ ìŠ¤ì¼€ì¼ë¡œ ì „íŒŒí•¨ìœ¼ë¡œì¨ ìŠ¤ì¼€ì¼ ê°„ ìƒí˜¸ì‘ìš©ì„ ë¶„ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "Such principles are of fundamental importance in many areas of physics and mathematics, as manifested for instance in statistical physics in the so-called renormalisation group, or leveraged in important numerical algorithms such as the Fast Multipole Method. In machine learning, multiscale representations and local invariance are the fundamental mathematical principles underpinning the efficiency of Convolutional Neural Networks and Graph Neural Networks and are typically implemented in the form of *local pooling*. In future work, we will further develop tools from computational harmonic analysis that unify these principles across our geometric domains and will shed light onto the statistical learning benefits of scale separation.\n",
    "\n",
    "ì´ëŸ¬í•œ ì›ë¦¬ëŠ” ë¬¼ë¦¬í•™ ë° ìˆ˜í•™ì˜ ë§ì€ ë¶„ì•¼ì—ì„œ ê·¼ë³¸ì ìœ¼ë¡œ ì¤‘ìš”í•˜ë©°, ì˜ˆë¥¼ ë“¤ì–´ ì¬ë…¸ë©€ë¼ì´ì œì´ì…˜ ê·¸ë£¹ì˜ í†µê³„ ë¬¼ë¦¬í•™ì—ì„œ ë‚˜íƒ€ë‚˜ê±°ë‚˜ ê³ ì† ë‹¤ì¤‘ê·¹ ë°©ë²•ê³¼ ê°™ì€ ì¤‘ìš”í•œ ìˆ˜ì¹˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. ë¨¸ì‹  ëŸ¬ë‹ì—ì„œ ë©€í‹°ìŠ¤ì¼€ì¼ í‘œí˜„ê³¼ êµ­ì†Œ ë¶ˆë³€ì„±ì€ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ê³¼ ê·¸ë˜í”„ ì‹ ê²½ë§ì˜ íš¨ìœ¨ì„±ì„ ë’·ë°›ì¹¨í•˜ëŠ” ê¸°ë³¸ ìˆ˜í•™ì  ì›ë¦¬ì´ë©°, ì¼ë°˜ì ìœ¼ë¡œ ë¡œì»¬ í’€ë§ì˜ í˜•íƒœë¡œ êµ¬í˜„ë©ë‹ˆë‹¤. í–¥í›„ ì‘ì—…ì—ì„œëŠ” ì´ëŸ¬í•œ ì›ë¦¬ë¥¼ ê¸°í•˜í•™ì  ì˜ì—­ ì „ë°˜ì— ê±¸ì³ í†µí•©í•˜ê³  ê·œëª¨ ë¶„ë¦¬ì˜ í†µê³„ì  í•™ìŠµ ì´ì ì„ ì¡°ëª…í•˜ëŠ” ê³„ì‚° ê³ ì¡°íŒŒ ë¶„ì„ ë„êµ¬ë¥¼ ì¶”ê°€ë¡œ ê°œë°œí•  ì˜ˆì •ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839f062",
   "metadata": {},
   "source": [
    "## 3.5 The Blueprint of Geometric Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f340825",
   "metadata": {},
   "source": [
    "The geometric principles of Symmetry, Geometric Stability, and Scale Separation discussed in Sections 3.1â€“3.4 can be combined to provide a universal blueprint for learning stable representations of high-dimensional data. These representations will be produced by functions f operating on signals $\\mathcal{X} (â„¦, \\mathcal{C})$ defined on the domain $â„¦$, which is endowed with a symmetry group $\\mathfrak{G}$. \n",
    "\n",
    "3.1~3.4ì ˆì—ì„œ ì„¤ëª…í•œ ëŒ€ì¹­, ê¸°í•˜í•™ì  ì•ˆì •ì„±, ìŠ¤ì¼€ì¼ ë¶„ë¦¬ì˜ ê¸°í•˜í•™ì  ì›ë¦¬ë¥¼ ê²°í•©í•˜ì—¬ ê³ ì°¨ì› ë°ì´í„°ì˜ ì•ˆì •ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë³´í¸ì ì¸ ì²­ì‚¬ì§„ì„ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ì€ ëŒ€ì¹­ ê·¸ë£¹ Gê°€ ë¶€ì—¬ëœ ë„ë©”ì¸ Î©ì— ì •ì˜ëœ ì‹ í˜¸ $X(Î©, C)$ì— ëŒ€í•´ ì‘ë™í•˜ëŠ” í•¨ìˆ˜ $f$ì— ì˜í•´ ìƒì„±ë©ë‹ˆë‹¤. \n",
    "\n",
    "The geometric priors we have described so far do not prescribe a specific architecture for building such representation, but rather a series of necessary conditions. However, they hint at an axiomatic construction that provably satisfies these geometric priors, while ensuring a highly expressive representation that can approximate any target function satisfying such priors. \n",
    "\n",
    "ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ ê¸°í•˜í•™ì  ì„ í–‰ì€ ì´ëŸ¬í•œ í‘œí˜„ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ íŠ¹ì • ì•„í‚¤í…ì²˜ë¥¼ ê·œì •í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì¼ë ¨ì˜ í•„ìˆ˜ ì¡°ê±´ì„ ê·œì •í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê¸°í•˜í•™ì  ì„ í–‰ ì¡°ê±´ì„ ì¦ëª…ì ìœ¼ë¡œ ë§Œì¡±í•˜ëŠ” ê³µë¦¬ì ì¸ êµ¬ì¡°ë¥¼ ì•”ì‹œí•˜ëŠ” ë™ì‹œì— ì´ëŸ¬í•œ ì„ í–‰ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  ëª©í‘œ í•¨ìˆ˜ì— ê·¼ì‚¬ì¹˜ë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ” í‘œí˜„ë ¥ì´ ë›°ì–´ë‚œ í‘œí˜„ì„ ë³´ì¥í•©ë‹ˆë‹¤. \n",
    "\n",
    "A simple initial observation is that, in order to obtain a highly expressive representation, we are required to introduce a non-linear element, since if $f$ is linear and G-invariant, then for all $x âˆˆ \\mathcal{X} (â„¦)$,\n",
    "\n",
    "ê°„ë‹¨í•œ ì²« ë²ˆì§¸ ê´€ì°°ì€ í‘œí˜„ë ¥ì´ ë†’ì€ í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ë¹„ì„ í˜• ìš”ì†Œë¥¼ ë„ì…í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. fê°€ ì„ í˜•ì´ê³  G-ë³€ìˆ˜ì¸ ê²½ìš° ëª¨ë“  x âˆˆ X (Î©)ì— ëŒ€í•´ ë¹„ì„ í˜• ìš”ì†Œë¥¼ ë„ì…í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤,\n",
    "\n",
    "<p>\n",
    "$$\n",
    "f(x) = \\frac{1}{Âµ(\\mathfrak{G})} \\int_{\\mathfrak{G}} f(\\mathfrak{g}.x)dÂµ(\\mathfrak{g}) = f \\left( \\frac{1}{Âµ(\\mathfrak{G})} \\int_{\\mathfrak{G}} (\\mathfrak{g}.x)dÂµ(\\mathfrak{g}) \\right)\n",
    "$$\n",
    "</p>\n",
    "\n",
    ">Here, $Âµ(g)$ is known as the Haar measure of the group $\\mathcal{G}$, and the integral is performed over the entire group.\n",
    ">\n",
    ">ì—¬ê¸°ì„œ $Âµ(g)$ëŠ” ê·¸ë£¹ Gì˜ í•˜ë¥´ ì¸¡ì •ê°’ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆìœ¼ë©° ì ë¶„ì€ ì „ì²´ ê·¸ë£¹ì— ëŒ€í•´ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n",
    "\n",
    "which indicates that $F$ only depends on x through the $\\mathfrak{G}$-average $Ax = \\frac{1}{Âµ(\\mathfrak{G})} \\int_{\\mathfrak{G}}(\\mathfrak{g}.x)dÂµ(\\mathfrak{g})$. In the case of images and translation, this would entail using only the average RGB color of the input! \n",
    "\n",
    "ì´ëŠ” $F$ê°€ $\\mathfrak{G}$-í‰ê·  $Ax = \\frac{1}{Âµ(\\mathfrak{G})} \\int_{\\mathfrak{G}}(\\mathfrak{g}.x)dÂµ(\\mathfrak{g})$ ë¥¼ í†µí•´ xì—ë§Œ ì˜ì¡´í•œë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ë¯¸ì§€ ë° ë²ˆì—­ì˜ ê²½ìš° ì…ë ¥ì˜ í‰ê·  RGB ìƒ‰ìƒë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤!\n",
    "\n",
    "While this reasoning shows that the family of *linear invariantsis* not a very rich object, the family of *linear equivariants* provides a much more powerful tool, since it enables the construction of rich and stable features by composition with appropriate non-linear maps, as we will now explain. Indeed, if $B : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{X} (â„¦, \\mathcal{C'})$ is $\\mathfrak{G}$-equivariant satisfying $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$ for all $x âˆˆ \\mathcal{X}$ and $\\mathfrak{g} âˆˆ \\mathfrak{G}$, and $Ïƒ : \\mathcal{C'} â†’ \\mathcal{C''}$ is an arbitrary (non-linear) map, then we easily verify that the composition $U := (Ïƒ â—¦ B) : X (â„¦, \\mathcal{C}) â†’ X (â„¦, \\mathcal{C''})$ is also $\\mathfrak{G}$-equivariant, where $Ïƒ : X (â„¦,\\mathcal{C'} ) â†’ X (â„¦, \\mathcal{C''})$ is the element-wise instantiation of Ïƒ given as $(Ïƒ(x))(u) := Ïƒ(x(u))$.\n",
    "\n",
    "ì´ ì¶”ë¡ ì€ ì„ í˜• ë¶ˆë³€ëŸ‰êµ°ì´ ê·¸ë‹¤ì§€ í’ë¶€í•œ ê°ì²´ëŠ” ì•„ë‹ˆì§€ë§Œ, ì„ í˜• ë“±ì‹êµ°ì€ ì´ì œ ì„¤ëª…í•˜ê² ì§€ë§Œ ì ì ˆí•œ ë¹„ì„ í˜• ë§µê³¼ í•¨ê»˜ êµ¬ì„±ë˜ì–´ ë”ìš± í’ë¶€í•˜ê³  ì•ˆì •ì ì¸ ê¸°ëŠ¥ì„ êµ¬ì„±í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í›¨ì”¬ ë” ê°•ë ¥í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‹¤ì œë¡œ B : X (Î©, C) â†’ X (Î©, C 0 )ê°€ ëª¨ë“  x âˆˆ Xì™€ g âˆˆ Gì— ëŒ€í•´ B(g.x) = g.B(x)ë¥¼ ë§Œì¡±í•˜ëŠ” G ë“±ì‹ì´ê³  Ïƒ : C 0 â†’ C00ì´ ì„ì˜ì˜ (ë¹„ì„ í˜•) ë§µì´ë¼ë©´, ìš°ë¦¬ëŠ” ì‰½ê²Œ êµ¬ì„± U := (Ïƒ â—¦ B) : X (Î©, C) â†’ X (Î©, C 00) ì—­ì‹œ G ë“±ì‹ì´ë©°, ì—¬ê¸°ì„œ Ïƒ : X (Î©, C 0 ) â†’ X (Î©, C 00)ëŠ” (Ïƒ(x))(u) := Ïƒ(x(u))ë¡œ ì£¼ì–´ì§„ Ïƒì˜ ì›ì†Œë³„ ì¸ìŠ¤í„´ìŠ¤í™”ì…ë‹ˆë‹¤.\n",
    "\n",
    "This simple property allows us to define a very general family of G-invariants, by composing $U$ with the group averages $A â—¦ U : X (â„¦, \\mathcal{C}) â†’ \\mathcal{C''}$. A natural question is thus whether any $\\mathfrak{G}$-invariant function can be approximated at arbitrary precision by such a model, for appropriate choices of $B$ and $Ïƒ$. It is not hard to adapt the standard Universal Approximation Theorems from unstructured vector inputs to show that shallow â€˜geometricâ€™ networks are also universal approximators, by properly generalising the group average to a general non-linear invariant. However, as already described in the case of Fourier versus Wavelet invariants, there is a fundamental tension between shallow global invariance and deformation stability. This motivates an alternative representation, which considers instead localised equivariant maps. Assuming that $â„¦$ is further equipped with a distance metric d, we call an equivariant map U localised if $(Ux)(u)$ depends only on the values of $x(v)$ for $N_u = {v : d(u, v) â‰¤ r}$, for some small radius $r$; the latter set $N_u$ is called the *receptive field*. \n",
    "\n",
    "ì´ ê°„ë‹¨í•œ ì†ì„±ì„ í†µí•´ ìš°ë¦¬ëŠ” Uë¥¼ ê·¸ë£¹ í‰ê·  $A â—¦ U : X (â„¦, \\mathcal{C}) â†’ \\mathcal{C''}$ ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë§¤ìš° ì¼ë°˜ì ì¸ G-ë¶ˆë³€ëŸ‰ êµ°ì„ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì€ $B$ì™€ $Ïƒ$ì˜ ì ì ˆí•œ ì„ íƒì— ëŒ€í•´ ì´ëŸ¬í•œ ëª¨ë¸ì— ì˜í•´ ì„ì˜ì˜ ì •ë°€ë„ë¡œ G-ë¶ˆë³€ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í™”í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ ë²¡í„° ì…ë ¥ì—ì„œ í‘œì¤€ ë²”ìš© ê·¼ì‚¬ ì •ë¦¬ë¥¼ ì ìš©í•˜ì—¬ ê·¸ë£¹ í‰ê· ì„ ì¼ë°˜ ë¹„ì„ í˜• ë¶ˆë³€ëŸ‰ìœ¼ë¡œ ì ì ˆíˆ ì¼ë°˜í™”í•¨ìœ¼ë¡œì¨ ì–•ì€ 'ê¸°í•˜í•™ì ' ë„¤íŠ¸ì›Œí¬ë„ ë²”ìš© ê·¼ì‚¬í™”ìì„ì„ ë³´ì—¬ì£¼ëŠ” ê²ƒì€ ì–´ë µì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í‘¸ë¦¬ì— ë¶ˆë³€ìˆ˜ì™€ ì›¨ì´ë¸”ë¦¿ ë¶ˆë³€ìˆ˜ì˜ ê²½ìš°ì—ì„œ ì´ë¯¸ ì„¤ëª…í–ˆë“¯ì´, ì–•ì€ ì „ì—­ ë¶ˆë³€ì„±ê³¼ ë³€í˜• ì•ˆì •ì„± ì‚¬ì´ì—ëŠ” ê·¼ë³¸ì ì¸ ê¸´ì¥ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë”°ë¼ì„œ êµ­ì†Œí™”ëœ ë“±ë³€ëŸ‰ ë§µì„ ê³ ë ¤í•˜ëŠ” ëŒ€ì•ˆì  í‘œí˜„ì´ í•„ìš”í•©ë‹ˆë‹¤. $Î©$ì— ê±°ë¦¬ ë©”íŠ¸ë¦­ dê°€ ì¶”ê°€ë¡œ ì¥ì°©ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•  ë•Œ, ì‘ì€ ë°˜ì§€ë¦„ $r$ì— ëŒ€í•´ $N_u = {v : d(u, v) â‰¤ r}$ì— ëŒ€í•´ $(Ux)(u)$ê°€ $x(v)$ì˜ ê°’ì—ë§Œ ì˜ì¡´í•˜ëŠ” ê²½ìš° ë“±ë³€ëŸ‰ ë§µ Uë¥¼ êµ­ì†Œí™”ëœ ë§µì´ë¼ê³  í•˜ë©°, í›„ìì˜ ì§‘í•© Nuë¥¼ ìˆ˜ìš©ì¥ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤.\n",
    "\n",
    ">Such proofs have been demonstrated, for example, for the Deep Sets model by Zaheer et al. (2017).\n",
    "\n",
    ">Meaningful metrics can be defined on grids, graphs, manifolds, and groups. A notable exception are sets, where there is no predefined notion of metric.\n",
    ">\n",
    ">ì˜ë¯¸ ìˆëŠ” ë©”íŠ¸ë¦­ì€ ê·¸ë¦¬ë“œ, ê·¸ë˜í”„, ë‹¤ì–‘ì²´ ë° ê·¸ë£¹ì— ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ëª©í•  ë§Œí•œ ì˜ˆì™¸ëŠ” ë©”íŠ¸ë¦­ì— ëŒ€í•œ ì‚¬ì „ ì •ì˜ëœ ê°œë…ì´ ì—†ëŠ” ì§‘í•©ì…ë‹ˆë‹¤.\n",
    "\n",
    "A single layer of local equivariant map $U$ cannot approximate functions with long-range interactions, but a composition of several local equivariant maps $U_J â—¦ U_Jâˆ’1 Â· Â· Â· â—¦ U_1$ increases the receptive field while preserving the stability properties of local equivariants. The receptive field is further increased by interleaving downsampling operators that coarsen the domain (again assuming a metric structure), completing the parallel with Multiresolution Analysis (MRA, see e.g. Mallat (1999)). \n",
    "\n",
    "ë‹¨ì¼ ê³„ì¸µì˜ êµ­ë¶€ ë“±ë³€ëŸ‰ ë§µ UëŠ” ì¥ê±°ë¦¬ ìƒí˜¸ì‘ìš©ì´ ìˆëŠ” í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í™”í•  ìˆ˜ ì—†ëŠ” ë°˜ë©´, ì—¬ëŸ¬ êµ­ë¶€ ë“±ë³€ëŸ‰ ë§µ UJ â—¦ UJ-1 - - - â—¦ U1ì˜ êµ¬ì„±ì€ êµ­ë¶€ ë“±ë³€ëŸ‰ì˜ ì•ˆì •ì„± íŠ¹ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ ìˆ˜ìš© í•„ë“œë¥¼ ì¦ê°€ì‹œí‚µë‹ˆë‹¤. ì˜ì—­ì„ ê±°ì¹ ê²Œ í•˜ëŠ” ë‹¤ìš´ìƒ˜í”Œë§ ì—°ì‚°ìë¥¼ ì¸í„°ë¦¬ë¹™í•˜ì—¬(ë‹¤ì‹œ ë©”íŠ¸ë¦­ êµ¬ì¡°ë¥¼ ê°€ì •í•˜ì—¬) ìˆ˜ìš© í•„ë“œë¥¼ ë”ìš± ì¦ê°€ì‹œì¼œ ë‹¤ì¤‘í•´ìƒë„ ë¶„ì„(MRA, Mallat (1999) ì°¸ì¡°)ê³¼ ë³‘ë ¬ë¡œ ì™„ì„±í•©ë‹ˆë‹¤. \n",
    "\n",
    ">The term â€˜receptive fieldâ€™ originated in the neuroscience literature, referring to the spatial domain that affects the output of a given neuron.\n",
    ">\n",
    ">'ìˆ˜ìš© í•„ë“œ'ë¼ëŠ” ìš©ì–´ëŠ” ì‹ ê²½ê³¼í•™ ë¬¸í—Œì—ì„œ ìœ ë˜í•œ ê²ƒìœ¼ë¡œ, ì£¼ì–´ì§„ ë‰´ëŸ°ì˜ ì¶œë ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê³µê°„ ì˜ì—­ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "In summary, the geometry of the input domain, with knowledge of an underyling symmetry group, provides three key building blocks: (i) a local equivariant map, (ii) a global invariant map, and (iii) a coarsening operator. These building blocks provide a rich function approximation space with prescribed invariance and stability properties by combining them together in a scheme we refer to as the Geometric Deep Learning Blueprint (Figure 8)\n",
    "\n",
    "ìš”ì•½í•˜ë©´, ì–¸ë”ë§ ëŒ€ì¹­ ê·¸ë£¹ì— ëŒ€í•œ ì§€ì‹ì´ ìˆëŠ” ì…ë ¥ ì˜ì—­ì˜ ê¸°í•˜í•™ì€ (i) ë¡œì»¬ ë“±ë³€ëŸ‰ ë§µ, (ii) ê¸€ë¡œë²Œ ë¶ˆë³€ëŸ‰ ë§µ, (iii) ì¡°ë„(coarsening) ì—°ì‚°ìë¼ëŠ” ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¹Œë”© ë¸”ë¡ì€ ê¸°í•˜í•™ì  ë”¥ ëŸ¬ë‹ ë¸”ë£¨í”„ë¦°íŠ¸ë¼ê³  í•˜ëŠ” ì²´ê³„ì—ì„œ í•¨ê»˜ ê²°í•©í•˜ì—¬ ê·œì •ëœ ë¶ˆë³€ì„± ë° ì•ˆì •ì„± ì†ì„±ì„ ê°–ì¶˜ í’ë¶€í•œ í•¨ìˆ˜ ê·¼ì‚¬í™” ê³µê°„ì„ ì œê³µí•©ë‹ˆë‹¤(ê·¸ë¦¼ 8)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec7d4f15",
   "metadata": {},
   "source": [
    "<div style=\"background-color:lightgray\">\n",
    "Geometric Deep Learning Blueprint\n",
    "\n",
    "Let $â„¦$ and $â„¦'$ be domains, $\\mathfrak{G}$ a symmetry group over $â„¦$, and write $â„¦' âŠ† â„¦$ if $â„¦'$ can be considered a compact version of $â„¦$.\n",
    "\n",
    "$â„¦$ê³¼  $â„¦'$ì„ ì˜ì—­ìœ¼ë¡œ, $\\mathfrak{G}$ë¥¼ $â„¦$ì— ëŒ€í•œ ëŒ€ì¹­ ê·¸ë£¹ìœ¼ë¡œ í•˜ê³ , $â„¦' âŠ† â„¦$ ì´ë©´ $â„¦'$ ë¥¼  $â„¦$ì˜ ì••ì¶• ë²„ì „ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "We define the following building blocks:\n",
    "\n",
    "ë‹¤ìŒê³¼ ê°™ì€ ë¹Œë”© ë¸”ë¡ì„ ì •ì˜í•©ë‹ˆë‹¤:\n",
    "\n",
    "\n",
    ">Linear $\\mathfrak{G}$-equivariant layer $B : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{X} (â„¦', \\mathcal{C'}) \\text{ satisfying } B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x) \\text{ for all } \\mathfrak{g} âˆˆ \\mathfrak{G} \\text{ and } x âˆˆ \\mathcal{X} (â„¦, \\mathcal{C}).$\n",
    ">\n",
    ">ì„ í˜• ğ”Š -ë“±ë³€ëŸ‰ ë ˆì´ì–´ $B : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{X} (â„¦', \\mathcal{C'})$ ëŠ” ëª¨ë“  $\\mathfrak{g} âˆˆ \\mathfrak{G}$ ë° $x âˆˆ \\mathcal{X} (â„¦, \\mathcal{C})$ì— ëŒ€í•´ $B(\\mathfrak{g}.x) = \\mathfrak{g}.B(x)$ ë¥¼ ë§Œì¡±í•©ë‹ˆë‹¤.\n",
    "\n",
    ">Nonlinearity $Ïƒ : \\mathcal{C} â†’ \\mathcal{C'}$ applied element-wise as $(Ïƒ(x))(u) = Ïƒ(x(u))$.\n",
    ">\n",
    ">ë¹„ì„ í˜•ì„± $Ïƒ : \\mathcal{C} â†’ \\mathcal{C'}$ëŠ” $(Ïƒ(x))(u) = Ïƒ(x(u))$ë¡œ ìš”ì†Œë³„ë¡œ ì ìš©ë©ë‹ˆë‹¤\n",
    "\n",
    ">Local pooling (coarsening) $P : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{X} (â„¦', \\mathcal{C})$, such that $â„¦' âŠ† â„¦$.\n",
    ">\n",
    ">ë¡œì»¬ í’€ë§(ê±°ì¹ ê²Œ í•¨) $P : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{X} (â„¦', \\mathcal{C})$, ì¦‰, $â„¦' âŠ† â„¦$\n",
    "\n",
    ">G-invariant layer (global pooling) $A : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{Y}$ satisfying $A(\\mathfrak{g}.x) = A(x)$ for all $\\mathfrak{g} âˆˆ \\mathfrak{G}$ and $x âˆˆ \\mathcal{X} (â„¦, \\mathcal{C})$.\n",
    ">\n",
    "> G-ë³€ìˆ˜ ë ˆì´ì–´(ê¸€ë¡œë²Œ í’€ë§) $A : \\mathcal{X} (â„¦, \\mathcal{C}) â†’ \\mathcal{Y}$ëŠ” ëª¨ë“  $\\mathfrak{g} âˆˆ \\mathfrak{G}$ ë° $x âˆˆ \\mathcal{X} (â„¦, \\mathcal{C})$ì— ëŒ€í•´ $A(\\mathfrak{g}.x) = A(x)$ë¥¼ ë§Œì¡±í•©ë‹ˆë‹¤.\n",
    "\n",
    "Using these blocks allows constructing G-invariant functions $f :\\mathfrak{X} (â„¦, \\mathfrak{C}) â†’ \\mathfrak{Y}$ of the form\n",
    "\n",
    "ì´ëŸ¬í•œ ë¸”ë¡ì„ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì˜ G-ë³€ìˆ˜ í•¨ìˆ˜ $f :\\mathfrak{X} (â„¦, \\mathfrak{C}) â†’ \\mathfrak{Y}$ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "$$\n",
    "    f = A â—¦ Ïƒ_J â—¦ B_J â—¦ P_{Jâˆ’1} â—¦ . . . â—¦ P_1 â—¦ Ïƒ_1 â—¦ B_1\n",
    "$$\n",
    "\n",
    "where the blocks are selected such that the output space of each block\n",
    "matches the input space of the next one. Different blocks may exploit\n",
    "different choices of symmetry groups $\\mathfrak{G}$.\n",
    "\n",
    "ì—¬ê¸°ì„œ ê° ë¸”ë¡ì˜ ì¶œë ¥ ê³µê°„ì´ ë‹¤ìŒ ë¸”ë¡ì˜ ì…ë ¥ ê³µê°„ê³¼ ì¼ì¹˜í•˜ë„ë¡ ë¸”ë¡ì´ ì„ íƒë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë¸”ë¡ì€ ë‹¤ë¥¸ ëŒ€ì¹­ ê·¸ë£¹ ğ”Šì„ ê³ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f4307f",
   "metadata": {},
   "source": [
    "### Different settings of Geometric Deep Learning\n",
    "\n",
    "ê¸°í•˜í•™ì  ë”¥ëŸ¬ë‹ì˜ ë‹¤ì–‘í•œ ì„¤ì •\n",
    "\n",
    "One can make an important distinction between the setting when the domain â„¦ is assumed to be fixed and one is only interested in varying input signals defined on that domain, or the domain is part of the input as varies together with signals defined on it. A classical instance of the former case is encountered in computer vision applications, where images are assumed to be defined on a fixed domain (grid). Graph classification is an example of the latter setting, where both the structure of the graph as well as the signal defined on it (e.g. node features) are important. In the case of varying domain, geometric stability (in the sense of insensitivity to the deformation of â„¦) plays a crucial role in Geometric Deep Learning architecture.\n",
    "\n",
    "ë„ë©”ì¸ Î©ì´ ê³ ì •ëœ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³  í•´ë‹¹ ë„ë©”ì¸ì— ì •ì˜ëœ ë‹¤ì–‘í•œ ì…ë ¥ ì‹ í˜¸ì—ë§Œ ê´€ì‹¬ì´ ìˆëŠ” ê²½ìš°ì™€ ë„ë©”ì¸ì— ì •ì˜ëœ ì‹ í˜¸ì™€ í•¨ê»˜ ë„ë©”ì¸ì´ ì…ë ¥ì˜ ì¼ë¶€ì¸ ê²½ìš°ì˜ ì„¤ì •ì„ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ìì˜ ì „í˜•ì ì¸ ì‚¬ë¡€ëŠ” ì´ë¯¸ì§€ê°€ ê³ ì •ëœ ë„ë©”ì¸(ê·¸ë¦¬ë“œ)ì— ì •ì˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ëŠ” ì»´í“¨í„° ë¹„ì „ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜í”„ ë¶„ë¥˜ëŠ” ê·¸ë˜í”„ì˜ êµ¬ì¡°ì™€ ê·¸ë˜í”„ì— ì •ì˜ëœ ì‹ í˜¸(ì˜ˆ: ë…¸ë“œ íŠ¹ì§•)ê°€ ëª¨ë‘ ì¤‘ìš”í•œ í›„ìì˜ ì„¤ì •ì— ëŒ€í•œ ì˜ˆì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ê²½ìš°, ê¸°í•˜í•™ì  ì•ˆì •ì„±(Î©ì˜ ë³€í˜•ì— ë¯¼ê°í•˜ì§€ ì•Šë‹¤ëŠ” ì˜ë¯¸ì—ì„œ)ì´ ê¸°í•˜í•™ì  ë”¥ ëŸ¬ë‹ ì•„í‚¤í…ì²˜ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
    "\n",
    "This blueprint has the right level of generality to be used across a wide range of geometric domains. Different Geometric Deep Learning methods thus differ in their choice of the domain, symmetry group, and the specific implementation details of the aforementioned building blocks. As we will see in the following, a large class of deep learning architectures currently in use fall into this scheme and can thus be derived from common geometric principles. \n",
    "\n",
    "ì´ ì²­ì‚¬ì§„ì€ ë‹¤ì–‘í•œ ê¸°í•˜í•™ì  ì˜ì—­ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ìˆ˜ì¤€ì˜ ë²”ìš©ì„±ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê¸°í•˜í•™ì  ë”¥ëŸ¬ë‹ ë°©ë²•ë§ˆë‹¤ ë„ë©”ì¸, ëŒ€ì¹­ ê·¸ë£¹, ì•ì„œ ì–¸ê¸‰í•œ ë¹Œë”© ë¸”ë¡ì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ ì„¸ë¶€ ì‚¬í•­ì— ëŒ€í•œ ì„ íƒì´ ë‹¤ë¦…ë‹ˆë‹¤. ì•„ë˜ì—ì„œ ì‚´í´ë³´ê² ì§€ë§Œ, í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë§ì€ ì¢…ë¥˜ì˜ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ê°€ ì´ ì²´ê³„ì— ì†í•˜ë©°, ë”°ë¼ì„œ ì¼ë°˜ì ì¸ ê¸°í•˜í•™ì  ì›ë¦¬ì—ì„œ íŒŒìƒë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "\n",
    "In the following sections (4.1â€“4.6) we will describe the various geometric domains focusing on the â€˜5Gâ€™, and in Sections 5.1â€“5.8 the specific implementations of Geometric Deep Learning on these domains. \n",
    "\n",
    "ë‹¤ìŒ ì„¹ì…˜(4.1-4.6)ì—ì„œëŠ” '5G' ì— ì´ˆì ì„ ë§ì¶˜ ë‹¤ì–‘í•œ ê¸°í•˜í•™ì  ì˜ì—­ì— ëŒ€í•´ ì„¤ëª…í•˜ê³ , ì„¹ì…˜ 5.1-5.8ì—ì„œëŠ” ì´ëŸ¬í•œ ì˜ì—­ì—ì„œ ê¸°í•˜í•™ì  ë”¥ëŸ¬ë‹ì˜ êµ¬ì²´ì ì¸ êµ¬í˜„ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336294b9",
   "metadata": {},
   "source": [
    "| Architecture         | Domain â„¦       | Symmetry group G                            |\n",
    "|----------------------|----------------|---------------------------------------------|\n",
    "| CNN                  | Grid           | Translation                                 |\n",
    "| Spherical CNN        | Sphere / SO(3)  | Rotation SO(3)                              |\n",
    "| Intrinsic / Mesh CNN | Manifold       | Isometry Iso(â„¦) <br>   Gauge symmetry SO(2) |\n",
    "| GNN                  | Graph          | Permutation Î£n                              |\n",
    "| Deep Sets            | Set            | Permutation Î£n                              |\n",
    "| Transformer          | Complete Graph | Permutation Î£n                              |\n",
    "| LSTM                 | 1D Grid        | Time warping                                |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
